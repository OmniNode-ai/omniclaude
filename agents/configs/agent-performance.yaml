agent_domain: performance
agent_purpose: Performance optimization specialist for bottleneck detection and system
  efficiency
agent_title: ONEX Anti-YOLO Method + BFROS Framework
agent_description: Performance optimization specialist for bottleneck detection and
  system efficiency with Phase 5A Performance Intelligence integration
agent_context: performance
domain_query: performance optimization patterns bottlenecks
implementation_query: optimization strategies resource utilization
match_count: 5
confidence_threshold: 0.6
knowledge_capture_level: comprehensive
capabilities:
  mandatory_functions: true
  template_system: true
  enhanced_patterns: true
  quality_intelligence: true
  automated_baseline_establishment: true
  roi_driven_optimization: true
  ab_testing_statistical_significance: true
  performance_trend_prediction: true
  historical_pattern_analysis: true
  cost_benefit_analysis: true
archon_mcp_enabled: true
correlation_tracking: true
parallel_capable: true
triggers:
  - "optimize performance"
  - "performance bottleneck"
  - "slow response time"
  - "high latency"
  - "resource optimization"
  - "optimize performance with ROI analysis"
  - "establish performance baseline"
  - "identify optimization opportunities"
  - "analyze performance trends"
  - "performance optimization report"
  - "A/B test optimization"
  - "measure performance impact"
  - "performance cost-benefit"
  - "predict performance degradation"

performance_intelligence_integration: |
  ## Performance Intelligence Integration (Phase 5A)

  ### Overview

  The agent-performance integrates comprehensive Performance Intelligence capabilities
  to provide data-driven, ROI-focused optimization with continuous monitoring and
  predictive analytics.

  ### Phase 1: Baseline Establishment

  **Purpose**: Establish statistical performance baseline before optimization

  **Tool**: `establish_performance_baseline`

  **Usage**:
  ```typescript
  establish_performance_baseline(operation_name, {
    duration_minutes: 10,
    metrics: ["response_time", "throughput", "memory_usage", "cpu_usage"]
  })
  ```

  **Returns**:
  - Statistical baseline (mean, median, std_dev)
  - Percentiles (p50, p90, p95, p99)
  - Sample size and confidence interval
  - Timestamp and measurement duration

  **Example**:
  ```typescript
  // Establish baseline for API endpoint
  establish_performance_baseline("api_endpoint_processing", {
    duration_minutes: 15,
    metrics: ["response_time", "throughput", "error_rate"]
  })

  // Returns:
  // {
  //   operation_name: "api_endpoint_processing",
  //   baseline_statistics: {
  //     response_time: { mean: 245ms, p95: 890ms, p99: 1200ms },
  //     throughput: { mean: 450 req/s, p95: 520 req/s },
  //     error_rate: { mean: 0.02% }
  //   },
  //   sample_size: 900,
  //   confidence_level: 0.95
  // }
  ```

  ### Phase 2: Opportunity Discovery

  **Purpose**: Identify optimization opportunities with ROI scoring

  **Tool**: `identify_optimization_opportunities`

  **Usage**:
  ```typescript
  identify_optimization_opportunities(operation_name)
  ```

  **Returns**:
  - Detected issues with severity
  - Expected improvement percentage
  - Implementation complexity (low/medium/high)
  - ROI score (impact/effort ratio)
  - Priority ranking

  **Example**:
  ```typescript
  identify_optimization_opportunities("database_queries")

  // Returns:
  // {
  //   opportunities: [
  //     {
  //       issue: "N+1 query pattern detected",
  //       severity: "high",
  //       expected_improvement: "85%",
  //       complexity: "low",
  //       roi_score: 9.2,
  //       priority: 1,
  //       recommendation: "Add eager loading with .includes()"
  //     },
  //     {
  //       issue: "Missing index on user_id column",
  //       severity: "medium",
  //       expected_improvement: "45%",
  //       complexity: "low",
  //       roi_score: 7.8,
  //       priority: 2
  //     },
  //     {
  //       issue: "Inefficient sorting algorithm",
  //       severity: "low",
  //       expected_improvement: "15%",
  //       complexity: "medium",
  //       roi_score: 3.2,
  //       priority: 5
  //     }
  //   ],
  //   total_opportunities: 5,
  //   high_roi_count: 2
  // }
  ```

  ### Phase 3: Apply Optimizations

  **Purpose**: Apply optimizations with A/B testing and impact measurement

  **Tool**: `apply_performance_optimization`

  **Usage**:
  ```typescript
  apply_performance_optimization(operation_name, optimization_type, {
    test_duration_minutes: 5,
    measure_impact: true,
    a_b_test: true,
    rollback_threshold: 0.05  // rollback if <5% improvement
  })
  ```

  **Returns**:
  - Performance improvement percentage
  - Statistical significance (p-value)
  - A/B test results (control vs treatment)
  - ROI analysis (benefit vs cost)
  - Recommendation: apply/rollback/test-more

  **Example**:
  ```typescript
  apply_performance_optimization("api_endpoint_processing", "query_optimization", {
    test_duration_minutes: 10,
    measure_impact: true,
    a_b_test: true
  })

  // Returns:
  // {
  //   optimization_type: "query_optimization",
  //   performance_improvement: "82%",
  //   statistical_significance: {
  //     p_value: 0.001,
  //     confidence: "99.9%",
  //     significant: true
  //   },
  //   a_b_test_results: {
  //     control_group: { mean: 245ms, p95: 890ms, samples: 500 },
  //     treatment_group: { mean: 44ms, p95: 120ms, samples: 500 },
  //     improvement: "82%"
  //   },
  //   roi_analysis: {
  //     implementation_cost: "2 hours",
  //     annual_savings: "$12,000 (server costs)",
  //     payback_period: "1 day",
  //     roi_ratio: 60
  //   },
  //   recommendation: "APPLY - High confidence, significant improvement"
  // }
  ```

  ### Phase 4: Continuous Monitoring

  **Purpose**: Track performance over time with trend prediction

  **Tool**: `monitor_performance_trends`

  **Usage**:
  ```typescript
  monitor_performance_trends(time_window_hours: 24, include_predictions: true)
  ```

  **Returns**:
  - Performance trends over time
  - Anomaly detection results
  - Predictive alerts for degradation
  - Degradation warnings
  - Recommended actions

  **Example**:
  ```typescript
  monitor_performance_trends(time_window_hours: 48, include_predictions: true)

  // Returns:
  // {
  //   trends: {
  //     response_time: {
  //       direction: "improving",
  //       rate: "-12% per day",
  //       confidence: 0.92
  //     },
  //     memory_usage: {
  //       direction: "degrading",
  //       rate: "+5% per day",
  //       confidence: 0.85
  //     }
  //   },
  //   anomalies_detected: [
  //     {
  //       timestamp: "2025-10-03T14:30:00Z",
  //       metric: "response_time",
  //       value: 2400ms,
  //       expected: 450ms,
  //       severity: "high",
  //       likely_cause: "Database connection pool exhaustion"
  //     }
  //   ],
  //   predictions: {
  //     next_24h: {
  //       response_time: { predicted: 420ms, confidence: 0.88 },
  //       memory_usage: { predicted: "2.1GB", confidence: 0.76, alert: "approaching limit" }
  //     },
  //     next_7d: {
  //       memory_usage: {
  //         predicted: "3.8GB",
  //         confidence: 0.62,
  //         alert: "CRITICAL - will exceed 4GB limit in 6 days"
  //       }
  //     }
  //   },
  //   recommended_actions: [
  //     "Investigate memory leak causing gradual degradation",
  //     "Plan capacity upgrade within 5 days",
  //     "Review recent code changes affecting memory usage"
  //   ]
  // }
  ```

  ### Phase 5: Reporting

  **Purpose**: Generate comprehensive optimization report

  **Tool**: `get_optimization_report`

  **Usage**:
  ```typescript
  get_optimization_report(time_window_hours: 24)
  ```

  **Returns**:
  - All optimizations applied in timeframe
  - Cumulative performance gain
  - Total ROI achieved
  - Recommendations for next optimizations
  - Success/failure breakdown

  **Example**:
  ```typescript
  get_optimization_report(time_window_hours: 168)  // 1 week

  // Returns:
  // {
  //   timeframe: "2025-09-26 to 2025-10-03",
  //   optimizations_applied: 12,
  //   optimizations_successful: 10,
  //   optimizations_rolled_back: 2,
  //   cumulative_performance_gain: {
  //     response_time: "-68% (from 890ms to 285ms)",
  //     throughput: "+145% (from 450 to 1100 req/s)",
  //     memory_usage: "-22% (from 2.8GB to 2.2GB)",
  //     cpu_usage: "-31% (from 65% to 45%)"
  //   },
  //   total_roi: {
  //     implementation_cost: "24 hours",
  //     annual_savings: "$48,000",
  //     payback_period: "4.5 days",
  //     roi_ratio: 80
  //   },
  //   top_optimizations: [
  //     { type: "query_optimization", improvement: "82%", roi: 60 },
  //     { type: "caching_strategy", improvement: "45%", roi: 35 },
  //     { type: "index_optimization", improvement: "38%", roi: 28 }
  //   ],
  //   next_recommendations: [
  //     {
  //       opportunity: "Implement connection pooling",
  //       expected_improvement: "25%",
  //       roi_score: 8.5,
  //       priority: 1
  //     },
  //     {
  //       opportunity: "Add Redis caching layer",
  //       expected_improvement: "40%",
  //       roi_score: 7.2,
  //       priority: 2
  //     }
  //   ]
  // }
  ```

  ## Enhanced Workflow

  ### Complete Performance Optimization Cycle

  ```
  1. ESTABLISH BASELINE
     └─> measure_performance_baseline()
         └─> Capture: mean, p95, p99, throughput, resource usage
         └─> Output: Statistical baseline with confidence intervals

  2. IDENTIFY OPPORTUNITIES
     └─> identify_optimization_opportunities()
         └─> Detect: N+1 queries, missing indexes, inefficient algorithms
         └─> Score: ROI = (expected_improvement / complexity)
         └─> Output: Ranked list of opportunities with ROI scores

  3. PRIORITIZE BY ROI
     └─> Sort opportunities by ROI score
         └─> High ROI (>8.0): Immediate action
         └─> Medium ROI (5.0-8.0): Schedule for next sprint
         └─> Low ROI (<5.0): Backlog for future consideration

  4. APPLY OPTIMIZATION
     └─> apply_performance_optimization()
         └─> A/B Test: 50% control, 50% treatment
         └─> Measure: Statistical significance (p-value < 0.05)
         └─> Validate: Improvement meets threshold
         └─> Decision: Apply if significant, rollback if not

  5. MONITOR RESULTS
     └─> monitor_performance_trends()
         └─> Track: Real-time performance metrics
         └─> Detect: Anomalies and degradation patterns
         └─> Predict: Future performance trends
         └─> Alert: Proactive warnings for issues

  6. GENERATE REPORT
     └─> get_optimization_report()
         └─> Summarize: All optimizations and their impact
         └─> Calculate: Total ROI and payback period
         └─> Recommend: Next high-ROI opportunities
         └─> Document: Success stories and lessons learned
  ```

  ## Integration with Agent Workflow

  ### Pre-Execution Phase

  ```typescript
  // Before starting optimization work
  async function preExecution(context) {
    // 1. Establish baseline
    const baseline = await establish_performance_baseline(
      context.operation_name,
      { duration_minutes: 10 }
    );

    // 2. Identify opportunities
    const opportunities = await identify_optimization_opportunities(
      context.operation_name
    );

    // 3. Sort by ROI and present to user
    const prioritized = opportunities.sort((a, b) => b.roi_score - a.roi_score);

    return {
      baseline,
      opportunities: prioritized.slice(0, 5), // Top 5
      recommendation: prioritized[0]
    };
  }
  ```

  ### During-Execution Phase

  ```typescript
  // During optimization implementation
  async function duringExecution(optimization) {
    // 1. Apply optimization with A/B testing
    const result = await apply_performance_optimization(
      optimization.operation_name,
      optimization.type,
      {
        test_duration_minutes: 5,
        a_b_test: true,
        measure_impact: true
      }
    );

    // 2. Validate statistical significance
    if (result.statistical_significance.p_value < 0.05) {
      if (result.performance_improvement_pct > 20) {
        return { decision: "APPLY", confidence: "high", result };
      } else {
        return { decision: "APPLY", confidence: "low", result };
      }
    } else {
      return { decision: "ROLLBACK", reason: "Not statistically significant", result };
    }
  }
  ```

  ### Post-Execution Phase

  ```typescript
  // After optimization is applied
  async function postExecution(operation_name) {
    // 1. Monitor for degradation
    const trends = await monitor_performance_trends(
      time_window_hours: 24,
      include_predictions: true
    );

    // 2. Check for anomalies
    if (trends.anomalies_detected.length > 0) {
      return {
        status: "WARNING",
        anomalies: trends.anomalies_detected,
        action: "Investigate and potentially rollback"
      };
    }

    // 3. Validate improvement persists
    const current_performance = trends.current_metrics;
    const baseline = await get_baseline(operation_name);

    const improvement = calculate_improvement(baseline, current_performance);

    return {
      status: "SUCCESS",
      improvement,
      trends: trends.predictions
    };
  }
  ```

  ### Continuous Monitoring Phase

  ```typescript
  // Ongoing performance monitoring (runs periodically)
  async function continuousMonitoring() {
    // 1. Monitor all critical operations
    const operations = await get_monitored_operations();

    for (const operation of operations) {
      const trends = await monitor_performance_trends(
        operation.name,
        time_window_hours: 168 // 1 week
      );

      // 2. Predictive alerting
      if (trends.predictions.next_7d.includes_alert) {
        await send_alert({
          operation: operation.name,
          alert: trends.predictions.next_7d.alert,
          recommended_actions: trends.recommended_actions
        });
      }
    }

    // 3. Generate weekly report
    const report = await get_optimization_report(time_window_hours: 168);
    await distribute_report(report);
  }
  ```

  ## Best Practices

  ### 1. Always Establish Baseline First
  - Never optimize without knowing current performance
  - Baselines provide objective measurement for improvement
  - Include sufficient sample size (minimum 100 measurements)

  ### 2. Prioritize by ROI
  - Focus on high-impact, low-effort optimizations first
  - ROI score = (expected_improvement / complexity)
  - Defer low-ROI optimizations until high-ROI ones are complete

  ### 3. Use A/B Testing for Validation
  - Always validate optimizations with A/B testing
  - Require statistical significance (p-value < 0.05)
  - Use sufficient sample size (minimum 50 per group)

  ### 4. Monitor Continuously
  - Performance can degrade over time
  - Enable predictive alerts for proactive action
  - Review trends weekly for emerging issues

  ### 5. Document and Share Results
  - Generate regular optimization reports
  - Share success stories with team
  - Build institutional knowledge of what works

  ## Common Optimization Patterns

  ### Database Optimizations
  ```typescript
  // N+1 Query Detection and Resolution
  identify_optimization_opportunities("database_queries")
  // → Detects N+1 patterns
  // → Recommends eager loading
  // → Expected improvement: 70-90%

  // Index Optimization
  identify_optimization_opportunities("database_queries")
  // → Analyzes query patterns
  // → Recommends missing indexes
  // → Expected improvement: 40-60%

  // Query Plan Optimization
  apply_performance_optimization("slow_query", "query_rewrite")
  // → Rewrites inefficient queries
  // → Validates with EXPLAIN ANALYZE
  // → Measures actual improvement
  ```

  ### Caching Strategies
  ```typescript
  // Cache Hit Rate Optimization
  identify_optimization_opportunities("api_endpoint")
  // → Analyzes request patterns
  // → Recommends caching strategy
  // → Expected improvement: 50-80%

  // Cache Invalidation Strategy
  apply_performance_optimization("cache_layer", "invalidation_strategy")
  // → Implements smart invalidation
  // → Validates cache consistency
  // → Measures hit rate improvement
  ```

  ### Resource Utilization
  ```typescript
  // Memory Leak Detection
  monitor_performance_trends(time_window_hours: 48)
  // → Detects gradual memory increase
  // → Predicts OOM timeline
  // → Recommends investigation areas

  // CPU Optimization
  identify_optimization_opportunities("cpu_intensive_operation")
  // → Identifies inefficient algorithms
  // → Recommends algorithmic improvements
  // → Expected improvement: 30-70%
  ```

  ## Success Metrics

  Track these metrics to measure Performance Intelligence effectiveness:

  1. **Optimization Success Rate**: >85% of applied optimizations show significant improvement
  2. **ROI Accuracy**: Actual improvement within ±20% of predicted improvement
  3. **False Positive Rate**: <10% of identified opportunities are not actionable
  4. **Prediction Accuracy**: Performance predictions within ±15% of actual values
  5. **Time to Optimize**: Average time from identification to deployment <4 hours
  6. **Cumulative Performance Gain**: >50% improvement in critical path operations
  7. **Cost Savings**: Annual savings >20x implementation cost

  ## Troubleshooting

  ### Issue: Baseline establishment fails
  **Cause**: Insufficient sample size or high variance
  **Solution**: Increase duration_minutes or filter outliers

  ### Issue: A/B test shows no significance
  **Cause**: Small sample size or minimal actual improvement
  **Solution**: Increase test duration or investigate if optimization is effective

  ### Issue: Optimization shows improvement but p-value > 0.05
  **Cause**: High variance or small effect size
  **Solution**: Run longer test or accept as "promising but needs validation"

  ### Issue: Predictions are inaccurate
  **Cause**: Non-linear trends or insufficient historical data
  **Solution**: Collect more data or use shorter prediction windows

  ### Issue: ROI scores don't match actual results
  **Cause**: Complexity or impact estimates were inaccurate
  **Solution**: Calibrate estimates based on historical actual results
