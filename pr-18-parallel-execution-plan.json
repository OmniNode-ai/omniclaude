{
  "plan_version": "2.0",
  "created": "2025-10-25",
  "pr_number": 18,
  "plan_type": "comprehensive_remaining_issues",
  "note": "This plan addresses ALL remaining issues after commit 5552d2d. Previous plan (v1.0) addressed M1-M5 which are already fixed.",

  "total_work_packages": 7,
  "total_issues_addressed": 15,
  "estimated_total_hours_sequential": 10.5,
  "estimated_total_hours_parallel": 4.75,
  "parallelization_efficiency": "2.2x speedup (4.75h vs 10.5h)",
  "speedup_explanation": "7 packages run in parallel, limited by longest package (WP2: 4.0h)",

  "work_packages": [
    {
      "package_id": "WP1",
      "polly_role": "agent-code-quality-specialist",
      "title": "Platform Compatibility & Execution Logger Enhancements",
      "priority": "critical",
      "estimated_hours": 2.5,
      "issues_fixed": ["C1", "N3", "N10"],
      "issue_details": {
        "C1": {
          "severity": "critical",
          "description": "Hardcoded /tmp directory - platform incompatibility (Windows fails)",
          "impact": "Agent execution logger completely broken on Windows and restricted environments"
        },
        "N3": {
          "severity": "minor",
          "description": "Missing enum for ExecutionStatus - use omnibase_core.EnumOperationStatus",
          "impact": "Type safety and validation missing"
        },
        "N10": {
          "severity": "minor",
          "description": "Progress percent not validated (should be 0-100)",
          "impact": "Invalid progress values not caught"
        }
      },
      "files_to_modify": [
        "/Volumes/PRO-G40/Code/omniclaude/agents/lib/agent_execution_logger.py"
      ],
      "dependencies": [],
      "parallel_safe": true,

      "key_changes": [
        {
          "change_id": "C1-fix",
          "lines": "31-32",
          "description": "Replace hardcoded /tmp with platform-appropriate temp directory",
          "before": "FALLBACK_LOG_DIR = Path(\"/tmp/omniclaude_logs\")\nFALLBACK_LOG_DIR.mkdir(exist_ok=True)",
          "after": "Lazy initialization function using tempfile.gettempdir() with fallback",
          "specific_steps": [
            "Import tempfile module",
            "Create _get_fallback_log_dir() function with platform detection",
            "Use tempfile.gettempdir() for base directory",
            "Add fallback to .omniclaude_logs in current directory if temp fails",
            "Add write test to verify directory is actually writable",
            "Replace FALLBACK_LOG_DIR constant with get_fallback_log_dir() function",
            "Update _write_fallback_log() to call get_fallback_log_dir()"
          ]
        },
        {
          "change_id": "N3-fix",
          "lines": "246",
          "description": "Use EnumOperationStatus from omnibase_core instead of string",
          "specific_steps": [
            "Add import: from omnibase_core.enums.enum_operation_status import EnumOperationStatus",
            "Update complete() method signature: status: EnumOperationStatus = EnumOperationStatus.SUCCESS",
            "Update all status comparisons to use enum",
            "Convert enum to .value when storing in database"
          ]
        },
        {
          "change_id": "N10-fix",
          "lines": "160-171",
          "description": "Add validation for progress percent (0-100 range)",
          "specific_steps": [
            "Add validation check: if percent is not None and not (0 <= percent <= 100)",
            "Log warning if out of range",
            "Return early without database update if invalid"
          ]
        }
      ],

      "test_requirements": [
        "Test on Windows (verify temp directory detection works)",
        "Test with read-only /tmp (verify fallback to .omniclaude_logs)",
        "Test concurrent imports (no race conditions)",
        "Test progress validation (reject percent > 100 or < 0)",
        "Test EnumOperationStatus usage in complete() method",
        "Run: pytest agents/tests/ -k agent_execution_logger -v"
      ],

      "success_criteria": [
        "Module imports successfully on Windows without /tmp",
        "All existing tests pass",
        "No module-level directory creation side effects",
        "Status parameter uses EnumOperationStatus type hints",
        "Progress percent validation logs warnings for invalid values",
        "Fallback directory is platform-appropriate (Windows: %TEMP%, Unix: /tmp or ~/.omniclaude_logs)"
      ],

      "imports_required": [
        "import tempfile",
        "from omnibase_core.enums.enum_operation_status import EnumOperationStatus"
      ]
    },

    {
      "package_id": "WP2",
      "polly_role": "agent-debug-intelligence",
      "title": "Kafka Consumer Reliability & Health Check Safety",
      "priority": "critical",
      "estimated_hours": 4.0,
      "issues_fixed": ["C2", "M6"],
      "issue_details": {
        "C2": {
          "severity": "critical",
          "description": "Kafka infinite retry loop - no offset commits on failure",
          "impact": "One malformed message causes infinite retry loop, consumer becomes stuck"
        },
        "M6": {
          "severity": "major",
          "description": "Health check race condition (TOCTOU vulnerability)",
          "impact": "Health check could return 200 when consumer is actually dead"
        }
      },
      "files_to_modify": [
        "/Volumes/PRO-G40/Code/omniclaude/consumers/agent_actions_consumer.py"
      ],
      "dependencies": [],
      "parallel_safe": true,

      "key_changes": [
        {
          "change_id": "C2-fix",
          "lines": "666-673",
          "description": "Add retry limits and offset commits to prevent infinite loops",
          "specific_steps": [
            "Add __init__ attributes: self.retry_counts = {}, self.max_retries = 3, self.backoff_base_ms = 100",
            "Modify process_batch() exception handler to track retries per message",
            "Add message key format: f'{topic}:{partition}:{offset}'",
            "Check retry count before reprocessing",
            "Apply exponential backoff: backoff_ms = base * (2 ** retry_count)",
            "After max_retries exceeded: send to DLQ AND commit offset",
            "CRITICAL: Use consumer.commit() with TopicPartition and OffsetAndMetadata",
            "Clean up retry_counts dict after successful DLQ send",
            "Add logging for retry attempts and DLQ sends"
          ]
        },
        {
          "change_id": "M6-fix",
          "lines": "131-135",
          "description": "Add threading lock for atomic health checks",
          "specific_steps": [
            "Import threading module",
            "Add __init__ attribute: self._health_lock = threading.Lock()",
            "Wrap health check conditions in: with self._health_lock:",
            "Make health check atomic (all 3 conditions checked under same lock)",
            "Optional: Create @property is_healthy for reusability"
          ]
        }
      ],

      "test_requirements": [
        "Test poison message handling (send malformed JSON to topic)",
        "Verify consumer doesn't hang on bad messages",
        "Verify messages sent to DLQ after max retries (3 attempts)",
        "Verify offsets committed after DLQ send (consumer moves past poison message)",
        "Test concurrent health checks during shutdown (100 parallel requests)",
        "Load test: parallel curl http://localhost:8080/health ::: {1..100}",
        "Verify no AttributeError or race conditions in health endpoint",
        "Measure retry backoff times (100ms, 200ms, 400ms)"
      ],

      "success_criteria": [
        "Consumer processes past poison messages (no infinite loop)",
        "Offsets committed after max retries reached",
        "Exponential backoff applied between retries (100ms, 200ms, 400ms)",
        "Health check returns consistent results under concurrent load",
        "No AttributeError or race conditions in health endpoint",
        "DLQ receives poison messages after retry limit",
        "Retry counts tracked and cleaned up correctly"
      ],

      "imports_required": [
        "import threading",
        "import time",
        "from kafka import TopicPartition, OffsetAndMetadata"
      ]
    },

    {
      "package_id": "WP3",
      "polly_role": "agent-testing-specialist",
      "title": "Fix Test AttributeError",
      "priority": "critical",
      "estimated_hours": 0.25,
      "issues_fixed": ["C3"],
      "issue_details": {
        "C3": {
          "severity": "critical",
          "description": "Test failure - AttributeError accessing private _producer attribute",
          "impact": "Test suite fails, blocks CI/CD pipeline"
        }
      },
      "files_to_modify": [
        "/Volumes/PRO-G40/Code/omniclaude/agents/tests/test_intelligence_event_client.py"
      ],
      "dependencies": [],
      "parallel_safe": true,

      "key_changes": [
        {
          "change_id": "C3-fix",
          "lines": "963-970",
          "description": "Fix private attribute access in test",
          "specific_steps": [
            "Line ~963: Change client.producer.send_and_wait to client._producer.send_and_wait",
            "Line ~970: Update mock assignment to use _producer (private attribute)",
            "Verify test passes after change"
          ]
        }
      ],

      "test_requirements": [
        "Run specific test: pytest agents/tests/test_intelligence_event_client.py -v",
        "Verify no AttributeError",
        "Verify test logic still works correctly",
        "Verify all assertions pass"
      ],

      "success_criteria": [
        "Test passes without AttributeError",
        "All assertions in test still valid",
        "No regression in other tests in same file"
      ],

      "imports_required": []
    },

    {
      "package_id": "WP4",
      "polly_role": "agent-code-quality-specialist",
      "title": "Fix Session ID Type Mismatch",
      "priority": "critical",
      "estimated_hours": 0.5,
      "issues_fixed": ["C4"],
      "issue_details": {
        "C4": {
          "severity": "critical",
          "description": "Session ID defaults to 'unknown' string instead of UUID (violates DB schema)",
          "impact": "Database inserts fail with schema violation, analytics grouping broken"
        }
      },
      "files_to_modify": [
        "/Volumes/PRO-G40/Code/omniclaude/claude_hooks/user-prompt-submit.sh"
      ],
      "dependencies": [],
      "parallel_safe": true,

      "key_changes": [
        {
          "change_id": "C4-fix",
          "lines": "136",
          "description": "Generate UUID instead of using 'unknown' string",
          "before": "SESSION_ID=\"${CLAUDE_SESSION_ID:-unknown}\"",
          "after": "SESSION_ID=\"${CLAUDE_SESSION_ID:-$(uuidgen)}\"",
          "specific_steps": [
            "Change default from 'unknown' to $(uuidgen)",
            "Verify uuidgen is available (standard on macOS/Linux)",
            "Test with CLAUDE_SESSION_ID set (should use provided value)",
            "Test with CLAUDE_SESSION_ID unset (should generate UUID)"
          ]
        }
      ],

      "test_requirements": [
        "Test with CLAUDE_SESSION_ID set (should use provided value)",
        "Test with CLAUDE_SESSION_ID unset (should generate UUID)",
        "Verify generated UUID is valid format (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx)",
        "Test script execution end-to-end",
        "Verify database inserts succeed (no schema violation)"
      ],

      "success_criteria": [
        "SESSION_ID is always a valid UUID",
        "Database inserts succeed (no schema violation)",
        "Analytics queries don't group all sessions as 'unknown'",
        "Script works on macOS and Linux",
        "No errors when CLAUDE_SESSION_ID is unset"
      ],

      "imports_required": []
    },

    {
      "package_id": "WP5",
      "polly_role": "agent-database-specialist",
      "title": "SQL Migration Rollback & View Fix",
      "priority": "major",
      "estimated_hours": 2.0,
      "issues_fixed": ["M7", "M8"],
      "issue_details": {
        "M7": {
          "severity": "major",
          "description": "Missing rollback migration for safe reversion",
          "impact": "Cannot safely rollback if migration causes production issues"
        },
        "M8": {
          "severity": "major",
          "description": "SQL view queries non-existent action_details->>'status' field",
          "impact": "View returns zero rows, unusable for analytics"
        }
      },
      "files_to_modify": [
        "/Volumes/PRO-G40/Code/omniclaude/sql/migrations/001_add_project_context_to_observability_tables.sql"
      ],
      "files_to_create": [
        "/Volumes/PRO-G40/Code/omniclaude/sql/migrations/001_add_project_context_to_observability_tables_down.sql"
      ],
      "dependencies": [],
      "parallel_safe": true,

      "key_changes": [
        {
          "change_id": "M7-fix",
          "description": "Create complete rollback migration",
          "specific_steps": [
            "Create new file: 001_add_project_context_to_observability_tables_down.sql",
            "BEGIN transaction",
            "DROP INDEX IF EXISTS (5 indexes)",
            "DROP VIEW IF EXISTS agent_activity_realtime",
            "DROP FUNCTION IF EXISTS extract_project_name",
            "ALTER TABLE DROP COLUMN IF EXISTS for all 5 tables",
            "COMMIT transaction",
            "Make idempotent with IF EXISTS checks"
          ]
        },
        {
          "change_id": "M8-fix",
          "lines": "109-133",
          "description": "Fix view to use action_type column instead of non-existent status field",
          "before": "COUNT(*) FILTER (WHERE action_details->>'status' = 'started')",
          "after": "COUNT(*) FILTER (WHERE action_type = 'tool_call')",
          "specific_steps": [
            "Replace action_details->>'status' = 'started' with action_type = 'tool_call'",
            "Replace action_details->>'status' = 'completed' with action_type = 'success'",
            "Replace action_details->>'status' = 'failed' with action_type = 'error'",
            "Update view to query existing columns only"
          ]
        }
      ],

      "test_requirements": [
        "Run UP migration on test database: psql -f 001_...sql",
        "Verify all columns, indexes, views created",
        "Run DOWN migration: psql -f 001_..._down.sql",
        "Verify all changes rolled back cleanly",
        "Run UP migration again (test idempotency)",
        "Query view and verify it returns data (not zero rows)",
        "Test view with sample data (verify counts are correct)",
        "Run DOWN migration twice (test idempotency of rollback)"
      ],

      "success_criteria": [
        "DOWN migration successfully rolls back all changes",
        "DOWN migration is idempotent (can run multiple times)",
        "UP migration works after DOWN migration",
        "View queries return accurate counts using action_type",
        "No SQL errors during migration or rollback",
        "View is actually usable (not returning zero rows)",
        "All 5 tables properly updated and rolled back"
      ],

      "imports_required": []
    },

    {
      "package_id": "WP6",
      "polly_role": "agent-code-quality-specialist",
      "title": "Intelligence Analysis Enhancements",
      "priority": "minor",
      "estimated_hours": 0.5,
      "issues_fixed": ["N4", "N9"],
      "issue_details": {
        "N4": {
          "severity": "minor",
          "description": "Missing enum for OperationType (uses string literals)",
          "impact": "Type safety missing, validation impossible"
        },
        "N9": {
          "severity": "minor",
          "description": "Monotonic timestamp instead of wall-clock time",
          "impact": "Timestamps not comparable across systems"
        }
      },
      "files_to_modify": [
        "/Volumes/PRO-G40/Code/omniclaude/analyze_intelligence.py"
      ],
      "dependencies": [],
      "parallel_safe": true,

      "key_changes": [
        {
          "change_id": "N4-fix",
          "lines": "47-51, 151-155",
          "description": "Create OperationType enum and use it",
          "specific_steps": [
            "Create OperationType enum with values: QUALITY_ASSESSMENT, PATTERN_EXTRACTION",
            "Replace string literal at line ~47-51 with OperationType.QUALITY_ASSESSMENT.value",
            "Replace string literal at line ~151-155 with OperationType.PATTERN_EXTRACTION.value",
            "Import Enum from enum module"
          ]
        },
        {
          "change_id": "N9-fix",
          "lines": "294",
          "description": "Replace monotonic timestamp with wall-clock time",
          "before": "\"timestamp\": asyncio.get_event_loop().time()",
          "after": "\"timestamp\": datetime.now(timezone.utc).isoformat()",
          "specific_steps": [
            "Import datetime and timezone from datetime module",
            "Replace asyncio.get_event_loop().time() with datetime.now(timezone.utc).isoformat()",
            "Verify timestamp is ISO 8601 format with timezone"
          ]
        }
      ],

      "test_requirements": [
        "Run analyze_intelligence.py with sample file",
        "Verify timestamps are ISO 8601 format with timezone (e.g., 2025-10-25T12:34:56.789+00:00)",
        "Verify OperationType enum values used correctly",
        "Check logs for proper timestamp formatting",
        "Test both QUALITY_ASSESSMENT and PATTERN_EXTRACTION operations"
      ],

      "success_criteria": [
        "OperationType enum defined and used consistently",
        "All timestamps are wall-clock time (not monotonic)",
        "Timestamps include timezone information (UTC)",
        "Script executes without errors",
        "Type checking passes (if enabled)",
        "Timestamps are comparable across systems"
      ],

      "imports_required": [
        "from datetime import datetime, timezone",
        "from enum import Enum"
      ]
    },

    {
      "package_id": "WP7",
      "polly_role": "agent-code-quality-specialist",
      "title": "Skills Enhancement - Parsing & Timestamps",
      "priority": "minor",
      "estimated_hours": 0.75,
      "issues_fixed": ["N1", "N5", "N14"],
      "issue_details": {
        "N1": {
          "severity": "minor",
          "description": "Boolean argument parsing bug (--success false evaluates to True)",
          "impact": "Cannot set success=False via CLI"
        },
        "N5": {
          "severity": "minor",
          "description": "Missing enum for ActionType",
          "impact": "Type safety and validation missing"
        },
        "N14": {
          "severity": "minor",
          "description": "Timezone-aware timestamps missing (datetime.utcnow() deprecated)",
          "impact": "Deprecation warnings, timezone-naive timestamps"
        }
      },
      "files_to_modify": [
        "/Volumes/PRO-G40/Code/omniclaude/skills/agent-tracking/log-agent-action/execute_unified.py",
        "/Volumes/PRO-G40/Code/omniclaude/skills/agent-tracking/log-agent-action/execute_kafka.py"
      ],
      "dependencies": [],
      "parallel_safe": true,

      "key_changes": [
        {
          "change_id": "N1-fix",
          "file": "execute_unified.py",
          "lines": "48-49",
          "description": "Fix boolean argument parsing",
          "before": "parser.add_argument(\"--success\", type=bool, default=True)",
          "after": "parser.add_argument(\"--success\", action=argparse.BooleanOptionalAction, default=True)",
          "specific_steps": [
            "Change --success argument to use action=BooleanOptionalAction",
            "This enables --success and --no-success flags",
            "Update help text to mention --no-success option"
          ]
        },
        {
          "change_id": "N5-fix",
          "file": "execute_unified.py",
          "lines": "65-88",
          "description": "Create ActionType enum for validation",
          "specific_steps": [
            "Create ActionType enum with values: TOOL_CALL, DECISION, ERROR, SUCCESS",
            "Use enum for --action-type choices validation",
            "Convert string to enum value when processing"
          ]
        },
        {
          "change_id": "N14-fix",
          "file": "execute_kafka.py",
          "lines": "167",
          "description": "Use timezone-aware timestamps",
          "before": "\"timestamp\": datetime.utcnow().isoformat()",
          "after": "\"timestamp\": datetime.now(timezone.utc).isoformat()",
          "specific_steps": [
            "Import timezone from datetime module",
            "Replace datetime.utcnow() with datetime.now(timezone.utc)",
            "Verify timestamp has timezone suffix (+00:00 or Z)"
          ]
        }
      ],

      "test_requirements": [
        "Test --success true and --no-success flags",
        "Test --success false (should now work correctly)",
        "Verify ActionType enum constrains action-type values",
        "Test invalid action-type value (should fail with clear error)",
        "Test timestamp generation (verify ISO 8601 with timezone)",
        "Run skill end-to-end: bash execute_kafka.py --action-type tool_call ...",
        "Verify no deprecation warnings"
      ],

      "success_criteria": [
        "--success false correctly sets success=False",
        "--no-success flag works correctly",
        "ActionType enum prevents invalid action types",
        "All timestamps are timezone-aware (UTC)",
        "No deprecation warnings for datetime.utcnow()",
        "Skills execute successfully with new changes",
        "CLI help text accurate"
      ],

      "imports_required": [
        "from datetime import datetime, timezone",
        "from enum import Enum",
        "import argparse"
      ]
    }
  ],

  "execution_order": {
    "phase_1_parallel": {
      "description": "All 7 work packages run in parallel (different files, zero conflicts)",
      "packages": ["WP1", "WP2", "WP3", "WP4", "WP5", "WP6", "WP7"],
      "estimated_duration": "4.0 hours (limited by WP2, the longest package)",
      "dispatch_strategy": "Launch all 7 Pollys in SINGLE message for true parallelism",
      "anti_pattern": "DO NOT launch Pollys sequentially in separate messages"
    },
    "phase_2_validation_parallel": {
      "description": "Run all validation tests in parallel",
      "tasks": [
        "Run all unit tests (pytest)",
        "Run integration tests",
        "Test on Windows platform (WP1)",
        "Test poison message handling (WP2)",
        "Test SQL migration up/down (WP5)",
        "Run linting and type checking",
        "Test bash hook with uuidgen (WP4)"
      ],
      "estimated_duration": "0.5 hours"
    },
    "phase_3_sequential_aggregation": {
      "description": "Aggregate results and finalize",
      "tasks": [
        "Review all test results",
        "Verify zero file conflicts",
        "Aggregate commits (optional - each Polly can commit independently)",
        "Update PR description with comprehensive changelog",
        "Run final pre-commit hooks"
      ],
      "estimated_duration": "0.25 hours"
    }
  },

  "shared_contracts": {
    "enum_strategy": {
      "use_existing_enums": {
        "ExecutionStatus": "from omnibase_core.enums.enum_operation_status import EnumOperationStatus",
        "OnexStatus": "from omnibase_core.enums.enum_onex_status import EnumOnexStatus",
        "note": "DO NOT create new ExecutionStatus enum - use EnumOperationStatus from omnibase_core"
      },
      "create_new_enums": {
        "OperationType": "WP6 creates this for analyze_intelligence.py (QUALITY_ASSESSMENT, PATTERN_EXTRACTION)",
        "ActionType": "WP7 creates this for log-agent-action skills (TOOL_CALL, DECISION, ERROR, SUCCESS)"
      }
    },
    "file_separation_strategy": "Each work package modifies completely different files - zero conflicts guaranteed",
    "branch_strategy": "All work on same branch feat/mvp-completion (no branching per Polly)",
    "testing_strategy": "Each Polly runs independent unit tests, then parallel integration tests in Phase 2",
    "commit_strategy": "Each Polly commits its changes independently with clear commit message",
    "timestamp_standard": "All timestamps must be timezone-aware UTC in ISO 8601 format",
    "error_handling_standard": "All timeout handlers must catch both TimeoutError and asyncio.TimeoutError"
  },

  "file_ownership_matrix": {
    "WP1_owns": ["agents/lib/agent_execution_logger.py"],
    "WP2_owns": ["consumers/agent_actions_consumer.py"],
    "WP3_owns": ["agents/tests/test_intelligence_event_client.py"],
    "WP4_owns": ["claude_hooks/user-prompt-submit.sh"],
    "WP5_owns": [
      "sql/migrations/001_add_project_context_to_observability_tables.sql",
      "sql/migrations/001_add_project_context_to_observability_tables_down.sql"
    ],
    "WP6_owns": ["analyze_intelligence.py"],
    "WP7_owns": [
      "skills/agent-tracking/log-agent-action/execute_unified.py",
      "skills/agent-tracking/log-agent-action/execute_kafka.py"
    ],
    "total_unique_files": 9,
    "conflicts": "NONE - all files are uniquely owned by single work package"
  },

  "risk_assessment": {
    "file_conflicts": "ZERO - all work packages modify different files",
    "integration_risk": "LOW - changes are isolated and well-defined",
    "testing_risk": "MEDIUM - need Windows testing for WP1, poison message testing for WP2",
    "rollback_risk": "LOW - WP5 creates rollback migration for safety",
    "performance_impact": "NONE - changes are fixes, not features",
    "breaking_changes": "NONE - all changes are backward compatible"
  },

  "quality_gates": {
    "pre_execution": [
      "Verify all Pollys have latest code from feat/mvp-completion branch",
      "Verify omnibase_core is installed with EnumOperationStatus available",
      "Verify test database is running and accessible (for WP5)",
      "Verify Kafka is running (for WP2 testing)",
      "Verify uuidgen command is available (for WP4)"
    ],
    "during_execution": [
      "Each Polly runs linting after changes (ruff, black, isort)",
      "Each Polly runs unit tests after changes (pytest)",
      "Each Polly verifies no new errors or warnings introduced",
      "Each Polly commits with descriptive message on success"
    ],
    "post_execution": [
      "All unit tests pass (pytest)",
      "All integration tests pass",
      "Windows platform tests pass (WP1)",
      "Kafka poison message tests pass (WP2)",
      "SQL migration roundtrip tests pass (WP5)",
      "Pre-commit hooks pass (all files)",
      "No linting errors (ruff, black, isort)",
      "No type checking errors (mypy if enabled)",
      "All 15 issues verified fixed"
    ]
  },

  "estimated_timeline": {
    "sequential_execution": "10.5 hours (2.5 + 4.0 + 0.25 + 0.5 + 2.0 + 0.5 + 0.75)",
    "parallel_execution_phase_1": "4.0 hours (longest work package is WP2)",
    "parallel_validation_phase_2": "0.5 hours (all tests run in parallel)",
    "sequential_aggregation_phase_3": "0.25 hours (review and finalize)",
    "total_with_parallelization": "4.75 hours (4.0 + 0.5 + 0.25)",
    "speedup_factor": "2.2x (10.5h sequential â†’ 4.75h parallel)",
    "note": "Actual speedup depends on Polly availability, machine resources, and test environment"
  },

  "success_metrics": {
    "issues_by_priority": {
      "critical_fixed": "C1, C2, C3, C4 (4 issues)",
      "major_fixed": "M6, M7, M8 (3 issues)",
      "minor_fixed": "N1, N3, N4, N5, N9, N10, N14 (7 issues)",
      "total_fixed": "15 issues across 3 priority levels"
    },
    "test_coverage": {
      "test_pass_rate": "100% (all existing tests must pass)",
      "platform_coverage": "macOS, Linux, Windows (WP1 enables Windows support)",
      "new_tests_added": "Poison message handling tests (WP2), migration rollback tests (WP5)"
    },
    "code_quality": {
      "zero_file_conflicts": "Guaranteed by file separation strategy",
      "enum_usage": "3 enums used/created (EnumOperationStatus, OperationType, ActionType)",
      "timezone_aware_timestamps": "100% (all timestamps now UTC with timezone)",
      "deprecated_code_removed": "datetime.utcnow() replaced everywhere"
    }
  },

  "polly_dispatch_instructions": {
    "critical_requirement": "Launch all 7 Pollys in SINGLE message for true parallelism",
    "correct_dispatch_example": "In ONE message, use 7 Task tool calls: Task(WP1), Task(WP2), Task(WP3), Task(WP4), Task(WP5), Task(WP6), Task(WP7)",
    "anti_pattern_warning": "DO NOT launch Pollys sequentially in separate messages - destroys parallelism benefits",
    "expected_behavior": "All 7 Pollys start simultaneously, work independently on different files, commit independently",
    "coordination": "No coordination needed - file separation ensures zero conflicts"
  },

  "lessons_learned_from_omniarchon": {
    "what_worked": [
      "File separation strategy - each Polly gets distinct files = zero conflicts",
      "Single branch strategy - no merge complexity, all work on feat/mvp-completion",
      "Clear interfaces defined upfront - enums, imports, contracts specified before execution",
      "Simultaneous launch - all Pollys in single message = maximum parallelism",
      "Independent testing - each Polly can test immediately without waiting"
    ],
    "anti_patterns_avoided": [
      "No branch-per-Polly (causes merge hell)",
      "No sequential Polly dispatch (destroys parallelism)",
      "No late interface definition (causes integration failures)",
      "No integration-dependent tests (blocks parallel testing)"
    ]
  }
}
