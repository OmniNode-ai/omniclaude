# Universal Agent Router Architecture

**Status**: Planning Phase
**Blocked By**: PR #22 (Observability infrastructure)
**Target Start**: After PR #22 merges
**Estimated Duration**: 4 weeks (staged rollout)
**Last Updated**: 2025-11-07

---

## Executive Summary

The Universal Agent Router is a **framework-agnostic**, **multi-protocol** routing service that provides intelligent agent selection with GPU-accelerated inference, multi-tier caching, and comprehensive observability. It's designed to serve ANY agent framework (Claude Code, AutoGPT, LangChain, CrewAI, etc.) through a unified API gateway.

**Key Value Propositions**:
- âš¡ **20-50ms GPU-accelerated routing** using vLLM on RTX 5090
- ğŸ’° **Cost optimization**: $4 per 1M requests vs $3000 all-Anthropic baseline
- ğŸ¯ **60-70% cache hit rate** with Valkey L1 cache
- ğŸŒ **Multi-protocol support**: HTTP, gRPC, Kafka, WebSocket
- ğŸ“Š **Complete observability**: Correlation tracking across all tiers
- ğŸ”„ **Graceful degradation**: 4-tier fallback cascade
- ğŸ—ï¸ **Framework-agnostic**: Universal API works with any agent system

---

## Architecture Overview

### Multi-Tier Routing Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT REQUEST                               â”‚
â”‚         (HTTP/gRPC/Kafka/WebSocket)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 1: VALKEY CACHE (L1)                                     â”‚
â”‚  â€¢ Exact match lookups                                          â”‚
â”‚  â€¢ <1ms response time                                           â”‚
â”‚  â€¢ 60-70% hit rate (estimated)                                  â”‚
â”‚  â€¢ TTL: 1 hour for routing decisions                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ CACHE MISS
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 2: vLLM LOCAL GPU (RTX 5090)                             â”‚
â”‚  â€¢ Location: 192.168.86.200:11434                               â”‚
â”‚  â€¢ Model: Llama-3.1-8B-Instruct (quantized)                     â”‚
â”‚  â€¢ Performance: 20-50ms routing time                            â”‚
â”‚  â€¢ Throughput: 100+ req/s                                       â”‚
â”‚  â€¢ Cost: ~$0.000004 per request (electricity)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ GPU UNAVAILABLE / LOW CONFIDENCE
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 3: FUZZY FALLBACK (CPU)                                  â”‚
â”‚  â€¢ Rapid-fuzz algorithm matching                                â”‚
â”‚  â€¢ Agent registry pattern matching                              â”‚
â”‚  â€¢ Performance: 5-10ms                                          â”‚
â”‚  â€¢ Confidence: 0.4-0.7 typical                                  â”‚
â”‚  â€¢ Cost: Free (local CPU)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ LOW CONFIDENCE (<0.6)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 4: REMOTE LLM (Anthropic/Gemini/Together)                â”‚
â”‚  â€¢ High-quality routing for complex queries                     â”‚
â”‚  â€¢ Performance: 200-500ms                                       â”‚
â”‚  â€¢ Cost: $0.003 per request (Anthropic Haiku)                  â”‚
â”‚  â€¢ Use: <5% of requests (fallback only)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Projections

| Tier | Hit Rate | Avg Latency | Cost per Request | Annual Cost (1M req) |
|------|----------|-------------|------------------|---------------------|
| **Valkey Cache** | 60-70% | <1ms | $0 | $0 |
| **vLLM GPU** | 25-35% | 20-50ms | $0.000004 | $4 |
| **Fuzzy Fallback** | 3-5% | 5-10ms | $0 | $0 |
| **Remote LLM** | <5% | 200-500ms | $0.003 | $3000 |
| **WEIGHTED AVERAGE** | 100% | **~36ms** | **$0.000004** | **~$4** |

**Cost Comparison**:
- Current (all-Anthropic): $3000 per 1M requests
- Universal Router: ~$4 per 1M requests
- **Savings**: 99.87% cost reduction

---

## Multi-Protocol API Gateway

### Protocol Support Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MULTI-PROTOCOL GATEWAY                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   HTTP     â”‚  â”‚   gRPC     â”‚  â”‚   Kafka    â”‚  â”‚ WebSocket  â”‚â”‚
â”‚  â”‚  REST API  â”‚  â”‚  Binary    â”‚  â”‚  Event Bus â”‚  â”‚  Real-time â”‚â”‚
â”‚  â”‚            â”‚  â”‚  Protocol  â”‚  â”‚  Async     â”‚  â”‚  Streaming â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â”‚
â”‚         â”‚                â”‚                â”‚                â”‚      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                              â”‚                                    â”‚
â”‚                              â–¼                                    â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                   â”‚  UNIFIED ROUTER CORE â”‚                        â”‚
â”‚                   â”‚  â€¢ Request validationâ”‚                        â”‚
â”‚                   â”‚  â€¢ Correlation IDs   â”‚                        â”‚
â”‚                   â”‚  â€¢ Tier orchestrationâ”‚                        â”‚
â”‚                   â”‚  â€¢ Response mapping  â”‚                        â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Protocol Specifications

#### HTTP/REST API

**Endpoint**: `POST /v1/route`

**Request**:
```json
{
  "request": "Help me implement ONEX node patterns",
  "context": {
    "agent_framework": "claude-code",
    "user_id": "user-123",
    "session_id": "session-456"
  },
  "options": {
    "max_recommendations": 3,
    "min_confidence": 0.7,
    "include_explanations": true
  }
}
```

**Response**:
```json
{
  "correlation_id": "550e8400-e29b-41d4-a716-446655440000",
  "recommendations": [
    {
      "agent_name": "agent-onex-architect",
      "confidence_score": 0.95,
      "routing_tier": "vllm",
      "explanation": "Specialized ONEX architecture agent with proven patterns",
      "estimated_cost": 0.000004,
      "latency_ms": 35
    }
  ],
  "performance": {
    "total_latency_ms": 35,
    "tier_used": "vllm",
    "cache_hit": false
  }
}
```

#### gRPC API

**Service Definition** (Protocol Buffers):

```protobuf
syntax = "proto3";

package universal_router.v1;

service UniversalRouter {
  rpc Route(RouteRequest) returns (RouteResponse) {}
  rpc RouteStream(RouteRequest) returns (stream RouteResponse) {}
}

message RouteRequest {
  string request = 1;
  map<string, string> context = 2;
  RouteOptions options = 3;
}

message RouteResponse {
  string correlation_id = 1;
  repeated AgentRecommendation recommendations = 2;
  PerformanceMetrics performance = 3;
}

message AgentRecommendation {
  string agent_name = 1;
  double confidence_score = 2;
  string routing_tier = 3;
  string explanation = 4;
}

message PerformanceMetrics {
  int64 total_latency_ms = 1;
  string tier_used = 2;
  bool cache_hit = 3;
}
```

**Benefits**:
- Binary protocol (faster than JSON)
- Native streaming support
- Strong typing with code generation
- 30-50% lower bandwidth vs HTTP

#### Kafka Event Bus

**Topics**:
- `universal-router.routing.requested.v1` - Incoming routing requests
- `universal-router.routing.completed.v1` - Successful routing
- `universal-router.routing.failed.v1` - Failed routing

**Event Schema** (Avro):

```json
{
  "type": "record",
  "name": "RoutingRequest",
  "namespace": "universal_router.v1",
  "fields": [
    {"name": "correlation_id", "type": "string"},
    {"name": "request", "type": "string"},
    {"name": "context", "type": {"type": "map", "values": "string"}},
    {"name": "max_recommendations", "type": "int", "default": 3},
    {"name": "min_confidence", "type": "double", "default": 0.7},
    {"name": "timestamp", "type": "long", "logicalType": "timestamp-millis"}
  ]
}
```

**Use Cases**:
- Async routing for batch jobs
- Event-driven agent orchestration
- Cross-service communication
- Audit trail and replay

#### WebSocket API

**Connection**: `wss://universal-router:8070/v1/stream`

**Message Format**:
```json
{
  "type": "route_request",
  "payload": {
    "request": "Help me debug this error",
    "context": {"session_id": "ws-session-123"}
  }
}
```

**Response Stream**:
```json
{
  "type": "route_progress",
  "payload": {
    "stage": "tier_vllm",
    "message": "Querying local GPU...",
    "progress": 0.5
  }
}

{
  "type": "route_complete",
  "payload": {
    "recommendations": [...],
    "performance": {...}
  }
}
```

**Use Cases**:
- Real-time agent coordination
- Live routing feedback
- Interactive debugging
- UI dashboards

---

## vLLM Integration

### Infrastructure

**Location**: `192.168.86.200:11434` (existing Ollama/vLLM endpoint)

**Hardware**:
- **GPU**: NVIDIA RTX 5090 (32GB VRAM)
- **Model**: Llama-3.1-8B-Instruct (quantized to 4-bit)
- **Memory**: ~8GB VRAM required
- **Throughput**: 100+ requests/second
- **Latency**: 20-50ms per request

### Model Configuration

**vLLM Launch Parameters**:
```bash
vllm serve meta-llama/Llama-3.1-8B-Instruct \
  --host 0.0.0.0 \
  --port 11434 \
  --tensor-parallel-size 1 \
  --dtype float16 \
  --quantization awq \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.8 \
  --enable-prefix-caching \
  --disable-log-requests
```

### Routing Prompt Template

```python
ROUTING_PROMPT = """You are an expert agent router. Analyze the user request and select the best agent.

Available Agents:
{agent_registry_json}

User Request: {user_request}

Context: {context_json}

Respond in JSON format:
{
  "agent_name": "selected-agent-name",
  "confidence_score": 0.95,
  "reasoning": "Brief explanation why this agent is best suited"
}

Requirements:
- confidence_score must be 0.0-1.0
- Only select agents from the provided registry
- Consider context (framework, user history, task complexity)
"""
```

### Performance Optimization

**Prefix Caching**:
- Cache agent registry (common prefix)
- Reduces repeated prompt processing
- 30-40% latency improvement for cache hits

**Batching**:
- Automatic request batching (up to 32 requests)
- Improves GPU utilization
- Maintains <50ms p95 latency

**Connection Pooling**:
- Persistent HTTP/2 connections
- Reduces connection overhead
- Async client with aiohttp

---

## Valkey Caching Strategy

### Cache Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VALKEY CACHE (L1)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Cache Key Format:                                              â”‚
â”‚    router:v1:{hash(request_text + context_fingerprint)}         â”‚
â”‚                                                                 â”‚
â”‚  Value Structure (JSON):                                        â”‚
â”‚  {                                                              â”‚
â”‚    "agent_name": "agent-onex-architect",                        â”‚
â”‚    "confidence_score": 0.95,                                    â”‚
â”‚    "routing_tier": "vllm",                                      â”‚
â”‚    "cached_at": "2025-11-07T14:30:00Z",                         â”‚
â”‚    "hit_count": 142                                             â”‚
â”‚  }                                                              â”‚
â”‚                                                                 â”‚
â”‚  TTL: 3600 seconds (1 hour)                                     â”‚
â”‚  Max Memory: 512MB                                              â”‚
â”‚  Eviction: LRU (Least Recently Used)                            â”‚
â”‚  Hit Rate: 60-70% (estimated)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cache Invalidation Strategy

**Invalidation Triggers**:
1. **Agent Registry Updates** - Invalidate all cached routes
2. **Low Confidence Routing** - Skip cache for confidence <0.7
3. **Explicit Cache Bust** - Client-provided `cache=false` flag
4. **Time-Based Expiry** - 1 hour TTL default

**Invalidation Pattern**:
```python
# Pattern-based invalidation
await valkey.delete_pattern("router:v1:*")

# Selective invalidation (agent update)
await valkey.delete_pattern(f"router:v1:*:{agent_name}:*")

# Full cache flush (emergency)
await valkey.flushdb()
```

### Cache Warming

**Warm Cache on Startup**:
```python
# Top 100 most common routing requests
COMMON_REQUESTS = [
    "Help me implement ONEX patterns",
    "Debug this error message",
    "Review my pull request",
    # ... 97 more
]

for request in COMMON_REQUESTS:
    # Pre-populate cache with vLLM routing
    result = await route_with_vllm(request)
    await cache_set(request, result, ttl=3600)
```

**Benefits**:
- 90%+ cache hit rate for first hour
- Reduces cold-start latency
- Improves user experience

---

## Framework-Agnostic Design

### Universal Request Format

All agent frameworks map to a common request format:

```python
@dataclass
class UniversalRouteRequest:
    """Framework-agnostic routing request."""

    # Required fields
    request: str  # User's natural language request

    # Optional context
    context: Dict[str, Any] = field(default_factory=dict)
    # Examples:
    #   {"agent_framework": "claude-code"}
    #   {"agent_framework": "autogpt"}
    #   {"agent_framework": "langchain"}
    #   {"agent_framework": "crewai"}

    # Routing options
    max_recommendations: int = 3
    min_confidence: float = 0.7
    include_explanations: bool = False

    # Observability
    correlation_id: Optional[str] = None
    session_id: Optional[str] = None
    user_id: Optional[str] = None
```

### Framework Adapters

**Claude Code Adapter**:
```python
class ClaudeCodeAdapter:
    """Adapter for Claude Code agent framework."""

    async def route(self, claude_request: str) -> RouteResponse:
        """Convert Claude Code request to universal format."""
        universal_request = UniversalRouteRequest(
            request=claude_request,
            context={"agent_framework": "claude-code"},
            correlation_id=generate_correlation_id()
        )
        return await universal_router.route(universal_request)
```

**LangChain Adapter**:
```python
class LangChainAdapter:
    """Adapter for LangChain agent framework."""

    async def route(self, langchain_input: Dict) -> AgentExecutor:
        """Convert LangChain input to universal format."""
        universal_request = UniversalRouteRequest(
            request=langchain_input["query"],
            context={"agent_framework": "langchain"},
            session_id=langchain_input.get("session_id")
        )
        response = await universal_router.route(universal_request)
        return self._create_langchain_executor(response)
```

**AutoGPT Adapter**:
```python
class AutoGPTAdapter:
    """Adapter for AutoGPT agent framework."""

    async def route(self, autogpt_goal: str) -> Agent:
        """Convert AutoGPT goal to universal format."""
        universal_request = UniversalRouteRequest(
            request=autogpt_goal,
            context={"agent_framework": "autogpt"},
            max_recommendations=1  # AutoGPT uses single agent
        )
        response = await universal_router.route(universal_request)
        return self._create_autogpt_agent(response)
```

### Agent Registry Format

Framework-agnostic agent definitions:

```yaml
# ~/.universal-router/agents/agent-onex-architect.yaml
agent_name: agent-onex-architect
version: 1.0.0
capabilities:
  - ONEX architecture design
  - 4-node pattern implementation
  - Contract schema design
keywords:
  - onex
  - architecture
  - node design
  - effect
  - compute
  - reducer
  - orchestrator
compatible_frameworks:
  - claude-code
  - langchain
  - crewai
performance:
  avg_routing_time_ms: 35
  confidence_threshold: 0.7
metadata:
  created_at: "2025-11-01T00:00:00Z"
  author: "Jonah Gray"
  description: "Specialized agent for ONEX architecture patterns"
```

---

## Observability & Traceability

### Correlation ID Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLIENT REQUEST                                              â”‚
â”‚  correlation_id: 550e8400-e29b-41d4-a716-446655440000        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 1: VALKEY CACHE                                        â”‚
â”‚  â€¢ Log: Cache lookup attempt                                 â”‚
â”‚  â€¢ Track: Cache hit/miss                                     â”‚
â”‚  â€¢ Duration: <1ms                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ (on cache miss)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 2: vLLM GPU                                            â”‚
â”‚  â€¢ Log: GPU inference request                                â”‚
â”‚  â€¢ Track: Token count, GPU utilization                       â”‚
â”‚  â€¢ Duration: 20-50ms                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ (on low confidence)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 3: FUZZY FALLBACK                                      â”‚
â”‚  â€¢ Log: Fuzzy matching attempt                               â”‚
â”‚  â€¢ Track: Match score, algorithm used                        â”‚
â”‚  â€¢ Duration: 5-10ms                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ (on low confidence)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIER 4: REMOTE LLM                                          â”‚
â”‚  â€¢ Log: External API call                                    â”‚
â”‚  â€¢ Track: Provider, model, cost                              â”‚
â”‚  â€¢ Duration: 200-500ms                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RESPONSE + OBSERVABILITY DATA                               â”‚
â”‚  â€¢ Total latency: sum of all tiers                           â”‚
â”‚  â€¢ Cost: tier-specific cost calculation                      â”‚
â”‚  â€¢ Logged to PostgreSQL: universal_router_events             â”‚
â”‚  â€¢ Published to Kafka: universal-router.events.v1            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Database Schema

```sql
CREATE TABLE universal_router_events (
    id BIGSERIAL PRIMARY KEY,

    -- Request identification
    correlation_id UUID NOT NULL,
    session_id VARCHAR(255),
    user_id VARCHAR(255),
    agent_framework VARCHAR(50),

    -- Request details
    request_text TEXT NOT NULL,
    request_context JSONB,

    -- Routing results
    agent_name VARCHAR(255),
    confidence_score NUMERIC(5,4),
    routing_tier VARCHAR(20),  -- 'cache', 'vllm', 'fuzzy', 'remote'

    -- Performance metrics
    total_latency_ms INTEGER,
    cache_hit BOOLEAN,
    vllm_latency_ms INTEGER,
    fuzzy_latency_ms INTEGER,
    remote_latency_ms INTEGER,

    -- Cost tracking
    estimated_cost_usd NUMERIC(10,8),

    -- Timestamps
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    -- Indexes
    INDEX idx_correlation_id (correlation_id),
    INDEX idx_agent_framework (agent_framework),
    INDEX idx_routing_tier (routing_tier),
    INDEX idx_created_at (created_at DESC)
);

-- Aggregated metrics view
CREATE MATERIALIZED VIEW universal_router_metrics AS
SELECT
    agent_framework,
    routing_tier,
    COUNT(*) as request_count,
    AVG(total_latency_ms) as avg_latency_ms,
    AVG(confidence_score) as avg_confidence,
    SUM(estimated_cost_usd) as total_cost_usd,
    AVG(CASE WHEN cache_hit THEN 1 ELSE 0 END) as cache_hit_rate
FROM universal_router_events
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY agent_framework, routing_tier;
```

### Prometheus Metrics

```python
from prometheus_client import Counter, Histogram, Gauge

# Request counters
router_requests_total = Counter(
    'universal_router_requests_total',
    'Total routing requests',
    ['agent_framework', 'routing_tier', 'status']
)

# Latency histograms
router_latency_seconds = Histogram(
    'universal_router_latency_seconds',
    'Routing latency in seconds',
    ['routing_tier'],
    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]
)

# Cache hit rate
router_cache_hit_rate = Gauge(
    'universal_router_cache_hit_rate',
    'Cache hit rate (0.0-1.0)'
)

# GPU utilization
vllm_gpu_utilization = Gauge(
    'universal_router_vllm_gpu_utilization',
    'vLLM GPU utilization percentage (0-100)'
)

# Cost tracking
router_cost_usd_total = Counter(
    'universal_router_cost_usd_total',
    'Total routing cost in USD',
    ['routing_tier']
)
```

### Grafana Dashboards

**Key Metrics**:
1. **Routing Performance**
   - P50/P95/P99 latency by tier
   - Request rate (req/s)
   - Error rate

2. **Cost Analysis**
   - Cost per request by tier
   - Daily/weekly/monthly spend
   - Cost savings vs baseline

3. **Cache Performance**
   - Hit rate over time
   - Cache size and evictions
   - Top cached routes

4. **GPU Monitoring**
   - GPU utilization %
   - vLLM throughput (req/s)
   - GPU memory usage

5. **Tier Distribution**
   - Requests by tier (pie chart)
   - Tier fallback rate
   - Confidence score distribution

---

## Graceful Degradation

### Fallback Cascade Logic

```python
async def route_with_fallback(
    request: UniversalRouteRequest
) -> RouteResponse:
    """Route request with automatic tier fallback."""

    correlation_id = request.correlation_id or generate_correlation_id()

    # TIER 1: Cache lookup
    try:
        cached_result = await cache_lookup(request)
        if cached_result:
            logger.info(f"[{correlation_id}] Cache HIT")
            return cached_result
    except Exception as e:
        logger.warning(f"[{correlation_id}] Cache failed: {e}")

    # TIER 2: vLLM GPU inference
    try:
        if await vllm_health_check():
            result = await route_with_vllm(request)
            if result.confidence_score >= 0.7:
                await cache_set(request, result)  # Cache for next time
                return result
            logger.info(
                f"[{correlation_id}] vLLM low confidence: "
                f"{result.confidence_score}"
            )
    except Exception as e:
        logger.warning(f"[{correlation_id}] vLLM failed: {e}")

    # TIER 3: Fuzzy fallback
    try:
        result = await route_with_fuzzy(request)
        if result.confidence_score >= 0.6:
            return result
        logger.info(
            f"[{correlation_id}] Fuzzy low confidence: "
            f"{result.confidence_score}"
        )
    except Exception as e:
        logger.warning(f"[{correlation_id}] Fuzzy failed: {e}")

    # TIER 4: Remote LLM (last resort)
    try:
        result = await route_with_remote_llm(request)
        return result
    except Exception as e:
        logger.error(f"[{correlation_id}] All tiers failed!")
        raise RoutingFailedError(
            f"All routing tiers failed: {e}",
            correlation_id=correlation_id
        )
```

### Health Checks

```python
@dataclass
class RouterHealthStatus:
    """Health status for all routing tiers."""

    valkey_healthy: bool
    vllm_healthy: bool
    fuzzy_healthy: bool
    remote_llm_healthy: bool

    overall_status: str  # 'healthy', 'degraded', 'unhealthy'

    # Tier-specific details
    valkey_latency_ms: Optional[int]
    vllm_latency_ms: Optional[int]
    vllm_gpu_utilization: Optional[float]

    @property
    def available_tiers(self) -> List[str]:
        """Return list of available tiers."""
        tiers = []
        if self.valkey_healthy:
            tiers.append('cache')
        if self.vllm_healthy:
            tiers.append('vllm')
        if self.fuzzy_healthy:
            tiers.append('fuzzy')
        if self.remote_llm_healthy:
            tiers.append('remote')
        return tiers

async def health_check() -> RouterHealthStatus:
    """Comprehensive health check for all tiers."""

    # Parallel health checks
    valkey_check = asyncio.create_task(check_valkey())
    vllm_check = asyncio.create_task(check_vllm())
    fuzzy_check = asyncio.create_task(check_fuzzy())
    remote_check = asyncio.create_task(check_remote_llm())

    results = await asyncio.gather(
        valkey_check, vllm_check, fuzzy_check, remote_check,
        return_exceptions=True
    )

    # Determine overall status
    healthy_count = sum(1 for r in results if r is True)
    if healthy_count >= 3:
        overall = 'healthy'
    elif healthy_count >= 2:
        overall = 'degraded'
    else:
        overall = 'unhealthy'

    return RouterHealthStatus(
        valkey_healthy=results[0] is True,
        vllm_healthy=results[1] is True,
        fuzzy_healthy=results[2] is True,
        remote_llm_healthy=results[3] is True,
        overall_status=overall
    )
```

**Health Endpoint**: `GET /v1/health`

```json
{
  "overall_status": "healthy",
  "available_tiers": ["cache", "vllm", "fuzzy", "remote"],
  "tier_health": {
    "valkey": {
      "healthy": true,
      "latency_ms": 1
    },
    "vllm": {
      "healthy": true,
      "latency_ms": 35,
      "gpu_utilization": 72.5
    },
    "fuzzy": {
      "healthy": true,
      "latency_ms": 5
    },
    "remote": {
      "healthy": true,
      "latency_ms": 250
    }
  }
}
```

---

## Technical Specifications

### System Requirements

**Infrastructure**:
- PostgreSQL 15+ (for event storage)
- Valkey 8.0+ or Redis 7.0+ (for caching)
- Kafka/Redpanda (for event bus)
- vLLM-compatible GPU (NVIDIA RTX 5090 or similar)

**Network**:
- Internal network: Docker Compose on `omninode-bridge-network`
- External access: Port 8070 (HTTP/WebSocket), 50051 (gRPC)
- GPU server: 192.168.86.200:11434 (existing vLLM endpoint)

**Software Dependencies**:
```python
# Core dependencies
fastapi>=0.115.0
uvicorn[standard]>=0.32.0
grpcio>=1.66.0
grpcio-tools>=1.66.0
aiokafka>=0.11.0
aiohttp>=3.10.0
redis>=5.0.0  # Valkey-compatible client

# vLLM integration
openai>=1.50.0  # vLLM OpenAI-compatible API

# Routing algorithms
rapidfuzz>=3.10.0

# Observability
prometheus-client>=0.21.0
opentelemetry-api>=1.27.0
opentelemetry-sdk>=1.27.0
```

### Configuration

**Environment Variables**:
```bash
# Service configuration
UNIVERSAL_ROUTER_HOST=0.0.0.0
UNIVERSAL_ROUTER_PORT=8070
UNIVERSAL_ROUTER_GRPC_PORT=50051

# Infrastructure
POSTGRES_HOST=192.168.86.200
POSTGRES_PORT=5436
POSTGRES_DATABASE=omninode_bridge
VALKEY_URL=redis://localhost:6379/0
KAFKA_BOOTSTRAP_SERVERS=omninode-bridge-redpanda:9092

# vLLM integration
VLLM_BASE_URL=http://192.168.86.200:11434/v1
VLLM_MODEL=meta-llama/Llama-3.1-8B-Instruct
VLLM_MAX_TOKENS=512
VLLM_TEMPERATURE=0.1

# Remote LLM fallback
ANTHROPIC_API_KEY=sk-ant-xxx
GEMINI_API_KEY=xxx
TOGETHER_API_KEY=xxx

# Caching
CACHE_TTL_SECONDS=3600
CACHE_MAX_SIZE_MB=512
CACHE_HIT_RATE_TARGET=0.65

# Performance tuning
MAX_CONCURRENT_REQUESTS=100
REQUEST_TIMEOUT_MS=5000
VLLM_TIMEOUT_MS=100
FUZZY_MIN_CONFIDENCE=0.6
REMOTE_LLM_MIN_CONFIDENCE=0.0

# Observability
ENABLE_PROMETHEUS_METRICS=true
ENABLE_OPENTELEMETRY=true
LOG_LEVEL=INFO
```

---

## Deployment Architecture

### Docker Compose Integration

**Location**: `deployment/docker-compose.yml`

```yaml
services:
  universal-router:
    image: omniclaude/universal-router:latest
    container_name: universal-router
    ports:
      - "8070:8070"      # HTTP/WebSocket
      - "50051:50051"    # gRPC
    environment:
      - POSTGRES_HOST=192.168.86.200
      - POSTGRES_PORT=5436
      - VALKEY_URL=redis://valkey:6379/0
      - KAFKA_BOOTSTRAP_SERVERS=omninode-bridge-redpanda:9092
      - VLLM_BASE_URL=http://192.168.86.200:11434/v1
    networks:
      - app_network
      - omninode-bridge-network
    depends_on:
      - valkey
      - postgres
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8070/v1/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  valkey:
    image: valkey/valkey:8.0
    container_name: universal-router-valkey
    ports:
      - "6379:6379"
    volumes:
      - valkey_data:/data
    networks:
      - app_network
    command: >
      valkey-server
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save ""
    restart: unless-stopped

networks:
  app_network:
    driver: bridge
  omninode-bridge-network:
    external: true

volumes:
  valkey_data:
```

### Service Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  UNIVERSAL ROUTER SERVICE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  API GATEWAY (FastAPI)                                     â”‚ â”‚
â”‚  â”‚  â€¢ HTTP/REST endpoints                                     â”‚ â”‚
â”‚  â”‚  â€¢ WebSocket server                                        â”‚ â”‚
â”‚  â”‚  â€¢ Request validation                                      â”‚ â”‚
â”‚  â”‚  â€¢ Correlation ID generation                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  gRPC SERVER                                               â”‚ â”‚
â”‚  â”‚  â€¢ Binary protocol                                         â”‚ â”‚
â”‚  â”‚  â€¢ Streaming support                                       â”‚ â”‚
â”‚  â”‚  â€¢ Type-safe contracts                                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  KAFKA CONSUMER/PRODUCER                                   â”‚ â”‚
â”‚  â”‚  â€¢ Event-driven routing                                    â”‚ â”‚
â”‚  â”‚  â€¢ Async request processing                                â”‚ â”‚
â”‚  â”‚  â€¢ Audit trail publishing                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ROUTER CORE                                               â”‚ â”‚
â”‚  â”‚  â€¢ Tier orchestration                                      â”‚ â”‚
â”‚  â”‚  â€¢ Fallback cascade logic                                  â”‚ â”‚
â”‚  â”‚  â€¢ Confidence scoring                                      â”‚ â”‚
â”‚  â”‚  â€¢ Cost tracking                                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚              â”‚              â”‚              â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Valkey  â”‚  â”‚   vLLM   â”‚  â”‚  Fuzzy   â”‚  â”‚  Remote  â”‚      â”‚
â”‚  â”‚  Cache   â”‚  â”‚   GPU    â”‚  â”‚ Fallback â”‚  â”‚   LLM    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4-Phase Implementation Plan

### Phase 1: Foundation (Week 1)

**Goal**: Core routing service with HTTP API and Valkey caching

**Tasks**:
1. **Service Scaffold**
   - [ ] Create `services/universal_router/` directory structure
   - [ ] FastAPI application with `/v1/route` endpoint
   - [ ] Request/response models with Pydantic
   - [ ] Correlation ID generation and propagation
   - [ ] Basic logging and error handling

2. **Valkey Integration**
   - [ ] Redis/Valkey client setup with async support
   - [ ] Cache key generation (hash of request + context)
   - [ ] Cache get/set with TTL
   - [ ] Cache invalidation patterns
   - [ ] Cache warming on startup

3. **Fuzzy Fallback**
   - [ ] Integrate rapid-fuzz library
   - [ ] Load agent registry from YAML files
   - [ ] Fuzzy matching algorithm with confidence scoring
   - [ ] Fallback to fuzzy when cache misses

4. **Database Integration**
   - [ ] Create `universal_router_events` table
   - [ ] SQLAlchemy models
   - [ ] Non-blocking event logging with async
   - [ ] Database connection pooling

5. **Testing**
   - [ ] Unit tests for cache logic
   - [ ] Unit tests for fuzzy matching
   - [ ] Integration test: HTTP â†’ cache â†’ fuzzy â†’ DB
   - [ ] Load test: 100 req/s sustained

**Success Criteria**:
- âœ… HTTP API responds in <10ms for cached routes
- âœ… Fuzzy fallback works in <10ms
- âœ… Events logged to PostgreSQL
- âœ… 100+ req/s throughput

**Deliverables**:
- Working HTTP API service
- Valkey caching operational
- Fuzzy fallback implemented
- Database logging functional

---

### Phase 2: GPU Acceleration (Week 2)

**Goal**: vLLM integration with local GPU inference

**Tasks**:
1. **vLLM Client**
   - [ ] OpenAI-compatible client for vLLM
   - [ ] Connection pooling and retry logic
   - [ ] Timeout handling (100ms max)
   - [ ] Health check endpoint

2. **Routing Prompt Engineering**
   - [ ] Design prompt template for routing
   - [ ] Include agent registry in prompt
   - [ ] Test prompt with various request types
   - [ ] Optimize for <50ms inference time

3. **Tier Orchestration**
   - [ ] Implement 4-tier fallback cascade
   - [ ] Cache â†’ vLLM â†’ Fuzzy â†’ Remote LLM
   - [ ] Confidence threshold logic (>0.7 for vLLM)
   - [ ] Performance tracking per tier

4. **vLLM Configuration**
   - [ ] Verify vLLM running at 192.168.86.200:11434
   - [ ] Test with Llama-3.1-8B-Instruct model
   - [ ] Configure prefix caching
   - [ ] Tune batch size and concurrency

5. **Testing**
   - [ ] Unit tests for vLLM client
   - [ ] Integration test: HTTP â†’ cache miss â†’ vLLM
   - [ ] Benchmark vLLM latency (target: <50ms p95)
   - [ ] Stress test: 100+ req/s with GPU

**Success Criteria**:
- âœ… vLLM responds in 20-50ms
- âœ… Confidence scores >0.7 for common requests
- âœ… Graceful fallback when GPU unavailable
- âœ… 100+ req/s throughput maintained

**Deliverables**:
- vLLM integration functional
- 4-tier fallback cascade working
- GPU performance metrics collected

---

### Phase 3: Multi-Protocol Support (Week 3)

**Goal**: gRPC, Kafka, and WebSocket support

**Tasks**:
1. **gRPC Server**
   - [ ] Define Protocol Buffer schemas
   - [ ] Generate Python code from .proto files
   - [ ] Implement `UniversalRouter` gRPC service
   - [ ] Add streaming support (`RouteStream`)
   - [ ] Test with gRPC client

2. **Kafka Event Bus**
   - [ ] Create Kafka topics (requested/completed/failed)
   - [ ] Kafka consumer for routing requests
   - [ ] Kafka producer for routing responses
   - [ ] Avro schema registration
   - [ ] Correlation ID propagation

3. **WebSocket Server**
   - [ ] WebSocket endpoint (`/v1/stream`)
   - [ ] Real-time routing progress updates
   - [ ] Multi-client broadcast support
   - [ ] Connection pooling and cleanup

4. **Framework Adapters**
   - [ ] Claude Code adapter (HTTP client)
   - [ ] LangChain adapter (Python package)
   - [ ] AutoGPT adapter (Python package)
   - [ ] CrewAI adapter (Python package)

5. **Testing**
   - [ ] gRPC client tests (unary + streaming)
   - [ ] Kafka producer/consumer tests
   - [ ] WebSocket connection tests
   - [ ] Multi-protocol load test

**Success Criteria**:
- âœ… gRPC service functional with streaming
- âœ… Kafka event bus operational
- âœ… WebSocket real-time updates working
- âœ… At least 2 framework adapters complete

**Deliverables**:
- gRPC server operational
- Kafka integration complete
- WebSocket support added
- Framework adapters published

---

### Phase 4: Observability & Production (Week 4)

**Goal**: Complete observability and production hardening

**Tasks**:
1. **Prometheus Metrics**
   - [ ] Counter: `router_requests_total`
   - [ ] Histogram: `router_latency_seconds`
   - [ ] Gauge: `router_cache_hit_rate`
   - [ ] Gauge: `vllm_gpu_utilization`
   - [ ] Counter: `router_cost_usd_total`
   - [ ] Metrics endpoint: `/metrics`

2. **Grafana Dashboards**
   - [ ] Routing performance dashboard
   - [ ] Cost analysis dashboard
   - [ ] Cache performance dashboard
   - [ ] GPU monitoring dashboard
   - [ ] Tier distribution dashboard

3. **OpenTelemetry Tracing**
   - [ ] Span creation for each tier
   - [ ] Trace propagation across services
   - [ ] Jaeger integration
   - [ ] End-to-end trace visualization

4. **Production Hardening**
   - [ ] Rate limiting (per user/session)
   - [ ] Circuit breaker for external services
   - [ ] Graceful shutdown handling
   - [ ] Docker health checks
   - [ ] Kubernetes manifests (optional)

5. **Documentation**
   - [ ] API reference (OpenAPI spec)
   - [ ] gRPC documentation
   - [ ] Deployment guide
   - [ ] Troubleshooting guide
   - [ ] Cost optimization guide

6. **Testing**
   - [ ] End-to-end production simulation
   - [ ] Chaos engineering tests (tier failures)
   - [ ] Load test: 1000+ req/s burst
   - [ ] Multi-day soak test

**Success Criteria**:
- âœ… All metrics exported to Prometheus
- âœ… Grafana dashboards operational
- âœ… Distributed tracing functional
- âœ… Service survives chaos tests
- âœ… Documentation complete

**Deliverables**:
- Production-ready Universal Router
- Complete observability stack
- Comprehensive documentation
- Deployment automation

---

## Dependencies and Prerequisites

### External Dependencies

**Infrastructure** (must exist):
- âœ… PostgreSQL at 192.168.86.200:5436 (exists)
- âœ… Kafka/Redpanda at 192.168.86.200:9092 (exists)
- âœ… vLLM endpoint at 192.168.86.200:11434 (exists)
- â³ Valkey service (will be added to docker-compose)

**Software Dependencies**:
- Python 3.12+
- Docker & Docker Compose
- vLLM-compatible GPU (RTX 5090 or similar)
- Agent registry YAML files

### Blocking Dependencies

**CRITICAL: PR #22 MUST MERGE FIRST**

**Why**:
1. **Observability infrastructure** - PR #22 includes action logging, manifest injection traceability, and database schemas required for Universal Router observability
2. **Type-safe configuration** - PR #22 migrates to Pydantic Settings, which Universal Router will use for configuration
3. **Docker Compose consolidation** - PR #22 establishes the deployment architecture that Universal Router will integrate into
4. **Kafka event patterns** - PR #22 establishes event bus patterns that Universal Router will follow

**PR #22 Status** (as of 2025-11-07):
- Branch: `fix/observability-data-flow-gaps`
- CI/CD: 3 checks failing (code quality, security scan, tests)
- Review: CodeRabbit review with 9 actionable + 4 critical + 12 nitpick comments
- Estimated resolution time: 1-2 days

**Action Plan**:
1. Resolve PR #22 CI/CD failures
2. Address CodeRabbit review comments
3. Merge PR #22 to main
4. **THEN** start Universal Router Phase 1

---

## Cost-Benefit Analysis

### Cost Comparison

**Baseline (All-Anthropic)**:
- Model: Claude Haiku (cheapest Anthropic model)
- Cost per request: $0.003
- Annual cost (1M requests): $3,000
- Latency: 200-500ms

**Universal Router (4-Tier)**:
- Tier 1 (Cache): 60-70% @ $0 = $0
- Tier 2 (vLLM): 25-35% @ $0.000004 = $1-$1.40
- Tier 3 (Fuzzy): 3-5% @ $0 = $0
- Tier 4 (Remote LLM): <5% @ $0.003 = $150
- **Total annual cost**: ~$151-$151.40
- **Savings**: $2,849 (95% reduction)
- **Weighted avg latency**: ~36ms (83% faster)

### ROI Calculation

**Development Cost**:
- 4 weeks Ã— 1 developer = 160 hours
- Developer rate: $100/hour (hypothetical)
- **Total development**: $16,000

**Annual Savings**:
- Cost savings: $2,849
- Performance improvement value: $5,000 (hypothetical - improved UX, faster iteration)
- **Total annual value**: $7,849

**ROI**:
- Payback period: 2.0 years
- 5-year ROI: $23,245 (net value after development cost)

**Intangible Benefits**:
- âœ… Framework-agnostic design (reusable across projects)
- âœ… Local GPU utilization (no external dependencies)
- âœ… Complete control over routing logic
- âœ… Observability and debugging capabilities
- âœ… Foundation for future AI infrastructure

---

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|---------|------------|
| **vLLM GPU unavailable** | Medium | High | 4-tier fallback cascade with fuzzy + remote LLM |
| **Cache invalidation bugs** | Low | Medium | Comprehensive testing + pattern-based invalidation |
| **Routing quality degradation** | Medium | High | Continuous monitoring + confidence thresholds |
| **Cost overruns (Remote LLM tier)** | Low | Medium | Rate limiting + cost alerts + tier usage tracking |
| **Performance degradation** | Low | High | Load testing + auto-scaling + circuit breakers |
| **Security vulnerabilities** | Low | Critical | API key rotation + rate limiting + input validation |

---

## Success Metrics

### Key Performance Indicators (KPIs)

**Performance**:
- âœ… P50 latency: <10ms (target), <50ms (acceptable)
- âœ… P95 latency: <50ms (target), <200ms (acceptable)
- âœ… P99 latency: <100ms (target), <500ms (acceptable)
- âœ… Throughput: 100+ req/s sustained
- âœ… Availability: 99.9% uptime

**Cost**:
- âœ… Cost per request: <$0.00001 (target)
- âœ… Annual cost: <$500 for 1M requests
- âœ… Cost savings: >90% vs all-Anthropic baseline

**Quality**:
- âœ… Routing accuracy: >95% (manual validation)
- âœ… Cache hit rate: >60%
- âœ… vLLM confidence: >0.7 average
- âœ… Remote LLM usage: <5% of requests

**Adoption**:
- âœ… Framework adapters: 3+ frameworks supported
- âœ… External usage: At least 1 non-OmniClaude project using router
- âœ… Documentation completeness: 100% API coverage

---

## Future Enhancements

**Post-MVP Improvements**:

1. **Multi-Model vLLM**
   - Support for multiple local models (Llama, Mistral, Gemma)
   - Automatic model selection based on request complexity
   - A/B testing for model comparison

2. **Agent Registry Intelligence**
   - AI-powered agent capability discovery
   - Automatic agent registry updates
   - Agent performance tracking and ranking

3. **Advanced Caching**
   - Semantic caching (similar requests â†’ same agent)
   - Multi-level cache (L1: Valkey, L2: Redis, L3: PostgreSQL)
   - Cache prewarming with ML predictions

4. **Cost Optimization**
   - Dynamic tier selection based on budget constraints
   - Request batching for cost efficiency
   - Provider switching based on pricing

5. **Multi-Region Support**
   - Geographic routing for low latency
   - Cross-region failover
   - Edge deployment for global scale

6. **Advanced Analytics**
   - ML-powered routing optimization
   - Agent performance prediction
   - Anomaly detection for routing quality

---

## Conclusion

The Universal Agent Router represents a **paradigm shift** in agent routing architecture:

- **99.87% cost reduction** ($3000 â†’ $4 per 1M requests)
- **83% latency improvement** (500ms â†’ 36ms weighted average)
- **Framework-agnostic** design for maximum reusability
- **Multi-tier fallback** for resilience and quality
- **Complete observability** for debugging and optimization

By leveraging local GPU acceleration with vLLM, intelligent caching with Valkey, and graceful degradation through multiple fallback tiers, we achieve both **exceptional performance** and **dramatic cost savings** while maintaining routing quality.

**Next Steps**:
1. Resolve PR #22 blocking issues
2. Begin Phase 1 implementation (Week 1)
3. Iterate through 4-phase rollout plan
4. Monitor metrics and optimize performance

**Questions or Feedback**:
- Technical lead: Jonah Gray
- Architecture review: [To be scheduled after PR #22 merge]
- Stakeholder alignment: [Pending management approval]

---

**Document Version**: 1.0.0
**Last Updated**: 2025-11-07
**Status**: Planning / Awaiting PR #22 Merge
**Next Review**: After PR #22 merge
