#!/usr/bin/env python3
"""
Agent Task Simulation - End-to-End Test

Simulates how the hook system would work with real agent invocation.
Tests the direct_single pathway which is production-ready.

Usage:
    poetry run python3 test_agent_task_simulation.py
"""

import asyncio
import sys
import json
from pathlib import Path
from datetime import datetime

# Add paths
sys.path.insert(0, str(Path("/Volumes/PRO-G40/Code/omniclaude/agents/parallel_execution")))
sys.path.insert(0, str(Path("/Users/jonah/.claude/hooks/lib")))

from agent_pathway_detector import AgentPathwayDetector
from agent_invoker import AgentInvoker


def format_agent_context(agent_name: str, agent_config: dict) -> str:
    """Format agent config as context injection (simulates hook behavior)."""

    # Extract key information
    purpose = agent_config.get('agent_purpose', 'N/A')
    domain = agent_config.get('agent_domain', 'N/A')
    capabilities = agent_config.get('capabilities', {})
    triggers = agent_config.get('triggers', [])

    # Format context similar to what the hook injects
    context = f"""

---
ðŸ¤– [Agent Identity Injection - Test Simulation]

**Agent**: {agent_name}
**Domain**: {domain}
**Purpose**: {purpose}

**Core Capabilities**:
"""

    # List capabilities
    for cap_name, enabled in list(capabilities.items())[:5]:
        if enabled:
            context += f"  â€¢ {cap_name.replace('_', ' ').title()}\n"

    if len(capabilities) > 5:
        context += f"  â€¢ ... and {len(capabilities) - 5} more\n"

    context += f"""
**Activation Triggers** ({len(triggers)} total):
  Primary: {', '.join(triggers[:3])}

**IMPORTANT**: You are assuming this agent's identity and should:
- Adapt your expertise and response patterns to match this domain
- Use domain-specific terminology and approaches
- Apply the agent's specialized capabilities to the task

---
"""

    return context


async def simulate_agent_task(user_prompt: str):
    """Simulate complete agent invocation flow."""

    print("\n" + "="*80)
    print("AGENT TASK SIMULATION")
    print("="*80)
    print(f"\nUser Prompt: {user_prompt}")
    print(f"Timestamp: {datetime.now().isoformat()}")

    # Step 1: Detect pathway
    print("\n--- Step 1: Pathway Detection ---")
    detector = AgentPathwayDetector()
    detection = detector.detect(user_prompt)

    print(f"âœ“ Pathway: {detection.pathway}")
    print(f"âœ“ Agents: {detection.agents}")
    print(f"âœ“ Confidence: {detection.confidence}")
    print(f"âœ“ Cleaned task: {detection.task}")

    if detection.pathway != "direct_single":
        print(f"\nâš  This simulation only supports direct_single pathway")
        print(f"  Detected pathway: {detection.pathway}")
        return False

    # Step 2: Invoke agent
    print("\n--- Step 2: Agent Invocation ---")
    invoker = AgentInvoker(correlation_id="test-simulation-001")

    try:
        result = await invoker.invoke(
            prompt=user_prompt,
            context={"simulation": True, "test_mode": True}
        )

        if not result.get('success'):
            print(f"âœ— Invocation failed: {result.get('error')}")
            return False

        print(f"âœ“ Success: {result['success']}")
        print(f"âœ“ Agent: {result.get('agent_name')}")
        print(f"âœ“ Execution time: {result.get('execution_time_ms', 0):.2f}ms")

        # Step 3: Format context (what would be injected into Claude's prompt)
        print("\n--- Step 3: Context Injection (Simulated) ---")

        agent_config = result.get('agent_config')
        if not agent_config:
            print("âœ— No agent config returned")
            return False

        context_text = format_agent_context(result['agent_name'], agent_config)

        print("Context to be injected into Claude's prompt:")
        print(context_text)

        # Step 4: Show what Claude would receive
        print("\n--- Step 4: Claude's Final Prompt ---")

        final_prompt = user_prompt + context_text

        print(f"Original prompt length: {len(user_prompt)} chars")
        print(f"Context length: {len(context_text)} chars")
        print(f"Final prompt length: {len(final_prompt)} chars")
        print(f"\nFirst 200 chars of final prompt:")
        print(f'"{final_prompt[:200]}..."')

        # Step 5: Simulate Claude's response
        print("\n--- Step 5: Expected Behavior ---")

        agent_name = result['agent_name']
        purpose = agent_config.get('agent_purpose', '')

        print(f"\nClaude would receive the full prompt with agent context and would:")
        print(f"  1. Read the agent identity: {agent_name}")
        print(f"  2. Understand the purpose: {purpose}")
        print(f"  3. Adapt behavior to match the agent's expertise")
        print(f"  4. Apply domain-specific capabilities")
        print(f"  5. Respond using agent-appropriate patterns and terminology")

        print("\n" + "="*80)
        print("âœ“ SIMULATION COMPLETE - System Working as Designed")
        print("="*80)

        return True

    except Exception as e:
        print(f"\nâœ— ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        await invoker.cleanup()


async def run_multiple_simulations():
    """Run multiple task simulations to demonstrate different agents."""

    test_cases = [
        {
            "prompt": "@agent-testing Analyze test coverage for the agent pathway system",
            "expected_agent": "agent-testing",
            "description": "Testing specialist for coverage analysis"
        },
        {
            "prompt": "@agent-commit Review staged changes and create commit message",
            "expected_agent": "agent-commit",
            "description": "Commit specialist for git operations"
        },
        {
            "prompt": "@agent-python-expert Optimize this async function for performance",
            "expected_agent": "agent-python-expert",
            "description": "Python specialist for optimization"
        }
    ]

    results = []

    for idx, test_case in enumerate(test_cases, 1):
        print(f"\n\n{'#'*80}")
        print(f"# TEST CASE {idx}/{len(test_cases)}")
        print(f"# {test_case['description']}")
        print(f"{'#'*80}")

        success = await simulate_agent_task(test_case['prompt'])
        results.append({
            "test": test_case['description'],
            "success": success
        })

        # Pause between tests
        if idx < len(test_cases):
            print("\n(Pausing 2 seconds before next test...)")
            await asyncio.sleep(2)

    # Summary
    print("\n\n" + "="*80)
    print("TEST SUITE SUMMARY")
    print("="*80)

    passed = sum(1 for r in results if r['success'])
    total = len(results)

    for idx, result in enumerate(results, 1):
        status = "âœ“ PASS" if result['success'] else "âœ— FAIL"
        print(f"{status}: Test {idx} - {result['test']}")

    print(f"\nPassed: {passed}/{total}")

    if passed == total:
        print("\nðŸŽ‰ All simulations passed! System ready for production.")
    else:
        print(f"\nâš  {total - passed} simulation(s) failed.")

    return passed == total


async def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="Agent task simulation")
    parser.add_argument("--prompt", help="Custom prompt to test (uses default tests if not provided)")
    parser.add_argument("--multiple", action="store_true", help="Run multiple test cases")

    args = parser.parse_args()

    if args.prompt:
        # Single custom test
        success = await simulate_agent_task(args.prompt)
    elif args.multiple:
        # Multiple predefined tests
        success = await run_multiple_simulations()
    else:
        # Default single test
        success = await simulate_agent_task(
            "@agent-testing Analyze test coverage for agent pathway system"
        )

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())
