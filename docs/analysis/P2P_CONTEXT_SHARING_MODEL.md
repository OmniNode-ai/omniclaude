# P2P Context Sharing Model - Decentralized Intelligence Network

**Date**: 2025-11-09
**Related**: DEBUG_REWARDS_CLARIFICATION_ANALYSIS.md (Phase 3 expansion)
**Model**: BitTorrent-like tracker + compute token incentives

---

## Executive Summary

**Vision**: A decentralized, peer-to-peer knowledge graph where participants earn compute tokens by sharing context/intelligence with other nodes. Instead of sending full context to LLMs (expensive), store metadata/pointers in a distributed graph and use RAG to retrieve only what's needed.

**Economic Model**:
- **Earn tokens**: By serving context to peers (outgoing bandwidth/queries)
- **Spend tokens**: On compute (LLM inference, embeddings, etc.)
- **Self-sustaining**: Network participants fund each other through natural usage

**Architecture**: BitTorrent-like tracker + distributed hash table (DHT) + compute token ledger

---

## The Problem (Current State)

### LLM Context Costs

**Current approach**: Send full context to LLM with every request
- Expensive: $3-15 per 1M input tokens depending on model
- Slow: Large context = longer processing time
- Wasteful: Most context is irrelevant to specific query
- Centralized: All context must go through central LLM provider

**Example**:
```
User query: "How do I implement ONEX Effect node?"

Current: Send entire ONEX docs (500KB) → LLM → response
Cost: ~$2-5 per query (500K tokens @ $4-10/1M)

P2P model: Query graph → Find relevant nodes → Retrieve 50KB → LLM → response
Cost: ~$0.20-0.50 per query (50K tokens)
Savings: 90% cost reduction
```

### Centralized Knowledge Storage

**Current issues**:
- Single point of failure (if Qdrant goes down, no intelligence)
- No incentive to contribute (altruism only)
- Limited by single organization's storage capacity
- No way to discover knowledge outside your system

---

## The Solution: P2P Context Sharing Network

### Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                    USER QUERY                                    │
│          "How do I implement ONEX Effect node?"                  │
└──────────────────────┬──────────────────────────────────────────┘
                       │
                       ▼
            ┌──────────────────────┐
            │  Local Intelligence  │
            │  - Metadata graph    │
            │  - Pointers to peers │
            │  - Cache layer       │
            └──────────┬───────────┘
                       │
                       ▼
            ┌──────────────────────┐
            │  P2P Tracker         │
            │  "Who has ONEX docs?"│
            │  → Peer A, B, C      │
            └──────────┬───────────┘
                       │
                       ▼
         ┌─────────────┴─────────────┐
         │                           │
    ┌────▼─────┐  ┌────▼─────┐  ┌───▼──────┐
    │ Peer A   │  │ Peer B   │  │ Peer C   │
    │ (online) │  │ (online) │  │ (offline)│
    │ Earns 10 │  │ Earns 10 │  │ Earns 0  │
    │ tokens   │  │ tokens   │  │ tokens   │
    └────┬─────┘  └────┬─────┘  └──────────┘
         │             │
         └──────┬──────┘
                │ (serve context)
                ▼
      ┌───────────────────┐
      │  RAG Retrieval    │
      │  50KB most        │
      │  relevant context │
      └────────┬──────────┘
               │
               ▼
      ┌───────────────────┐
      │  LLM (Gemini)     │
      │  Cost: $0.20      │
      │  vs $2.00         │
      │  (90% savings)    │
      └────────┬──────────┘
               │
               ▼
      ┌───────────────────┐
      │  Response         │
      │  to user          │
      └───────────────────┘
```

### Key Components

#### 1. P2P Tracker (BitTorrent-like)

**Purpose**: Discover which peers have which context

**Protocol**:
```json
{
  "announce": "https://tracker.omninode.ai/announce",
  "peers": [
    {
      "peer_id": "abc123...",
      "ip": "192.168.1.100",
      "port": 6881,
      "has_context": ["onex_docs", "python_stdlib", "agent_patterns"],
      "online": true,
      "reputation": 0.95,
      "uptime": "99.2%"
    }
  ]
}
```

**Operations**:
- `announce`: Register what context you have (heartbeat every 30s)
- `scrape`: Query who has specific context
- `peer_list`: Get list of peers with requested context
- `reputation`: Track peer reliability for token rewards

#### 2. Distributed Metadata Graph

**Storage**: Local graph database (Memgraph, Neo4j, or simple SQLite)

**Structure**:
```cypher
// Node: Knowledge chunk
(:Context {
  hash: "sha256:abc123...",
  title: "ONEX Effect Node Implementation",
  size_bytes: 4096,
  embedding_vector: [...],
  tags: ["onex", "effect", "node", "python"],
  peers_with_content: ["peer_a", "peer_b"],
  last_updated: "2025-11-09T10:30:00Z"
})

// Edge: Relationships
(:Context)-[:REFERENCES]->(:Context)
(:Context)-[:PART_OF]->(:Document)
(:Context)-[:AUTHORED_BY]->(:Peer)
```

**Benefits**:
- Fast local queries (no network roundtrip for metadata)
- Stores pointers, not full content (minimal storage)
- Can query "who has X?" without downloading X
- RAG-optimized (embedding vectors for similarity search)

#### 3. Compute Token Ledger (Kafka Event Sourcing)

**Leverage Existing Infrastructure**: Use Kafka/Redpanda as the distributed ledger (you already have this!)

**Key Insight**: Kafka IS a ledger - durable, distributed, append-only event log with replay capability.

**Kafka Topics**:
```yaml
# Token transaction events (source of truth)
token.transactions.v1:
  partitions: 10 (by peer_id)
  retention: unlimited (compact by peer_id for balances)

# Materialized peer balances (computed from transactions)
token.balances.v1:
  partitions: 10 (by peer_id)
  retention: unlimited (compacted - latest balance only)
```

**Event Schema** (token.transactions.v1):
```json
{
  "transaction_id": "uuid",
  "peer_id": "abc123",
  "transaction_type": "earned" | "spent",
  "amount": 10,
  "reason": "context_served" | "context_retrieved" | "llm_query",
  "context_hash": "sha256:...",
  "counterparty_peer_id": "def456",
  "timestamp": "2025-11-09T10:30:00Z"
}
```

**Balance Event Schema** (token.balances.v1):
```json
{
  "peer_id": "abc123",
  "balance": 5000,
  "total_earned": 10000,
  "total_spent": 5000,
  "contexts_served": 250,
  "contexts_retrieved": 125,
  "reputation": 0.95,
  "updated_at": "2025-11-09T10:30:00Z"
}
```

**Token Flow** (Event Sourcing):
1. Peer A requests context from Peer B
2. Peer B serves context (5KB)
3. Publish event to `token.transactions.v1`:
   ```json
   {"peer_id": "peer_b", "type": "earned", "amount": 10, "context_hash": "..."}
   {"peer_id": "peer_a", "type": "spent", "amount": 10, "context_hash": "..."}
   ```
4. Consumer updates `token.balances.v1` (compacted topic):
   ```json
   {"peer_id": "peer_b", "balance": 5010, ...}
   {"peer_id": "peer_a", "balance": 4990, ...}
   ```
5. Valkey cache updated for fast reads

**Benefits of Kafka Ledger**:
1. ✅ **Already deployed** - no new infrastructure
2. ✅ **Distributed** - Redpanda handles replication
3. ✅ **Durable** - retention unlimited, events never lost
4. ✅ **Replay** - rebuild balances from transaction log
5. ✅ **Audit trail** - complete transaction history
6. ✅ **Performance** - Kafka optimized for high-throughput
7. ✅ **Event sourcing** - natural fit for token ledger

**Architecture**:
```
Token Transaction (event) → Kafka topic → Balance Consumer → Compacted balance topic
                                              ↓
                                         Valkey cache (fast reads)
                                              ↓
                                         PostgreSQL (analytics only)
```

#### Token Sustainability Analysis

**Token Velocity** (earn → spend cycle time):
- **Target**: >1 cycle/week per peer
- **Justification**: With daily earning (~210 tokens) and daily spending (~200 tokens), average peer completes ~7 cycles/week
- **Why achievable**:
  - Peers earn passively (serve while online)
  - Peers spend actively (retrieve when needed)
  - Natural balance: Those who query more also serve more (proportional to online time)
  - Example: 8-hour online peer earns 70 tokens serving, spends 65 tokens retrieving → cycle complete in 1 day

**Token Supply Scaling**:

| Network Size | Daily Issued Tokens | Total Supply (30 days) | Notes |
|-------------|---------------------|------------------------|-------|
| 10 peers | 2,100/day (210 per peer) | 63,000 | Initial bootstrap |
| 100 peers | 21,000/day | 630,000 | Sufficient liquidity |
| 1,000 peers | 210,000/day | 6,300,000 | Requires monitoring |
| 10,000 peers | 2,100,000/day | 63,000,000 | Supply cap needed |

**Maximum Token Supply Cap**:
- **Recommendation**: Implement soft cap at 100M tokens (supports ~10,000 active peers)
- **Approach**: Dynamic earning rate adjustment
  - Supply <50M → standard rates (2 tokens/KB)
  - Supply 50-100M → 90% earning rate (1.8 tokens/KB)
  - Supply >100M → 50% earning rate (1 token/KB)
- **Alternative**: Hard cap + token burning (spend = burn, preventing infinite inflation)
- **Rationale**: Unlimited issuance works for small networks (<1000 peers) but creates hyperinflation risk at scale

**Hoarding Scenarios**:

| Scenario | Risk Level | Mitigation |
|----------|-----------|------------|
| Earn 1000, spend 100/year → 900 hoarded | High | Token expiration (10% decay/year) |
| Early adopters hoard 100K+ tokens | Medium | Gradual supply increase favors new participants |
| Whales manipulate token price | Low | No exchange market (tokens non-transferable between peers) |

**Token Expiration Policy Justification**:
- **Policy**: 10% decay/year (~0.027% per day)
- **Rationale**:
  1. **Encourages circulation**: Use tokens or lose them (modest penalty)
  2. **Tax on contributors**: Rewards active participants over passive hoarders
  3. **Prevents stagnation**: Tokens must flow to maintain network health
  4. **Fairness**: New peers not disadvantaged by early adopter hoarding
- **Implementation**: Daily decay calculation in token balance consumer
  - `new_balance = old_balance * 0.99973` (per day)
  - Decay logged as transaction event for auditability
- **Why 10%**: High enough to discourage hoarding, low enough to not penalize short-term savers (lose <1% per month)

**Token Sustainability Formula**:
```
Sustainable if: (Total Earned/day) ≈ (Total Spent/day) ± 20%

Network health check:
  IF supply_growth_rate > 1.2: Decrease earning rates
  IF supply_growth_rate < 0.8: Increase earning rates OR increase free tier
  IF hoarding_ratio > 0.5: Increase decay rate to 15%/year
```

**Monitoring Triggers**:
- Token velocity <0.5 cycles/week → Increase earning rates by 20%
- Supply growing >30%/week → Activate supply cap sooner
- >50% peers have balance >1000 tokens → Introduce token expiration

#### Monitoring and Observability

**Key Metrics** (Dashboard Requirements):

1. **Token Economics**:
   - **Token velocity**: Earn → spend cycle time (target: >1.0 cycles/week)
   - **Balance distribution**: Histogram of peer balances (detect hoarding)
   - **Supply growth rate**: Daily/weekly token issuance (detect inflation)
   - **Earning/spending ratio**: Supply vs. demand balance (target: 0.8-1.2)
   - **Token flow visualization**: Sankey diagram showing token movement between peers

2. **Peer Health**:
   - **Active peers**: Count of peers announcing in last 24h (target: growing)
   - **Peer uptime**: Average uptime % across network (target: >95%)
   - **Peer churn rate**: % peers leaving per week (target: <5%)
   - **New peer registration rate**: Daily signups (monitor Sybil attacks)
   - **Peer profitability**: % peers earning more than spending (target: >70%)

3. **Context Quality**:
   - **Context hit rate**: % queries finding relevant context (target: >80%)
   - **RAG precision**: % retrieved context marked relevant by users (target: >90%)
   - **Verification rate**: % contexts verified by peers (target: >50%)
   - **Average context reputation**: Network-wide reputation score (target: >0.8)
   - **Bad actor rate**: % peers with reputation <0.5 (target: <5%)

4. **Performance**:
   - **Context retrieval latency**: P50/P95/P99 (target: P95 <500ms)
   - **Tracker response time**: Announce/scrape latency (target: <100ms)
   - **Token transaction throughput**: Tx/sec processed (target: >1000 tx/sec)
   - **Balance query latency**: Valkey cache hit rate + latency (target: <10ms)
   - **Kafka consumer lag**: Token balance consumer lag (target: <100ms)

**Alerting Thresholds** (PagerDuty/Slack Integration):

| Metric | Warning Threshold | Critical Threshold | Action |
|--------|------------------|-------------------|--------|
| **Token drift** | >20% imbalance (supply/demand) | >30% imbalance | Emergency parameter adjustment |
| **Peer drop** | >30% decline in 7 days | >50% decline in 7 days | Emergency airdrop + investigate |
| **Token velocity** | <0.7 cycles/week | <0.5 cycles/week | Increase earning rates by 20% |
| **Bad actor rate** | >10% peers reputation <0.5 | >20% peers reputation <0.5 | Review blocklist criteria |
| **Context hit rate** | <70% | <50% | Improve RAG quality |
| **Retrieval latency** | P95 >800ms | P95 >1200ms | Scale peer infrastructure |
| **Consumer lag** | >500ms | >2000ms | Scale consumer instances |

**Dashboard Sections** (Grafana/Kibana):

1. **Network Overview**:
   - Active peers (24h rolling)
   - Total contexts served (24h rolling)
   - Token velocity (7d moving average)
   - Cost savings vs. baseline (running total)

2. **Token Economics**:
   - Supply growth rate (line chart, 30d)
   - Balance distribution (histogram)
   - Top earners (leaderboard, top 20 peers)
   - Top spenders (leaderboard, top 20 peers)
   - Token flow (Sankey diagram, daily)

3. **Peer Stats**:
   - Peer uptime distribution (histogram)
   - Peer reputation distribution (histogram)
   - Churn rate (7d rolling)
   - New peer registrations (daily bar chart)
   - Peer profitability % (gauge)

4. **Content Quality**:
   - Context hit rate (7d moving average)
   - RAG precision (user feedback)
   - Verification rate (% contexts verified)
   - Bad actor rate (gauge with threshold line)
   - Top contexts by retrieval count (table)

5. **Performance**:
   - Context retrieval latency (P50/P95/P99 line chart)
   - Tracker response time (P50/P95/P99 line chart)
   - Token transaction throughput (tx/sec line chart)
   - Kafka consumer lag (line chart with threshold)
   - Balance query latency (histogram)

**Audit Logging** (Trace Transactions for Disputes):

1. **Transaction Audit Trail**:
   - Every token transaction logged to PostgreSQL (immutable)
   - Fields: `transaction_id`, `peer_id`, `counterparty_peer_id`, `amount`, `reason`, `timestamp`, `signature`
   - Indexed by `peer_id`, `counterparty_peer_id`, `timestamp` (fast dispute resolution)

2. **Dispute Resolution**:
   - User claims: "I was charged but didn't receive context"
   - Query: `SELECT * FROM token_transactions WHERE transaction_id = '{id}'`
   - Verify: Signature valid? Balance deducted? Context delivered?
   - Action: Refund if proven invalid (reverse transaction)

3. **Compliance Reporting**:
   - Monthly report: Total tokens issued, spent, balance distribution
   - Quarterly audit: Transaction anomalies, bad actor summary
   - Export: CSV/JSON for external auditing (SEC/tax compliance if tokens become currency)

**Observability Stack**:

| Component | Tool | Purpose |
|-----------|------|---------|
| **Metrics** | Prometheus | Collect time-series metrics from all services |
| **Dashboards** | Grafana | Visualize metrics with custom dashboards |
| **Logging** | Loki / ELK Stack | Centralized log aggregation and search |
| **Tracing** | Jaeger / Zipkin | Distributed tracing (tracker → peer → Kafka) |
| **Alerting** | Alertmanager | Alert routing to PagerDuty/Slack |
| **Audit** | PostgreSQL | Immutable transaction audit trail |

**Health Check Endpoints** (for monitoring):

```http
GET /health
Response:
{
  "status": "healthy",
  "version": "1.0.0",
  "uptime_seconds": 86400,
  "peers_online": 127,
  "contexts_total": 4567,
  "token_velocity_7d": 1.2,
  "balance_supply_demand_ratio": 0.95
}

GET /metrics (Prometheus format)
# HELP p2p_token_velocity Token earn → spend cycle time (cycles per week)
# TYPE p2p_token_velocity gauge
p2p_token_velocity 1.2

# HELP p2p_active_peers Number of peers announcing in last 24h
# TYPE p2p_active_peers gauge
p2p_active_peers 127

# HELP p2p_context_retrieval_latency_seconds Context retrieval latency
# TYPE p2p_context_retrieval_latency_seconds histogram
p2p_context_retrieval_latency_seconds_bucket{le="0.1"} 450
p2p_context_retrieval_latency_seconds_bucket{le="0.5"} 920
p2p_context_retrieval_latency_seconds_bucket{le="1.0"} 980
p2p_context_retrieval_latency_seconds_count 1000
```

**Monitoring Maturity Roadmap**:

- **Phase 1** (Weeks 1-2): Basic metrics (active peers, token velocity, latency)
- **Phase 1** (Weeks 3-4): Alerting (critical thresholds, PagerDuty integration)
- **Phase 2** (Weeks 5-8): Advanced dashboards (token flow visualization, reputation distribution)
- **Phase 3** (Weeks 9-12): Audit logging (dispute resolution, compliance reporting)
- **Phase 3+** (Ongoing): ML anomaly detection (Sybil attacks, unusual patterns)

#### 4. Context Serving Protocol

**HTTP API** (or gRPC for efficiency):

```http
GET /context/{hash}
Authorization: Bearer {peer_token}
X-Requester-Peer-ID: peer_a

Response:
200 OK
Content-Type: application/json
X-Token-Cost: 10
X-Context-Size-Bytes: 5120

{
  "hash": "sha256:abc123...",
  "content": "...",
  "metadata": {...},
  "embedding": [...]
}
```

**Token Deduction**:
- Automatic via smart contract or API call
- Requester signs transaction with private key
- Server validates signature and deducts tokens
- Server credits tokens to serving peer

#### Transaction Idempotency and Semantics

**Transaction Semantics**: **Exactly-once delivery** (strongest guarantee)
- Each transaction has unique `transaction_id` (UUID)
- Kafka producer uses `enable.idempotence=true` (prevents duplicates)
- Consumer tracks `transaction_id` in processed set (deduplication)

**Idempotency Handling**:

| Scenario | Problem | Solution |
|----------|---------|----------|
| Requester crash before ack | Charged twice (retry without context) | Transaction ID in request; server checks if already processed |
| Server crash mid-transaction | Partial deduction (deducted but not credited) | Atomic transaction: Publish both debit + credit events in same Kafka transaction |
| Network timeout | Requester doesn't know if charged | Response includes `transaction_id`; requester can query status |
| Duplicate request (retry) | Double charge | Server maintains recently processed transaction IDs (TTL 5 min) |

**Atomicity Guarantees**:
1. **Signature validation fails mid-transaction** → No events published (rollback)
   - Validation happens BEFORE publishing to Kafka
   - Response: `401 Unauthorized` with reason

2. **Balance insufficient mid-transaction** → No events published (rollback)
   - Balance check happens BEFORE publishing to Kafka
   - Response: `402 Payment Required` with current balance

3. **Both debit + credit succeed or neither**:
   ```python
   # Kafka transactional producer
   producer.begin_transaction()
   try:
       producer.send("token.transactions.v1", debit_event)
       producer.send("token.transactions.v1", credit_event)
       producer.commit_transaction()
   except:
       producer.abort_transaction()
   ```

**Smart Contract Necessity**:
- **Phase 1**: **NO smart contract** (API-level transactions via Kafka)
  - Sufficient for centralized tracker + Kafka ledger
  - Kafka transactions provide atomicity (all-or-nothing)
  - Lower complexity, faster development

- **Phase 2**: **Optional smart contract** (for DHT/decentralized)
  - Required only if removing central authority
  - Blockchain-backed (Ethereum, Solana, or custom chain)
  - Higher complexity, slower transactions, but trustless

- **Recommendation**: Start with Kafka transactions (Phase 1), add smart contracts only if decentralization requires it (Phase 2+)

**Transaction Flow (Exactly-Once)**:
```
1. Requester generates transaction_id (UUID)
2. Requester signs: {transaction_id, peer_id, context_hash, max_cost}
3. Server validates signature
4. Server checks balance >= cost
5. Server checks transaction_id not already processed (deduplication)
6. Server begins Kafka transaction
7. Server publishes debit event (requester)
8. Server publishes credit event (server)
9. Server commits Kafka transaction (atomic)
10. Server returns context + transaction_id (requester can verify)
```

**Idempotency Cache**:
- Store processed `transaction_id` in Valkey with 5-minute TTL
- Key: `tx:processed:{transaction_id}` → Value: `{status, timestamp}`
- On duplicate request: Return cached result (idempotent response)

---

## Economic Model Details

### Token Earning (Supply Side)

**How peers earn tokens**:

| Activity | Tokens Earned | Notes |
|----------|---------------|-------|
| **Serve context** | 2 tokens/KB | Bandwidth incentive |
| **Contribute new context** | 100-1000 tokens | One-time reward for novel content |
| **Maintain high uptime** | 10 tokens/day | Reliability bonus |
| **Verify other peers' context** | 5 tokens/verification | Quality control |
| **Run tracker node** | 50 tokens/day | Infrastructure support |

**Example earnings**:
- Serve 100 KB/day → 200 tokens/day
- Contribute 10 new documents → 1000 tokens one-time
- 99% uptime → 10 tokens/day
- **Total**: ~210 tokens/day sustainable

### Token Spending (Demand Side)

**How peers spend tokens**:

| Activity | Tokens Spent | Notes |
|----------|--------------|-------|
| **Retrieve context from peer** | 2 tokens/KB | Matches serving rate |
| **LLM inference** | Variable | Based on provider pricing |
| **Embedding generation** | 0.1 tokens/doc | For new contributions |
| **Priority queries** | 2x normal | Fast lane for urgent requests |

**Example spending**:
- Retrieve 50 KB/day → 100 tokens/day
- 10 LLM queries/day → 100 tokens/day (avg 10 tokens/query)
- **Total**: ~200 tokens/day sustainable

**Equilibrium**: Average peer earns ~210 tokens/day, spends ~200 tokens/day → slight surplus encourages participation

### Token Economics Calibration

**Initial parameters** (adjustable dynamically):
```yaml
# Serving rates
tokens_per_kb_served: 2
new_context_bonus: 100-1000  # Based on uniqueness
uptime_bonus: 10/day
verification_reward: 5

# Retrieval costs
tokens_per_kb_retrieved: 2  # Matches serving
llm_cost_multiplier: 1.0    # 1:1 with provider pricing
embedding_cost_per_doc: 0.1

# Free tier
new_user_starter_tokens: 1000
daily_free_tier_tokens: 50  # For light usage

# Reputation
min_reputation_to_earn: 0.5
max_reputation_bonus: 1.5x
reputation_decay_rate: 0.01/day (if offline)
```

**Dynamic adjustments**:
- If token velocity too high → increase earning rates
- If token velocity too low → decrease earning rates
- If peers hoard tokens → add expiration (e.g., 10% decay/year)
- If peers run out → increase free tier allocation

### Token Economics Tuning Strategy

**Initial Tuning Approach**:

1. **Simulation Phase** (Pre-Launch):
   - Monte Carlo simulation with 10/100/1000 peer scenarios
   - Test earning/spending rates with synthetic workloads
   - Model hoarding, freeloading, and Sybil attack scenarios
   - Validate token velocity targets (>1 cycle/week achievable)
   - Deliverable: Calibrated initial parameters with confidence intervals

2. **A/B Testing Phase** (First 3 Months):
   - Split network into cohorts (e.g., Cohort A: 2 tokens/KB, Cohort B: 3 tokens/KB)
   - Measure retention, satisfaction, token velocity per cohort
   - Iterate parameters every 2 weeks based on metrics
   - Deliverable: Optimal parameter set with empirical validation

3. **Continuous Monitoring Phase** (Ongoing):
   - Real-time dashboards (token velocity, balance distribution, peer churn)
   - Weekly review of economic health metrics
   - Automated alerts trigger manual review
   - Quarterly parameter adjustments based on trends

**Escape Hatches**:

| Scenario | Detection Signal | Intervention | Timeline |
|----------|-----------------|--------------|----------|
| **Earning rates too low** | Peer churn >20%/week | Increase earning rates by 50% | 24 hours |
| **Earning rates too high** | Token supply growth >50%/week | Decrease earning rates by 30% | 48 hours |
| **Hyperinflation** | Supply doubling <7 days | Emergency supply cap + rate freeze | 12 hours |
| **Mass exodus** | Active peers drop >50% in 7 days | 10x free tier + emergency airdrop | 6 hours |
| **Stagnation** | Token velocity <0.3 cycles/week | Force expiration (20% decay/year) | 7 days |

**Monitoring Thresholds** (Trigger Intervention):

| Metric | Green Zone | Yellow Zone (Review) | Red Zone (Intervene) |
|--------|-----------|---------------------|---------------------|
| **Token Velocity** | >1.0 cycles/week | 0.5-1.0 cycles/week | <0.5 cycles/week |
| **Supply/Demand Ratio** | 0.8-1.2 | 0.5-0.8 or 1.2-1.5 | <0.5 or >1.5 |
| **Peer Profitability %** | >70% | 50-70% | <50% |
| **Token Hoarding Rate** | <30% peers hoard >1000 | 30-50% peers hoard | >50% peers hoard |
| **Supply Growth Rate** | <20%/week | 20-40%/week | >40%/week |
| **Active Peer Count** | Growing or stable | Declining <10%/week | Declining >10%/week |

**Intervention Playbook**:

1. **Yellow Zone** → Schedule review meeting (within 3 days)
   - Analyze root cause (behavior patterns, external factors)
   - Prepare parameter adjustment proposal
   - Communicate to network (transparency builds trust)

2. **Red Zone** → Immediate intervention (within 24 hours)
   - Execute predefined escape hatch (see table above)
   - Emergency governance vote if major parameter change
   - Post-mortem analysis (document learnings)

**Parameter Adjustment Constraints**:
- **Maximum change per adjustment**: ±30% (avoid shock to economy)
- **Minimum time between adjustments**: 7 days (allow metrics to stabilize)
- **Grandfathering**: Existing balances unaffected by rate changes (fairness)
- **Transparency**: All parameter changes announced 48 hours in advance

**Example Intervention Scenario**:
```
Week 1: Supply growing 45%/week (RED ZONE)
Action: Decrease earning rates from 2 tokens/KB → 1.4 tokens/KB (30% cut)
Announcement: "To maintain token sustainability, earning rates temporarily reduced.
               Will review in 2 weeks based on supply growth trends."

Week 3: Supply growth reduced to 15%/week (GREEN ZONE)
Action: Maintain new rates for 4 more weeks to confirm stability

Week 7: Supply growth stable at 10-15%/week, peer satisfaction stable
Decision: Keep rates at 1.4 tokens/KB (new equilibrium found)
```

**Validation Metrics** (Is tuning working?):
- Token velocity converges to 1.0-1.5 cycles/week (stable)
- Peer churn rate <5%/week (retention)
- Supply/demand ratio oscillates around 1.0 ± 0.2 (balance)
- Intervention frequency decreases over time (maturity)

---

## Implementation Phases

### Phase 1: Centralized P2P Tracker (4-6 weeks)

**Goal**: Prove P2P discovery works with centralized tracker

**Team Size Assumptions**:
- **1-2 engineers**: 6 weeks (serial development, limited testing)
- **3-4 engineers**: 4 weeks (parallel development, adequate testing)
- **5+ engineers**: 3 weeks (full parallelization, comprehensive testing)

**Recommendation**: 3-4 engineers for optimal balance (4-week timeline below assumes this)

**Deliverables**:
- Tracker server (announce, scrape, peer_list endpoints)
- Peer client library (announce context, discover peers)
- **Token ledger (Kafka topics)** ⭐ **LEVERAGE EXISTING INFRASTRUCTURE**
  - Create `token.transactions.v1` and `token.balances.v1` topics
  - Token balance consumer (updates compacted topic)
  - Valkey cache integration (fast balance reads)
- Context serving protocol (HTTP API)
- Metadata graph (local SQLite or Memgraph)
- Integration testing suite (Kafka consumer testing, schema migrations)
- Performance benchmarks (token transaction throughput, balance query latency)

**Phase 1 Milestones** (4-week timeline):

#### Week 1: Foundation + Infrastructure
- **Milestone 1.1: Tracker Server Core** (Days 1-3)
  - FastAPI server with `/announce`, `/scrape`, `/peer_list` endpoints
  - In-memory peer registry (PostgreSQL integration later)
  - Health check endpoints
  - Docker container + basic tests

- **Milestone 1.2: Kafka Topics + Schema** (Days 4-5)
  - Create `token.transactions.v1` and `token.balances.v1` topics in Redpanda
  - Avro/Protobuf schema definitions
  - Schema registry integration
  - Topic configuration (partitions, replication, retention)

- **Integration Checkpoint** (End of Week 1):
  - ✅ Tracker server responds to health checks
  - ✅ Kafka topics created and accepting test events
  - ✅ Schema validation working

#### Week 2: Token Ledger + Peer Client
- **Milestone 2.1: Token Balance Consumer** (Days 6-8)
  - Kafka consumer reads `token.transactions.v1`
  - Updates `token.balances.v1` compacted topic
  - Valkey cache integration (write-through)
  - Exactly-once semantics (idempotency)
  - Consumer lag monitoring

- **Milestone 2.2: Peer Client Library** (Days 9-10)
  - Python library: `announce_context()`, `discover_peers()`, `retrieve_context()`
  - Token transaction signing (private key)
  - Automatic heartbeat (30s interval)
  - Retry logic with exponential backoff

- **Integration Checkpoint** (End of Week 2):
  - ✅ Token consumer processes transactions and updates balances
  - ✅ Peer client can register with tracker
  - ✅ Balance queries return correct values from Valkey

#### Week 3: Context Serving + Metadata Graph
- **Milestone 3.1: Context Serving API** (Days 11-13)
  - `GET /context/{hash}` endpoint with token deduction
  - Signature validation (verify requester)
  - Balance checking (402 Payment Required if insufficient)
  - Atomic token transactions (Kafka transactional producer)
  - Content storage (S3 or local filesystem)

- **Milestone 3.2: Metadata Graph** (Days 14-15)
  - Local SQLite or Memgraph setup
  - Context node schema (hash, title, embedding, tags, peers)
  - Basic graph queries (find by hash, similarity search)
  - Integration with peer client (announce contexts to tracker)

- **Integration Checkpoint** (End of Week 3):
  - ✅ Context retrieval works end-to-end (peer A → tracker → peer B → context)
  - ✅ Token deduction and crediting functional
  - ✅ Metadata graph populated with test contexts

#### Week 4: Testing + Performance Validation
- **Milestone 4.1: Integration Testing** (Days 16-18)
  - Multi-peer scenario tests (10+ simulated peers)
  - Kafka consumer lag tests (handle backpressure)
  - Schema migration tests (add fields, ensure backward compatibility)
  - Failure scenario tests (peer offline, insufficient balance, network timeout)
  - Load testing (100+ concurrent requests)

- **Milestone 4.2: Performance Benchmarks** (Days 19-20)
  - Token transaction throughput: Target >1000 tx/sec
  - Balance query latency: Target <10ms (from Valkey)
  - Context retrieval end-to-end: Target <200ms (P95)
  - Consumer lag under load: Target <100ms

- **Final Integration Checkpoint** (End of Week 4):
  - ✅ All integration tests passing (10+ peers scenario)
  - ✅ Performance benchmarks meet targets
  - ✅ Schema migrations tested (forward/backward compatible)
  - ✅ Monitoring dashboards deployed (token velocity, balance distribution)
  - ✅ Documentation complete (API docs, runbook, architecture diagrams)

**Testing Scope Breakdown**:

| Test Type | Coverage | Timeline |
|-----------|----------|----------|
| **Unit Tests** | Tracker endpoints, peer client methods, consumer logic | Ongoing (each milestone) |
| **Integration Tests** | Multi-peer scenarios, Kafka consumer lag, schema migrations | Week 4 (Days 16-18) |
| **Load Tests** | 100+ concurrent peers, 1000+ tx/sec | Week 4 (Day 19) |
| **Failure Tests** | Peer offline, insufficient balance, network partition | Week 4 (Day 18) |
| **Schema Migration** | Add/remove fields, backward compatibility | Week 4 (Day 17) |

**Risk Mitigation**:
- **Integration delays** (Kafka consumer issues) → Allocate 2 extra days buffer in Week 2
- **Schema migration complexity** → Test early with Avro schema evolution in Week 1
- **Performance bottlenecks** → Benchmark Valkey + Kafka separately in Week 2 (before integration)

**Architecture**:
```
┌──────────┐      ┌──────────┐      ┌──────────┐
│ Peer A   │◀────▶│ Tracker  │◀────▶│ Peer B   │
│ (client) │      │ (central)│      │ (client) │
└──────────┘      └──────────┘      └──────────┘
     │                                    │
     └────────── P2P content ────────────┘
              (direct connection)
                       ↓
            Token transaction events
                       ↓
         ┌────────────────────────────┐
         │  Kafka/Redpanda (ledger)   │
         │  - token.transactions.v1   │
         │  - token.balances.v1       │
         └────────────────────────────┘
```

**Benefits of Kafka Ledger**:
- ✅ **No new database** - use existing Redpanda infrastructure
- ✅ **Event sourcing** - natural audit trail
- ✅ **Replay capability** - rebuild state from transaction log
- ✅ **Distributed** - already replicated and durable

**Success metrics**:
- 10+ peers register with tracker
- 100+ context chunks served peer-to-peer
- Token earning/spending via Kafka events works correctly
- 50%+ cost reduction vs full-context LLM queries
- Token balance consumer lag <100ms

### Phase 2: DHT for Decentralization (3-4 weeks)

**Goal**: Remove centralized tracker dependency

**Deliverables**:
- Distributed Hash Table (Kademlia or Chord)
- Peer discovery without central tracker
- Content-addressable storage (IPFS-like)
- Reputation tracking across DHT
- Token ledger consensus (simple PoA or federated)

**Architecture**:
```
┌──────────┐      ┌──────────┐      ┌──────────┐
│ Peer A   │◀────▶│ Peer B   │◀────▶│ Peer C   │
│ (DHT)    │      │ (DHT)    │      │ (DHT)    │
└──────────┘      └──────────┘      └──────────┘
     ▲                  ▲                  ▲
     │                  │                  │
     └──────────────────┴──────────────────┘
              Fully decentralized P2P
```

**Success metrics**:
- No single point of failure
- 100+ peers in DHT
- Context discovery <500ms
- 99% uptime without central server

### Phase 3: Advanced Features (4-6 weeks)

**Goal**: Production-ready with advanced economics

**Deliverables**:
- Reputation-weighted token rewards
- Quality verification (peers validate each other's content)
- Context deduplication (same content from multiple peers)
- Caching layer (frequently accessed content cached locally)
- Analytics dashboard (token flow, peer stats, network health)
- Privacy controls (encrypted context, access control)

**Advanced economics**:
- Reputation multiplier (high-rep peers earn 1.5x)
- Staking (lock tokens for higher reputation)
- Slashing (bad actors lose staked tokens)
- Governance (token holders vote on parameter changes)

**Success metrics**:
- 1000+ peers in network
- 10,000+ context chunks served/day
- 90%+ cost reduction vs full-context LLM
- <5% bad actors (reputation filtering)

---

## Technical Specifications

### Peer Discovery Protocol

**Announce** (every 30s heartbeat):
```json
POST /announce
{
  "peer_id": "abc123...",
  "ip": "192.168.1.100",
  "port": 6881,
  "contexts": [
    {
      "hash": "sha256:...",
      "title": "ONEX Effect Node",
      "size_bytes": 4096,
      "tags": ["onex", "effect"]
    }
  ],
  "uptime_pct": 99.2,
  "reputation": 0.95,
  "token_balance": 5000
}

Response:
{
  "status": "ok",
  "peers_online": 127,
  "contexts_total": 4567
}
```

**Scrape** (find peers with context):
```json
GET /scrape?context_hash=sha256:abc123...

Response:
{
  "peers": [
    {
      "peer_id": "def456...",
      "ip": "192.168.1.101",
      "port": 6881,
      "reputation": 0.98,
      "uptime_pct": 99.5,
      "latency_ms": 12,
      "token_cost": 10
    }
  ],
  "total_peers": 3,
  "avg_latency_ms": 15
}
```

### Context Retrieval Protocol

**Request context**:
```http
GET /context/{hash}
Authorization: Bearer {peer_token}
X-Requester-Peer-ID: peer_a
X-Max-Token-Cost: 50

Response (Success):
200 OK
Content-Type: application/json
X-Token-Cost: 10
X-Context-Size-Bytes: 5120
X-Peer-Reputation: 0.95
X-Transaction-ID: 550e8400-e29b-41d4-a716-446655440000

{
  "hash": "sha256:abc123...",
  "content": "...",
  "metadata": {
    "title": "ONEX Effect Node",
    "author": "peer_xyz",
    "created_at": "2025-10-01T00:00:00Z",
    "tags": ["onex", "effect"],
    "references": ["sha256:def456..."]
  },
  "embedding": [0.123, 0.456, ...]
}

Response (Insufficient Balance):
402 Payment Required
Content-Type: application/json

{
  "error": "insufficient_balance",
  "message": "Requester has insufficient tokens to retrieve context",
  "required_tokens": 10,
  "current_balance": 5,
  "deficit": 5,
  "peer_id": "peer_a",
  "context_hash": "sha256:abc123...",
  "options": [
    "Earn more tokens by serving context to other peers",
    "Request free tier tokens (50 tokens/day)",
    "Contact network governance for emergency allocation"
  ]
}

Response (Invalid Signature):
401 Unauthorized
Content-Type: application/json

{
  "error": "invalid_signature",
  "message": "Request signature validation failed",
  "peer_id": "peer_a",
  "reason": "Signature does not match peer's public key"
}

Response (Context Not Found):
404 Not Found
Content-Type: application/json

{
  "error": "context_not_found",
  "message": "Requested context hash does not exist",
  "context_hash": "sha256:abc123...",
  "suggestions": [
    "Check hash spelling",
    "Context may have been removed by author",
    "Query tracker for available contexts"
  ]
}

Response (Rate Limited):
429 Too Many Requests
Content-Type: application/json
Retry-After: 60

{
  "error": "rate_limit_exceeded",
  "message": "Too many requests from this peer",
  "peer_id": "peer_a",
  "limit": "100 requests per minute",
  "retry_after_seconds": 60
}

Response (Server Error):
500 Internal Server Error
Content-Type: application/json

{
  "error": "internal_error",
  "message": "Failed to retrieve context due to server error",
  "transaction_id": "550e8400-e29b-41d4-a716-446655440000",
  "support": "Contact support@omninode.ai with transaction_id for assistance"
}
```

**Token deduction flow**:
1. Requester includes signed transaction in request
2. Server validates signature → If invalid, return 401 Unauthorized
3. Server checks balance >= cost → If insufficient, return 402 Payment Required
4. Server checks transaction_id not already processed (idempotency) → If duplicate, return cached result
5. Server deducts tokens from requester's balance (via Kafka transaction)
6. Server credits tokens to own balance (via Kafka transaction)
7. Transaction logged to ledger (both debit + credit atomic)
8. If any step fails → Rollback Kafka transaction, return appropriate error
9. Return context + transaction_id (requester can verify)

### Content Verification Protocol

**Submit verification**:
```json
POST /verify
{
  "verifier_peer_id": "ghi789...",
  "context_hash": "sha256:abc123...",
  "verification": "valid" | "invalid" | "outdated",
  "reason": "Content matches description, high quality",
  "signature": "..."
}

Response:
{
  "status": "ok",
  "reward_tokens": 5,
  "context_reputation_updated": true
}
```

**Reputation update**:
- Valid verification → Content author gains 0.01 reputation
- Invalid verification → Content author loses 0.05 reputation
- Verifier earns 5 tokens per verification
- Multiple verifications required for consensus (e.g., 3+ verifiers)

### Reputation Scoring Mechanism (Complete Specification)

**Reputation Initialization**:
- **New peers**: Start at 1.0 reputation (neutral)
- **Justification**:
  1. **Trust by default**: Encourages participation (easier to join)
  2. **Sybil resistance via time**: New peers earn tokens slowly (rate-limited for first 30 days)
  3. **Earn full reputation**: Requires sustained good behavior (6+ months)
  4. **Alternative considered**: Start at 0.5 → Rejected (creates cold-start problem, discourages new peers)

**Verification Impact Scaling**:

| Verification Type | Base Impact | Scaled by Verifier Count | Formula |
|------------------|-------------|--------------------------|---------|
| **Valid content** | +0.01 | Yes (diminishing returns) | `+0.01 / sqrt(verifier_count)` |
| **Invalid content** | -0.05 | Yes (amplified penalty) | `-0.05 * sqrt(verifier_count)` |
| **Outdated content** | -0.02 | Yes (moderate penalty) | `-0.02 * sqrt(verifier_count)` |

**Examples**:
- 1 verifier says valid → +0.01 reputation
- 4 verifiers say valid → +0.01/2 = +0.005 each (total +0.02)
- 9 verifiers say valid → +0.01/3 = +0.0033 each (total +0.03)
- 1 verifier says invalid → -0.05 reputation
- 4 verifiers say invalid → -0.05*2 = -0.10 each (total -0.40) ← **SEVERE**

**Rationale**: Diminishing returns prevent reputation farming (can't get 100 friends to verify same content for +1.0 rep), while amplified penalties quickly remove bad actors.

**Minimum Reputation Threshold** (Sybil Prevention):
- **Earn tokens**: reputation ≥ 0.5 (below this, no earnings)
- **Serve context**: reputation ≥ 0.3 (can still serve, but flagged as low-quality)
- **Verify content**: reputation ≥ 0.7 (high-quality verifiers only)
- **Run tracker node**: reputation ≥ 0.9 (infrastructure requires trust)

**Blocklist Criteria** (Automatic):
- reputation < 0.3 → **Soft ban** (cannot earn tokens, can retrieve with own balance)
- reputation < 0.0 → **Hard ban** (blocked from network, peer_id blacklisted)
- 3+ verified malicious actions (spam, fake content, Sybil attack) → **Immediate hard ban**

**Malicious Action Detection**:
- **Spam**: >10 context announcements/minute (rate limit)
- **Fake content**: 3+ invalid verifications from high-rep peers (consensus)
- **Sybil attack**: Multiple peer_ids from same IP + similar announce times

**Anti-Sybil Strategy**:

1. **Time-based earning limits** (first 30 days):
   - Days 0-7: Max 50 tokens/day (25% of normal)
   - Days 8-14: Max 100 tokens/day (50% of normal)
   - Days 15-30: Max 150 tokens/day (75% of normal)
   - Days 31+: Max 200 tokens/day (full rate)

2. **Vouching system** (optional, Phase 2+):
   - New peers can request vouch from high-rep peer (reputation ≥ 0.9)
   - Vouching bypasses earning limits (instant trust transfer)
   - Voucher loses 0.1 reputation if vouched peer turns malicious
   - **Incentive**: Vouching earns 50 tokens (risk vs. reward)

3. **Proof-of-work** (lightweight, prevents mass registration):
   - New peer must solve SHA-256 puzzle (difficulty: ~10 seconds on average laptop)
   - Puzzle refreshed every 24 hours (prevents pre-computation)
   - **Cost**: 10 seconds upfront vs. ~$0 in actual cost (Sybil must spend real time)

4. **IP-based rate limiting** (network-level):
   - Max 5 peer registrations per IP per 24 hours
   - Max 10 peer registrations per /24 subnet per 24 hours
   - Exception: Known VPN/Tor exit nodes (allow, but with higher reputation threshold = 0.8)

5. **Behavioral analysis** (ML-based, Phase 3):
   - Peers with similar announce patterns → flagged for manual review
   - Peers with identical context hashes but different peer_ids → suspicious
   - Sudden reputation spikes (0.5 → 0.9 in <7 days) → anomaly detection

**Reputation Recovery Path** (for false positives):
- Soft ban (0.3-0.5) → Can appeal to governance (submit evidence of good behavior)
- Hard ban (<0.0) → Permanent (peer_id blacklisted, must create new peer_id)
- Appeal process: 3 high-rep peers (≥0.9) vote to restore (2/3 majority required)

**Reputation Decay** (for inactivity):
- Offline >30 days → -0.01 reputation/day
- Offline >90 days → Peer marked inactive (removed from tracker, can re-register)
- **Rationale**: Discourages squatting on high-reputation accounts

**Reputation Leaderboard** (optional, gamification):
- Top 100 peers by reputation displayed publicly
- High-rep peers earn 1.5x tokens (reputation bonus)
- **Risk**: Reputation farming incentive (mitigated by diminishing returns)

**Reputation Metrics Dashboard** (for monitoring):
- Average network reputation: Target 0.8 (healthy)
- % peers with reputation <0.5: Target <5% (bad actor rate)
- Reputation distribution: Target bell curve centered at 0.8

**Consensus Verification Threshold**:
- **Minimum verifiers**: 3 (for content reputation update)
- **Reputation-weighted voting**: Yes
  - Verifier reputation 0.9 → vote weight = 1.0
  - Verifier reputation 0.7 → vote weight = 0.7
  - Verifier reputation 0.5 → vote weight = 0.5
- **Consensus formula**:
  ```
  total_weight_valid = sum(verifier_reputation for valid votes)
  total_weight_invalid = sum(verifier_reputation for invalid votes)

  IF total_weight_valid > total_weight_invalid * 2:
      content is VALID (update reputation)
  ELSE IF total_weight_invalid > total_weight_valid * 2:
      content is INVALID (update reputation + flag author)
  ELSE:
      INCONCLUSIVE (no reputation update, wait for more verifiers)
  ```
- **Example**:
  - 2 verifiers (0.9, 0.8) say valid → total_weight_valid = 1.7
  - 1 verifier (0.6) says invalid → total_weight_invalid = 0.6
  - 1.7 > 0.6 * 2 (1.2) → **VALID** (content author gains reputation)

---

## Database Schema

### Peer Registry

```sql
CREATE TABLE peers (
    peer_id TEXT PRIMARY KEY,
    ip_address INET,
    port INT,
    public_key TEXT,
    reputation NUMERIC DEFAULT 1.0,
    uptime_pct NUMERIC,
    last_seen_at TIMESTAMPTZ DEFAULT NOW(),
    total_contexts_served BIGINT DEFAULT 0,
    total_bytes_served BIGINT DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_peers_reputation ON peers(reputation DESC);
CREATE INDEX idx_peers_last_seen ON peers(last_seen_at DESC);
```

### Context Registry

**Embedding Model Choice**: `all-MiniLM-L6-v2` (384 dimensions) or `all-mpnet-base-v2` (768 dimensions)
- **Recommendation**: `all-MiniLM-L6-v2` for Phase 1 (faster, smaller, sufficient quality)
- **Upgrade path**: `all-mpnet-base-v2` for Phase 2+ (higher quality, requires more storage)
- **Configurable**: Set via environment variable `EMBEDDING_MODEL` and `EMBEDDING_DIMENSION`

```sql
-- Make embedding dimension configurable via application constant
-- Default: 384 (all-MiniLM-L6-v2), can be changed to 768 (all-mpnet-base-v2)
-- NOTE: Dimension must match embedding model in application code

CREATE TABLE contexts (
    hash TEXT PRIMARY KEY,
    title TEXT,
    description TEXT,
    size_bytes BIGINT,
    embedding VECTOR(384), -- Configurable: 384 (MiniLM) or 768 (MPNet)
    tags TEXT[],
    author_peer_id TEXT REFERENCES peers(peer_id),
    reputation NUMERIC DEFAULT 1.0,
    verification_count INT DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- INDEXES FOR CONTEXTS TABLE

-- 1. Embedding similarity (RAG queries)
-- Using ivfflat for approximate nearest neighbor (ANN) search
-- ivfflat parameters: lists=100 for ~10K vectors, scale up to lists=1000 for 100K+ vectors
CREATE INDEX idx_contexts_embedding ON contexts USING ivfflat(embedding vector_cosine_ops) WITH (lists=100);

-- 2. Tag-based filtering (multi-tag queries)
CREATE INDEX idx_contexts_tags ON contexts USING GIN(tags);

-- 3. Reputation filtering (find high-quality contexts)
CREATE INDEX idx_contexts_reputation ON contexts(reputation DESC);

-- 4. Author lookup (find all contexts by peer)
CREATE INDEX idx_contexts_author ON contexts(author_peer_id);

-- 5. Size filtering (find small/large contexts)
CREATE INDEX idx_contexts_size ON contexts(size_bytes);

-- PERFORMANCE TESTING PLAN FOR PGVECTOR
-- Test with 10K+ contexts to validate ivfflat performance:
-- 1. Measure query latency (target: <50ms for top 10 similar contexts)
-- 2. Tune ivfflat lists parameter (100 → 500 → 1000) based on dataset size
-- 3. Compare ivfflat vs. hnsw index (hnsw better for read-heavy workloads)
-- 4. Monitor index build time (ivfflat: ~1-2 min for 10K vectors)

-- PGVECTOR EXTENSION REQUIREMENT
-- Install: CREATE EXTENSION IF NOT EXISTS vector;
-- Version: >=0.5.0 (for ivfflat + hnsw support)
-- Docs: https://github.com/pgvector/pgvector
```

### Peer-Context Mapping

```sql
CREATE TABLE peer_contexts (
    peer_id TEXT REFERENCES peers(peer_id),
    context_hash TEXT REFERENCES contexts(hash),
    announced_at TIMESTAMPTZ DEFAULT NOW(),
    online_status BOOLEAN DEFAULT TRUE, -- Track if peer is currently serving
    PRIMARY KEY (peer_id, context_hash)
);

-- INDEXES FOR PEER_CONTEXTS TABLE

-- 1. Peer lookup (find all contexts for a peer)
CREATE INDEX idx_peer_contexts_peer ON peer_contexts(peer_id);

-- 2. Context lookup (find all peers with a context)
CREATE INDEX idx_peer_contexts_context ON peer_contexts(context_hash);

-- 3. Peer selection (find online peers with high reputation)
-- Composite index for efficient peer discovery queries
CREATE INDEX idx_peer_contexts_selection ON peer_contexts(context_hash, online_status)
    WHERE online_status = TRUE;

-- 4. Announced time (find recently announced contexts)
CREATE INDEX idx_peer_contexts_announced ON peer_contexts(announced_at DESC);
```

### Token Ledger (Kafka/Redpanda Topics)

**Source of Truth**: Kafka topics (already deployed in your infrastructure)

```yaml
# Token transaction events (append-only log)
token.transactions.v1:
  partitions: 10
  replication_factor: 3
  retention: unlimited
  cleanup_policy: delete  # Keep all transactions

# Peer balances (compacted topic - latest balance only)
token.balances.v1:
  partitions: 10
  replication_factor: 3
  retention: unlimited
  cleanup_policy: compact  # Only keep latest balance per peer_id

# Token analytics (materialized view for queries)
token.analytics.v1:
  partitions: 1
  replication_factor: 3
  retention: 30 days
  cleanup_policy: delete
```

**Consumer Groups**:
- `token-balance-updater`: Consumes transactions, updates balances
- `token-analytics`: Materializes to PostgreSQL for complex queries
- `token-cache-updater`: Updates Valkey cache for fast reads

**PostgreSQL** (analytics only, not source of truth):
```sql
-- Materialized view of balances (rebuilt from Kafka if needed)
CREATE TABLE peer_token_balances_mv (
    peer_id TEXT PRIMARY KEY,
    balance NUMERIC DEFAULT 0,
    total_earned NUMERIC DEFAULT 0,
    total_spent NUMERIC DEFAULT 0,
    last_transaction_id UUID,
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Analytics queries (time-series, aggregates)
CREATE TABLE token_analytics (
    date DATE,
    peer_id TEXT,
    earned_today NUMERIC,
    spent_today NUMERIC,
    balance_end_of_day NUMERIC,
    PRIMARY KEY (date, peer_id)
);

-- INDEX FOR ANALYTICS
CREATE INDEX idx_token_analytics_date ON token_analytics(date DESC);
CREATE INDEX idx_token_analytics_peer ON token_analytics(peer_id);

-- LEDGER ARCHIVAL STRATEGY
-- Problem: Kafka topics with unlimited retention grow indefinitely
-- Solution: Archive old transactions to cold storage after N months

-- ARCHIVAL POLICY:
-- 1. Keep last 6 months in Kafka (hot storage, fast queries)
-- 2. Archive 6-12 months to PostgreSQL (warm storage, slower queries)
-- 3. Archive 12+ months to S3/Glacier (cold storage, rare access)

-- Implementation:
-- 1. Kafka retention: 180 days (6 months) for token.transactions.v1
-- 2. Nightly job: Archive transactions older than 180 days to PostgreSQL
-- 3. Quarterly job: Move PostgreSQL transactions older than 12 months to S3

CREATE TABLE token_transactions_archive (
    transaction_id UUID PRIMARY KEY,
    peer_id TEXT,
    transaction_type TEXT,
    amount NUMERIC,
    reason TEXT,
    context_hash TEXT,
    counterparty_peer_id TEXT,
    timestamp TIMESTAMPTZ,
    archived_at TIMESTAMPTZ DEFAULT NOW()
);

-- INDEXES FOR ARCHIVE TABLE
CREATE INDEX idx_archive_peer ON token_transactions_archive(peer_id);
CREATE INDEX idx_archive_timestamp ON token_transactions_archive(timestamp DESC);
CREATE INDEX idx_archive_counterparty ON token_transactions_archive(counterparty_peer_id);

-- ARCHIVAL BENEFITS:
-- 1. Reduced Kafka storage costs (retain only recent transactions)
-- 2. Maintain complete audit trail (archive never deleted)
-- 3. Performance: Hot queries (last 6 months) remain fast
-- 4. Compliance: Full transaction history for disputes/audits
```

---

## Cost-Benefit Analysis

### Current Full-Context LLM Approach

**Assumptions**:
- Average query: 500KB context
- Input cost: $5/1M tokens (Gemini Pro)
- Output cost: $15/1M tokens
- 1000 queries/day

**Costs**:
```
Input: 500KB × 1000 queries × $5/1M = $2500/day
Output: 50KB × 1000 queries × $15/1M = $750/day
Total: $3250/day = $97,500/month
```

### P2P Context-Sharing Approach

**Assumptions**:
- Average query: 50KB context (90% reduction via RAG)
- P2P retrieval cost: 50KB × 2 tokens/KB = 100 tokens
- Token value: $0.001 per token (based on compute cost)
- LLM cost reduced by 90%

**Costs** (Base case: 90% context reduction):
```
P2P retrieval: 50KB × 1000 queries × $0.001/KB = $50/day
Input: 50KB × 1000 queries × $5/1M = $250/day
Output: 50KB × 1000 queries × $15/1M = $750/day
Total: $1050/day = $31,500/month

Savings: $97,500 - $31,500 = $66,000/month (68% reduction)
```

### Sensitivity Analysis (Context Reduction Rate)

**Validation Data**: Based on existing Qdrant usage in OmniClaude and similar RAG systems:
- **OmniClaude manifest injection**: Reduces context from ~500KB (full docs) to ~50KB (relevant patterns) → **90% reduction**
- **Academic RAG research**: 80-95% context reduction typical for domain-specific retrieval (cited: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", Lewis et al., 2020)
- **Industry benchmarks**: LangChain RAG applications report 70-90% context reduction (OpenAI case studies)

**ROI at Different Context Reduction Rates**:

| Context Reduction | Avg Context Size | Monthly Cost | Savings vs. Baseline | ROI % |
|------------------|------------------|--------------|----------------------|-------|
| **50% reduction** | 250KB | $58,500 | $39,000/month | 40% |
| **70% reduction** | 150KB | $42,000 | $55,500/month | 57% |
| **90% reduction** | 50KB | $31,500 | $66,000/month | 68% |
| **95% reduction** | 25KB | $27,000 | $70,500/month | 72% |

**Cost Breakdown by Reduction Rate**:

```
50% reduction (250KB context):
  P2P retrieval: 250KB × 1000 × $0.001/KB = $250/day
  LLM input: 250KB × 1000 × $5/1M = $1250/day
  LLM output: 50KB × 1000 × $15/1M = $750/day
  Total: $2250/day = $67,500/month (31% savings)

70% reduction (150KB context):
  P2P retrieval: 150KB × 1000 × $0.001/KB = $150/day
  LLM input: 150KB × 1000 × $5/1M = $750/day
  LLM output: 50KB × 1000 × $15/1M = $750/day
  Total: $1650/day = $49,500/month (49% savings)

90% reduction (50KB context):
  P2P retrieval: 50KB × 1000 × $0.001/KB = $50/day
  LLM input: 50KB × 1000 × $5/1M = $250/day
  LLM output: 50KB × 1000 × $15/1M = $750/day
  Total: $1050/day = $31,500/month (68% savings)

95% reduction (25KB context):
  P2P retrieval: 25KB × 1000 × $0.001/KB = $25/day
  LLM input: 25KB × 1000 × $5/1M = $125/day
  LLM output: 50KB × 1000 × $15/1M = $750/day
  Total: $900/day = $27,000/month (72% savings)
```

**Break-Even Analysis**:
- **Minimum reduction for profitability**: 30% (otherwise P2P costs exceed LLM savings)
- **Target reduction**: 70-90% (based on validated OmniClaude performance)
- **Conservative estimate**: 70% reduction (still yields 57% cost savings = $55,500/month)

**Validation Plan**:
1. **Phase 1 Pilot** (first month):
   - Measure actual context reduction rate with 10+ peers
   - Track precision (% relevant context retrieved)
   - Validate cost savings vs. projections

2. **Adjust Economics** (if needed):
   - If reduction <70% → Improve RAG quality (better embeddings, query optimization)
   - If reduction >95% → Consider lower P2P retrieval costs (increase token value)

**Risk Mitigation**:
- **Pessimistic scenario** (50% reduction): Still 40% cost savings ($39K/month)
- **Optimistic scenario** (95% reduction): Up to 72% cost savings ($70.5K/month)
- **Expected scenario** (70-90% reduction): 57-68% cost savings ($55-66K/month)

**Additional benefits**:
- Decentralized (no single point of failure)
- Scales horizontally (more peers = more capacity)
- Community-driven (network effects)
- Privacy-preserving (can encrypt sensitive context)
- Cost savings reinvested in network growth (bootstrap more peers)

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| **Sybil attacks** (fake peers) | Medium | High | Reputation system, stake requirements |
| **Freeloading** (consume without contributing) | High | Medium | Token economics (must earn to spend) |
| **Bad context quality** | Medium | Medium | Verification protocol, reputation filtering |
| **Network fragmentation** | Low | High | DHT ensures connectivity, tracker fallback |
| **Token economics imbalance** | Medium | Medium | Dynamic parameter adjustment, monitoring |
| **Privacy concerns** | Medium | Medium | Encryption, access controls, private contexts |
| **Scalability bottlenecks** | Low | Medium | Sharding, CDN-like edge nodes |

---

## Success Metrics

### Network Health

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Active peers** | >100 (3 months) | Peers announcing in last 24h |
| **Context chunks** | >10,000 | Total unique contexts in network |
| **Uptime** | >99% | Network available for queries |
| **Avg query latency** | <500ms | Time to discover + retrieve context |

### Economic Health

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Token velocity** | >1 cycle/week | Earn → spend → earn circulation |
| **Token supply/demand** | ±20% balance | Supply (earned) vs demand (spent) |
| **Peer profitability** | >80% profitable | % peers earning more than spending |
| **Cost reduction** | >60% vs baseline | Savings vs full-context LLM approach |

### Quality Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Context quality score** | >0.8 | Average verification score |
| **Bad actor rate** | <5% | % peers with reputation <0.5 |
| **RAG precision** | >90% | % relevant context in retrievals |
| **User satisfaction** | >4/5 | Peer rating of network usefulness |

---

## Integration with Phase 3 (Reward System)

The P2P context-sharing model **enhances** the STF contribution reward system:

### Dual Token Earning Paths

1. **Contribute STFs** (transformation functions)
   - One-time reward: 100-1000 tokens
   - Ongoing: Earn when others use your STF

2. **Share context** (P2P bandwidth)
   - Ongoing: Earn 2 tokens/KB served
   - Passive income: Context served while you sleep

### Synergies

- **STF contributors** can share their implementation examples via P2P
- **Context sharers** can discover new STFs from network
- **Network effects**: More contributors → more context → more value → more contributors

### Combined Timeline

**Phase 1 (Debug Loop)** → **Phase 3a (STF Rewards)** → **Phase 3b (P2P Context Sharing)**

- Weeks 1-4: Debug Loop (STF registry foundation)
- Weeks 5-8: STF Rewards (contribution incentives)
- Weeks 9-14: P2P Context Sharing (bandwidth incentives)

**Total**: 14 weeks for full economic ecosystem

### Phase Mapping Diagram

**Cross-Reference: How P2P Phases Map to Broader Initiative Phases**

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    BROADER INITIATIVE PHASES                             │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Phase 1: Debug Loop (Weeks 1-4)                                        │
│  ├─ Parallel Debugging System                                           │
│  ├─ STF (State Transformation Function) Registry                        │
│  ├─ Agent Integration with Debug Skills                                 │
│  └─ Foundation for token economy                                        │
│                                                                          │
├──────────────────────────────────────┬───────────────────────────────────┤
│                                      │                                   │
│  Phase 2: Infrastructure (Parallel)  │  Phase 3a: STF Rewards (Weeks 5-8)│
│  ├─ DHT Implementation               │  ├─ Contribution Incentives       │
│  ├─ Decentralized Tracker            │  ├─ One-time Bonuses              │
│  └─ (From P2P Phase 2)               │  ├─ Usage-based Earnings          │
│                                      │  └─ STF Marketplace                │
│                                      │                                   │
└──────────────────────────────────────┴───────────────────────────────────┘
                                       │
                                       ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                    P2P CONTEXT SHARING PHASES                            │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  P2P Phase 1: Centralized Tracker (Weeks 9-12 of Initiative)            │
│  ├─ Tracker Server (announce, scrape, peer_list)                        │
│  ├─ Peer Client Library                                                 │
│  ├─ Token Ledger (Kafka topics) ← LEVERAGE EXISTING INFRASTRUCTURE      │
│  ├─ Context Serving Protocol (HTTP API)                                 │
│  └─ Metadata Graph (local SQLite/Memgraph)                              │
│                                                                          │
│  SUCCESS CRITERIA: 10+ peers, 100+ contexts served, 50%+ cost reduction │
│                                                                          │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  P2P Phase 2: DHT Decentralization (Weeks 13-16 of Initiative)          │
│  ├─ Distributed Hash Table (Kademlia/Chord)                             │
│  ├─ Peer Discovery without Central Tracker                              │
│  ├─ Content-addressable Storage (IPFS-like)                             │
│  ├─ Reputation Tracking across DHT                                      │
│  └─ Token Ledger Consensus (PoA/Federated)                              │
│                                                                          │
│  SUCCESS CRITERIA: 100+ peers, no SPOF, <500ms discovery, 99% uptime    │
│                                                                          │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  P2P Phase 3: Advanced Features (Weeks 17-22 of Initiative)             │
│  ├─ Reputation-weighted Token Rewards                                   │
│  ├─ Quality Verification (peer validation)                              │
│  ├─ Context Deduplication                                               │
│  ├─ Caching Layer (frequent contexts)                                   │
│  ├─ Analytics Dashboard (token flow, peer stats)                        │
│  └─ Privacy Controls (encrypted context, ACLs)                          │
│                                                                          │
│  SUCCESS CRITERIA: 1000+ peers, 10K+ contexts/day, 90%+ cost reduction  │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

**Phase Dependencies**:

| Initiative Phase | Depends On | Enables | Duration |
|-----------------|-----------|---------|----------|
| **Phase 1: Debug Loop** | (none) | STF registry, agent integration | 4 weeks |
| **Phase 3a: STF Rewards** | Phase 1 complete | Token earning via contributions | 4 weeks |
| **P2P Phase 1: Centralized Tracker** | Phase 1 (Kafka infra), Phase 3a (token ledger) | P2P context sharing | 4 weeks |
| **P2P Phase 2: DHT** | P2P Phase 1 complete | Decentralized network | 4 weeks |
| **P2P Phase 3: Advanced** | P2P Phase 2 complete | Production-ready P2P economy | 6 weeks |

**Parallel Execution Opportunities**:

1. **Weeks 5-8**: STF Rewards development can start while Debug Loop testing finishes
2. **Weeks 9-12**: P2P Phase 1 can start in parallel with STF Rewards final testing
3. **Weeks 13-16**: P2P Phase 2 (DHT) development can start while P2P Phase 1 is in beta

**Critical Path**: Debug Loop → STF Rewards → P2P Phase 1 → P2P Phase 2 → P2P Phase 3 (22 weeks total)

**Fast Track** (with parallel execution): 18 weeks (20% time savings)

**Integration Points**:

| Integration | Description | Timeline |
|------------|-------------|----------|
| **STF Registry ↔ P2P Context** | STFs advertised via P2P network | Week 9 |
| **Token Ledger ↔ P2P Rewards** | Unified token system for STF + P2P earning | Week 9 |
| **Debug Intelligence ↔ P2P Context** | Debug patterns shared via P2P | Week 12 |
| **Agent Skills ↔ P2P Discovery** | Agents query P2P for context | Week 12 |

---

## Conclusion

The P2P context-sharing model transforms the compute token economy from a **single-sided market** (contributors earn tokens) to a **two-sided marketplace**:

**Supply side**:
- Contribute STFs (one-time + ongoing)
- Share context (ongoing passive income)
- Verify quality (ongoing active income)

**Demand side**:
- Retrieve context from peers
- Use LLM compute
- Generate embeddings

**Network effects**:
- More peers → more context → lower latency → more value
- More contributors → more STFs → more patterns → more value
- More verifiers → higher quality → more trust → more value

**This is how you build a sustainable, decentralized knowledge economy.**

---

**Document Version**: 1.0
**Last Updated**: 2025-11-09
**Status**: Proposal for Review
**Next Steps**:
1. Validate P2P tracker architecture
2. Define token economics parameters
3. Build Phase 1 prototype (centralized tracker)
4. Measure cost reduction vs full-context LLM
