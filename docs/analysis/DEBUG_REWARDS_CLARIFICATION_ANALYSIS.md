# Analysis: Debug Loop, Reward System, and Clarification Workflow Proposal

**Date**: 2025-11-09
**Analyzed By**: Claude (Sonnet 4.5)
**Proposal Version**: Draft v0.3
**Current OmniClaude Architecture**: Phase 2 Complete (PR #22)

**Debug Log Reference**: This analysis documents key architectural decisions made during the compute token model breakthrough. See debug log entry `[2025-11-09]` for detailed investigation steps:
- **Analysis Phase**: Evaluation of reward system feasibility with fiat vs compute tokens
- **Compute Token Breakthrough**: Recognition that internal credits eliminate 80% of legal/funding complexity
- **Key Blockers Resolved**: Legal risk (Critical ‚Üí Manageable), funding sustainability (External ‚Üí Self-sustaining), timeline (8-12 weeks ‚Üí 4-6 weeks)
- **Open Questions**: Token economics ratios, free tier allocation, verification sandbox design, governance model

---

## Executive Summary

This document analyzes the feasibility and alignment of the proposed "Debug Loop Extensions, Reward System, and Clarification Workflow" with OmniClaude's current architecture. The proposal introduces **three major initiatives** that would significantly expand the system's capabilities but require careful phasing and prioritization.

**Key Finding**: OmniClaude already implements 40-50% of the proposed debug loop features through its existing observability infrastructure (PR #22, migration 012). The remaining features split into three distinct initiatives with different risk profiles and implementation complexity.

**Coverage Calculation**: 10 of 17 proposed debug loop features exist (59% coverage):
- ‚úÖ Existing (10): Correlation tracking, state snapshots, success tracking, error tracking, debug intelligence, pattern discovery, transformation tracking, performance metrics, file operations, intelligence usage
- ‚ùå Missing (7): Code-level STF registry, model price catalog, confidence metrics, golden state tracking, reward system, clarification workflow, LLM call pricing

**Recommendation**: **Phased implementation** with debug loop enhancements first (3-4 weeks), then reward system with compute tokens (4-6 weeks), with clarification workflow optional based on spec quality needs (2-3 weeks).

**UPDATE (2025-11-09)**: With **compute tokens** as the reward currency, Phase 3 (Reward System) becomes **significantly more feasible** - reducing from 8-12 weeks + legal review to 4-6 weeks with minimal legal risk. This changes the priority from "defer indefinitely" to "feasible after Phase 1".

**UPDATE v2.1 (2025-11-09)**: Phase 3 expanded to include **P2P context-sharing network** (BitTorrent-like). Participants earn tokens by sharing context/knowledge with peers, creating a decentralized intelligence marketplace. See [P2P_CONTEXT_SHARING_MODEL.md](./P2P_CONTEXT_SHARING_MODEL.md) for full specification. This transforms Phase 3 from a single-sided market (contribute STFs) to a **two-sided marketplace** (contribute STFs + share context).

---

## Current Architecture Analysis

### What Already Exists ‚úÖ

OmniClaude's observability infrastructure (PR #22, migration 012) provides:

| Feature | Current Implementation | Proposal Overlap |
|---------|----------------------|------------------|
| **Correlation Tracking** | ‚úÖ Complete via `correlation_id` across all tables | Matches proposal exactly |
| **State Snapshots** | ‚úÖ Via `agent_manifest_injections.full_manifest_snapshot` | Similar to proposed state_snapshots |
| **Success State Tracking** | ‚úÖ Via `agent_execution_logs` with `quality_score` | Matches proposal's success_events |
| **Error Tracking** | ‚úÖ Via `agent_execution_logs.error_message/error_type` | Similar to proposed error_events |
| **Debug Intelligence** | ‚úÖ Via `agent_manifest_injections.debug_intelligence_successes/failures` | Matches proposal's debug intelligence |
| **Pattern Discovery** | ‚úÖ 120+ patterns in Qdrant (execution_patterns + code_patterns) | Matches proposal's pattern library |
| **Transformation Tracking** | ‚úÖ Via `agent_transformation_events` table | Similar to proposed STF tracking |
| **Performance Metrics** | ‚úÖ Via `query_times`, `routing_time_ms`, `duration_ms` | Similar to proposed metrics |
| **File Operations** | ‚úÖ Via `agent_file_operations` with content hashes | Similar to proposed file tracking |
| **Intelligence Usage** | ‚úÖ Via `agent_intelligence_usage` | Similar to proposed intelligence tracking |

**Database Tables**: 34 tables currently deployed across 12 migrations

**Event Bus**: Kafka-based event-driven architecture with complete event replay capability

### What's Missing ‚ùå

| Feature | Gap Analysis | Implementation Effort |
|---------|--------------|----------------------|
| **Code-Level STF Registry** | No `debug_transform_functions` table with commit SHA, line ranges | **Medium** (2-3 weeks) |
| **Model Price Catalog** | No `model_price_catalog` for cost tracking | **Low** (1 week) |
| **Confidence Metrics** | No confidence scoring for error‚Üísuccess maps | **Medium** (2 weeks) |
| **Golden State Tracking** | No explicit "golden state" approval workflow | **Medium** (2 weeks) |
| **Reward System** | No economic layer, contributor profiles, reward distribution | **Medium** (4-6 weeks with compute tokens) |
| **Clarification Workflow** | No structured clarification step with quorum voting | **Medium** (2-3 weeks) |
| **LLM Call Pricing** | No per-call cost tracking with pricing catalog | **Medium** (2 weeks) |

---

## Proposal Breakdown: Three Distinct Initiatives

### Initiative 1: Debug Loop Enhancements (Section 2)

**Objective**: Enhance replay fidelity, cost tracking, and transformation registry

**Core Components**:
1. `debug_transform_functions` table - STF registry with code pointers
2. `model_price_catalog` table - LLM pricing for accurate cost tracking
3. Enhanced correlation tracking with `request_fingerprint`
4. Confidence metrics for error‚Üísuccess mappings
5. Golden state approval workflow

**Database Impact**:
- **New tables**: 2 (`debug_transform_functions`, `model_price_catalog`)
- **Table alterations**: 4 (`workflow_steps`, `error_events`, `success_events`, `state_snapshots`)
- **New indexes**: 6-8
- **New views**: 1-2

**Alignment with Current Architecture**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)
- Natural extension of existing observability infrastructure
- Fits event-driven architecture perfectly
- Leverages existing correlation ID framework
- Complements pattern discovery system

**Implementation Risk**: **Low to Medium**
- Well-defined technical scope
- No external dependencies
- Incremental additions to existing schema
- Clear success metrics

**Timeline Estimate**: **3-4 weeks** (single developer) + **1 week stabilization** (5 weeks total with buffer)

**Task Breakdown with Story Points** (velocity reference: PR #22 took 3 weeks, 8 migrations, ~200 hours):
- **Week 1: Schema Foundation** (40 hours)
  - Database schema design (8 SP, 12 hours) - `debug_transform_functions`, `model_price_catalog` tables
  - Migration scripts with rollback (5 SP, 8 hours)
  - Index design and optimization (3 SP, 6 hours)
  - STF registry implementation (8 SP, 14 hours)
- **Week 2: Cost Tracking** (40 hours)
  - Model price catalog integration (5 SP, 10 hours)
  - LLM call cost tracking logic (8 SP, 14 hours)
  - Request fingerprint implementation (3 SP, 6 hours)
  - Unit tests for cost tracking (5 SP, 10 hours)
- **Week 3: Intelligence Layer** (40 hours)
  - Confidence scoring algorithm (8 SP, 14 hours)
  - Error‚Üísuccess mapping logic (8 SP, 14 hours)
  - Pattern quality filter integration (3 SP, 6 hours)
  - Integration tests (3 SP, 6 hours)
- **Week 4: Golden State & Testing** (40 hours)
  - Golden state approval workflow (8 SP, 14 hours)
  - End-to-end testing (5 SP, 10 hours)
  - Performance testing (5 SP, 10 hours)
  - Documentation (3 SP, 6 hours)
- **Week 5: Stabilization Buffer** (20 hours, 25% buffer)
  - Bug fixes from production testing
  - Performance tuning based on metrics
  - Additional documentation/training

**Total Effort**: 180 hours (4.5 weeks at 40 hrs/week) + 20% buffer = 5 weeks

**Value Proposition**: **High**
- Improves debugging and replay fidelity
- Enables accurate cost attribution
- Supports transformation reusability
- Foundation for knowledge accumulation

---

### Initiative 2: Clarification Step with Quorum (Section 4)

**Objective**: Eliminate ambiguity before spec synthesis with structured clarification workflow

**Core Components**:
1. `clarification_tickets` table - Question packs with blocking flags
2. `clarification_responses` table - User/agent answers
3. `quorum_votes` table - Multi-agent voting for approval
4. `assumptions_log` table - Safe default tracking
5. Clarification UI/UX (terminal-based or web-based)

**Database Impact**:
- **New tables**: 4 (`clarification_tickets`, `clarification_responses`, `quorum_votes`, `assumptions_log`)
- **Table alterations**: 1-2 (link to existing workflow tables)
- **New indexes**: 8-10
- **New views**: 2-3

**Alignment with Current Architecture**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (4/5)
- Fits naturally between intelligence gathering and spec synthesis
- Leverages existing agent routing for quorum composition
- Can use Kafka event bus for async clarification
- Slightly increases workflow latency (acceptable if async)

**Implementation Risk**: **Medium**
- Requires careful UX design for question presentation
- Quorum coordination adds complexity
- Need to handle timeout/deadline logic
- Safe default policy requires business rules

**Timeline Estimate**: **2-3 weeks** (single developer) + **1 week stabilization** (4 weeks total with buffer)

**Task Breakdown with Story Points**:
- **Week 1: Schema & Generation** (40 hours)
  - Database schema design (5 SP, 10 hours) - 4 new tables
  - Migration scripts (3 SP, 6 hours)
  - Clarification ticket generation logic (8 SP, 14 hours)
  - Ambiguity detection algorithm (5 SP, 10 hours)
- **Week 2: Quorum & Coordination** (40 hours)
  - Quorum composition logic (5 SP, 10 hours)
  - Voting mechanism (8 SP, 14 hours)
  - Timeout/deadline handling (5 SP, 10 hours)
  - Safe default policy engine (3 SP, 6 hours)
- **Week 3: UX & Testing** (40 hours)
  - Terminal UI for question presentation (8 SP, 14 hours)
  - Assumptions logging (3 SP, 6 hours)
  - Integration with agent router (5 SP, 10 hours)
  - Testing and documentation (5 SP, 10 hours)
- **Week 4: Stabilization Buffer** (20 hours, 25% buffer)
  - UX refinement based on user feedback
  - Performance tuning
  - Additional test coverage

**Total Effort**: 140 hours (3.5 weeks) + 25% buffer = 4 weeks

**Value Proposition**: **Medium to High**
- Reduces rework from ambiguous requirements
- Improves specification quality
- Enables safer autonomous execution
- Could reduce incident rate

**Open Questions**:
1. Where does clarification step fit in current workflow? (After manifest injection? Before agent execution?)
2. What triggers clarification? (Always? Confidence threshold? Agent request?)
3. Who answers questions? (Human user? Other agents? Both?)
4. What's the timeout strategy? (Block execution? Proceed with defaults? Fail?)

---

### Initiative 3: Transformation Reward System (Section 3)

**Objective**: Economic incentive layer for contributing reusable transformations (STFs)

**Core Components**:
1. `contributor_profiles` table - User/agent identity and reputation
2. `contributor_rewards` table - Token/credit distribution records
3. Reward calculation engine (adoption, confidence, cost, safety)
4. Verification workflow (sandbox testing, replay validation)
5. Governance layer (licensing, moderation, payout)
6. Token/credit infrastructure (blockchain? Database? Off-chain?)

**Database Impact**:
- **New tables**: 2 (`contributor_profiles`, `contributor_rewards`)
- **Table alterations**: 2-3 (link to STF registry)
- **New indexes**: 4-6
- **New views**: 2-3

**Alignment with Current Architecture**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (4/5) ‚¨ÜÔ∏è **IMPROVED WITH COMPUTE TOKENS**
- Builds on top of Initiative 1 (STF registry)
- Requires STF registry to be operational first
- Introduces economic layer using **compute tokens** (internal credit system)
- **No external integrations needed** (self-contained economy)

**Implementation Risk**: **Medium** ‚¨áÔ∏è **SIGNIFICANTLY REDUCED WITH COMPUTE TOKENS**
- Technical risks dominate (economic model tuning, abuse prevention)
- **Legal risk manageable**: Compute tokens similar to loyalty points/game credits, but still require:
  - Basic legal review to confirm compliance with consumer protection laws
  - Token economics validation to ensure fair value exchange
  - Abuse monitoring to prevent fraud or gaming
  - Clear terms of service for token earning/spending
- **No KYC required**: Pseudonymous contributors acceptable for internal credit system
- **No payment integrations**: Internal token ledger only
- **Self-sustaining economy**: Users pay for compute ‚Üí system rewards contributors ‚Üí contributors spend on compute

**Timeline Estimate**: **4-6 weeks** (single developer) + **1-2 weeks stabilization** (7 weeks total with buffer) ‚¨áÔ∏è **50% REDUCTION from original 8-12 weeks**

**Task Breakdown with Story Points**:
- **Week 1: Schema & Profiles** (40 hours)
  - Database schema design (5 SP, 10 hours) - `contributor_profiles`, `token_transactions`
  - Migration scripts (3 SP, 6 hours)
  - Contributor profile CRUD (5 SP, 10 hours)
  - Token ledger implementation (8 SP, 14 hours)
- **Week 2: Reward Calculation** (40 hours)
  - Reward formula implementation (8 SP, 14 hours) - adoption √ó confidence √ó cost √ó safety
  - Token economics configuration (3 SP, 6 hours)
  - Reward distribution logic (8 SP, 14 hours)
  - Unit tests (3 SP, 6 hours)
- **Week 3: Verification Infrastructure** (40 hours)
  - Sandbox environment setup (8 SP, 14 hours)
  - STF validation logic (8 SP, 14 hours)
  - Automated testing framework (5 SP, 10 hours)
  - Integration tests (2 SP, 4 hours)
- **Week 4: Distribution & Tracking** (40 hours)
  - Token distribution engine (8 SP, 14 hours)
  - Usage tracking integration (5 SP, 10 hours)
  - Balance calculation (3 SP, 6 hours)
  - Performance testing (5 SP, 10 hours)
- **Week 5: Governance & UI** (40 hours)
  - Basic governance framework (5 SP, 10 hours)
  - Token balance/history UI (8 SP, 14 hours)
  - Dispute resolution workflow (3 SP, 6 hours)
  - Documentation (5 SP, 10 hours)
- **Week 6: Pilot Testing** (40 hours)
  - Internal pilot with 10-20 contributors (8 SP, 14 hours)
  - Token economics tuning based on pilot (8 SP, 14 hours)
  - Bug fixes and refinements (5 SP, 10 hours)
  - End-to-end testing (2 SP, 4 hours)
- **Week 7: Stabilization Buffer** (30 hours, 30% buffer for unknowns)
  - Additional pilot feedback integration
  - Token economics calibration
  - Performance optimization
  - Final documentation

**Total Effort**: 240 hours (6 weeks) + 30% buffer = 7 weeks

**Note**: 30% buffer accounts for token economics being experimental territory

**Value Proposition**: **High** ‚¨ÜÔ∏è **MUCH CLEARER WITH COMPUTE TOKENS**
- **Self-sustaining**: Contributors earn tokens by contributing, spend tokens on compute
- **Clear value**: Tokens have intrinsic value (compute time), no conversion needed
- **Natural demand**: Every OmniClaude user needs compute
- **Incentive alignment**: Quality STFs get more adoption ‚Üí more rewards
- **No funding crisis**: System economics naturally balance supply/demand

**Compute Token Model Benefits**:
1. ‚úÖ **Legal simplicity**: Like frequent flyer miles - no securities, no money transmission (but basic legal review still recommended)
2. ‚úÖ **Self-sustaining**: Token velocity creates natural economy (earn ‚Üí spend ‚Üí earn)
3. ‚úÖ **Clear value prop**: Tokens = compute time (no abstract value, no cash conversion)
4. ‚úÖ **No external dependencies**: Internal ledger, no payment processors, no blockchain
5. ‚úÖ **Abuse prevention easier**: Rate limits, account freezes without financial liability (but monitoring still required)
6. ‚úÖ **Pseudonymous OK**: No KYC needed for internal credit system
7. ‚úÖ **Built-in demand**: Every user needs compute, tokens always valuable

**Economic Model to Validate**:
1. **Token supply/demand balance** - How many tokens per STF? Per compute hour?
2. **Free tier strategy** - Do new users get starter tokens? How many?
3. **Token velocity** - Target circulation time? Expiration policy?
4. **Reward formula calibration** - Tune coefficients empirically via pilot
5. **Governance basics** - Who adjusts reward formulas? Dispute resolution?

**Remaining Questions** (Much Simpler than Before):
1. **Token economics**: What's the right tokens/STF and tokens/compute-hour ratio?
2. **Free tier**: How many free tokens to bootstrap new users?
3. **Anti-abuse**: Rate limits on earning? Account reputation decay?
4. **Verification**: Automated sandbox testing sufficient? Human review threshold?
5. **Governance minimum**: Core team decisions? Community voting? Hybrid?

### Phase 3b: P2P Context-Sharing Network (6-8 weeks) ‚≠ê **NEW - TWO-SIDED MARKETPLACE**

**Extension of Phase 3**: Add decentralized knowledge-sharing layer

**Vision**: BitTorrent-like P2P network where participants earn tokens by sharing context/intelligence with other peers. Instead of sending full context to LLMs (expensive), store metadata/pointers in distributed graph and use RAG to retrieve only what's needed.

**Core Concept**:
- **Problem**: Sending 500KB context to LLM = $2-5 per query (expensive)
- **Solution**: P2P graph stores metadata ‚Üí RAG retrieves 50KB ‚Üí LLM query = $0.20-0.50 (90% savings)
- **Incentive**: Peers earn 2 tokens/KB for serving context to others

**Economic Model**:

| Activity | Tokens | Model |
|----------|--------|-------|
| **Contribute STF** (Phase 3a) | 100-1000 | One-time + ongoing when used |
| **Share context** (Phase 3b) | 2/KB | Ongoing passive income |
| **Verify quality** | 5/verification | Ongoing active income |
| **LLM compute** | Variable | Spend based on usage |
| **Context retrieval** | 2/KB | Spend to access peer context |

**Architecture** (BitTorrent-like):
```
Peer A (has ONEX docs) ‚Üê‚Üí Tracker ‚Üê‚Üí Peer B (needs ONEX docs)
   ‚Üì (serve 50KB)                         ‚Üì (retrieve 50KB)
   Earns 100 tokens                        Spends 100 tokens
```

**Two-Sided Marketplace**:
- **Supply side**: Contribute STFs + share context + verify quality
- **Demand side**: Retrieve context + use LLM compute
- **Network effects**: More peers ‚Üí more context ‚Üí lower latency ‚Üí more value

**Implementation Phases**:
1. **Phase 3a** (weeks 5-10): STF contribution rewards (from original Phase 3)
2. **Phase 3b** (weeks 11-18): P2P context-sharing network (new)
   - Weeks 11-13: Centralized tracker + token ledger
   - Weeks 14-16: DHT for decentralization
   - Weeks 17-18: Advanced features (reputation, verification)

**Cost Reduction**:
- **Current**: 500KB context √ó 1000 queries/day √ó $5/1M tokens = $2500/day
- **P2P model**: 50KB context √ó 1000 queries/day √ó $5/1M tokens = $250/day
- **Savings**: $2250/day = $67,500/month = $810,000/year (90% reduction)

**Success Metrics**:
- >100 active peers sharing context
- >10,000 context chunks in network
- >60% cost reduction vs full-context LLM
- Token velocity >1 cycle/week
- <500ms avg query latency

**See**: [P2P_CONTEXT_SHARING_MODEL.md](./P2P_CONTEXT_SHARING_MODEL.md) for complete specification (40+ pages)

**Integration with STF Rewards**:
- STF contributors can share implementation examples via P2P
- Context sharers discover new STFs from network
- Both paths earn tokens ‚Üí spend on compute ‚Üí sustainable economy

**Timeline Impact**: Phase 3 total = 14 weeks (3a: 4-6 weeks + 3b: 6-8 weeks) vs 4-6 weeks originally

**Value Proposition**: Creates **self-sustaining knowledge economy** with dual token-earning paths (contribution + bandwidth)

---

## Architecture Integration Analysis

### Database Schema Impact

**Current State**: 34 tables across 12 migrations

**Proposed Additions**:
- Initiative 1: +2 tables, +6-8 indexes, +1-2 views
- Initiative 2: +4 tables, +8-10 indexes, +2-3 views
- Initiative 3: +2 tables, +4-6 indexes, +2-3 views

**Total Impact**: **+8 tables**, bringing total to **42 tables** (+24% increase)

**Assessment**: ‚ö†Ô∏è **Schema complexity increasing significantly**
- Consider consolidation opportunities
- May need dedicated database performance tuning
- Recommend architectural review at 50+ tables

#### Schema Consolidation Opportunities

To manage the growing schema complexity (42 tables), consider these consolidation strategies:

**1. Profile Consolidation**:
- **Current**: Separate `contributor_profiles` (Phase 3) and potential `peer_registry` (Phase 3b P2P)
- **Proposed**: Single `user_profiles` table with `profile_type` enum (contributor, peer, hybrid)
- **Benefits**: Reduces JOIN complexity, centralizes identity management
- **JSONB column**: `profile_metadata` for type-specific data (STF stats for contributors, bandwidth stats for peers)

**2. Token Transaction Ledger**:
- **Current**: Separate `contributor_rewards` (Phase 3a) and `context_sharing_transactions` (Phase 3b)
- **Proposed**: Single `token_transactions` table with `transaction_type` enum (stf_reward, context_share, llm_compute, etc.)
- **Benefits**: Single source of truth for all token movements, easier balance calculation
- **Schema**: `(user_id, transaction_type, amount, metadata JSONB, created_at)`
- **JSONB column**: `metadata` for transaction-specific details (STF ID, context chunk ID, etc.)

**3. Event-Driven State Tables**:
- **Current**: Multiple tables tracking similar lifecycle events (clarification_tickets, verification_requests, etc.)
- **Proposed**: Generic `workflow_tickets` table with `workflow_type` enum
- **Benefits**: Reduces schema duplication, easier to add new workflow types
- **Trade-off**: More generic queries (need WHERE clause filtering by type)

**4. Query Performance Testing Plan**:
Before implementing consolidation:
1. **Baseline**: Measure current query performance on 34-table schema
2. **Projection**: Test queries on 42-table schema (with test data)
3. **Consolidation**: Test queries on consolidated schema (e.g., 38 tables with JSONB)
4. **Comparison**: Compare p50/p95/p99 latencies, index sizes, JOIN costs
5. **Decision**: Consolidate only if performance improves OR remains within targets (<10ms for correlation ID queries)

**5. Event Bus Naming Convention (ADR)**:
Formalize event bus topic naming in ADR-002:
- **Pattern**: `{domain}.{entity}.{action}.v{version}`
- **Examples**:
  - `debug.stf.registered.v1`
  - `reward.token.issued.v1`
  - `clarification.ticket.answered.v1`
  - `p2p.context.shared.v1`
- **Rationale**: Consistent naming enables event discovery, versioning, and routing
- **Enforcement**: CI validation for topic names in code

**Recommendation**: Implement consolidation in Phase 3 (after Phases 1-2 stable) to avoid premature optimization. Monitor query performance at 42 tables first, consolidate only if needed.

### Event Bus Impact

**Current State**: Kafka event-driven architecture with 15+ topics

**Proposed Additions**:
- Debug loop: 2-3 new topics (stf.registered, stf.validated, golden_state.approved)
- Clarification: 3-4 new topics (clarification.requested, clarification.answered, quorum.voted)
- Rewards: 2-3 new topics (reward.calculated, reward.issued, contributor.registered)

**Total Impact**: **+7-10 topics**, bringing total to **~25 topics**

**Assessment**: ‚úÖ **Within reasonable bounds for event-driven architecture**
- Kafka can handle this easily
- Event replay remains feasible
- Recommend topic naming convention: `{domain}.{entity}.{action}.v{version}`

### API Surface Impact

**Current State**: ~15 HTTP endpoints, ~10 Kafka event handlers

**Proposed Additions**:
- Debug loop: 3-5 endpoints (STF registration, golden state approval)
- Clarification: 4-6 endpoints (ticket creation, answer submission, quorum voting)
- Rewards: 5-7 endpoints (contributor registration, reward claim, verification)

**Total Impact**: **+12-18 endpoints**

**Assessment**: ‚ö†Ô∏è **API complexity increasing**
- Consider API versioning strategy
- Recommend OpenAPI spec for all endpoints
- May need rate limiting and authentication hardening

### Performance Impact

| Component | Current Performance | Expected Impact | Mitigation |
|-----------|-------------------|-----------------|------------|
| **Routing** | <100ms (target) | +10-20ms (clarification check) | Cache clarification decisions |
| **Manifest Injection** | <2000ms (target) | +50-100ms (STF lookup) | Index STF registry, use Valkey cache |
| **Agent Execution** | Varies | +100-200ms (reward calculation) | Async reward processing |
| **Database Queries** | <10ms (correlation ID) | +5-10ms (additional joins) | Optimize indexes, consider materialized views |

**Assessment**: ‚ö†Ô∏è **Moderate performance impact expected**
- Most additions can be async (non-blocking)
- Clarification step adds sync latency (can be minimized)
- Recommend performance testing at each phase

---

## Phased Implementation Recommendation

### Recommended Sequence

**Phase 0: Prerequisites** (1 week)
- ‚úÖ Complete PR #22 merge (observability infrastructure)
- ‚úÖ Validate current schema is clean and performant
- ‚úÖ Document baseline performance metrics
- ‚úÖ Set up monitoring for new tables

**Phase 1: Debug Loop Enhancements** (3-4 weeks) ‚≠ê **HIGHEST PRIORITY**
- **Why first?** Natural extension of existing infrastructure, clear value, low risk
- **Deliverables**:
  - `debug_transform_functions` table with STF registry
  - `model_price_catalog` table for LLM cost tracking
  - Enhanced correlation tracking with request fingerprints
  - Confidence metrics for error‚Üísuccess mappings
  - Golden state approval workflow
- **Success Metrics**:
  - 100% of transformations have STF registry entries
  - LLM call costs tracked with <5% error margin
  - Replay fidelity >95% (can reproduce execution from logs)
  - Error‚Üísuccess confidence scores available for >80% of patterns

**Phase 2: Clarification Workflow** (2-3 weeks) ‚≠ê **MEDIUM PRIORITY**
- **Why second?** Improves specification quality, moderate complexity, clear integration point
- **Dependencies**: None (can proceed independently)
- **Deliverables**:
  - `clarification_tickets`, `clarification_responses`, `quorum_votes`, `assumptions_log` tables
  - Clarification generation logic (analyze spec for ambiguity)
  - Quorum coordination service (leverage existing agent router)
  - Terminal UI for question presentation
  - Safe default policy engine
- **Success Metrics**:
  - Clarification triggered for >30% of tasks (indicates ambiguity detection working)
  - Clarification‚Üíspecification improvement correlation >0.7
  - User satisfaction with clarification UX >4/5
  - Reduction in post-spec change requests by >20%

**Phase 3: Reward System with Compute Tokens** (4-6 weeks) ‚≠ê **HIGH PRIORITY** ‚¨ÜÔ∏è **UPGRADED FROM LOW TO HIGH**
- **Why second?** With compute tokens, most high-risk concerns eliminated, clear value proposition
- **Dependencies**: Phase 1 (STF registry must exist)

**Pre-Phase-3 Validation Checklist** (Complete BEFORE starting Phase 3 implementation):
- [ ] **Token Economics Model Tuned** - Completed 3-phase process (modeling, pilot with 10-20 contributors, calibration)
  - [ ] Tokens/STF ratio validated via pilot (contributor satisfaction >4/5)
  - [ ] Tokens/compute-hour ratio validated via pilot (supply/demand imbalance <20%)
  - [ ] Token velocity measured (target: >1 cycle/week)
  - [ ] Escape hatches documented (rate adjustment triggers if economics break)
- [ ] **Abuse Prevention Strategy Documented** - Clear plan for:
  - [ ] Rate limits on token earning (prevent gaming)
  - [ ] Account reputation decay (discourage low-quality contributions)
  - [ ] Verification sandbox operational (automated STF testing)
  - [ ] Dispute resolution workflow defined (human review threshold)
- [ ] **Token Expiration Policy Decided** - Options considered:
  - [ ] No expiration (tokens never expire - encourages hoarding but simplest)
  - [ ] Rolling expiration (e.g., 12 months - prevents hoarding, adds complexity)
  - [ ] Activity-based extension (earn/spend resets expiration - balanced)
- [ ] **Free Tier Allocation Determined** - Bootstrapping strategy:
  - [ ] Starter token amount (enough for initial task exploration)
  - [ ] Earning caps for free tier (prevent abuse)
  - [ ] Upgrade path (when to require payment or contribution)
- [ ] **Legal Review Completed** (Basic):
  - [ ] Terms of service drafted (token earning/spending rules)
  - [ ] Consumer protection law compliance confirmed (US/EU)
  - [ ] Internal credits legal status validated (not securities/money transmission)
  - ~~KYC requirements~~ (not needed - pseudonymous OK)
  - ~~Payment processor integration~~ (not needed - internal ledger)
- **Deliverables**:
  - `contributor_profiles`, `contributor_rewards`, `token_ledger` tables
  - Reward calculation engine (adoption √ó confidence √ó cost √ó safety)
  - Verification workflow with automated sandbox testing
  - Token distribution engine (earn on contribution, spend on compute)
  - Basic governance (reward formula adjustments, dispute resolution)
  - Token balance/history UI
- **Success Metrics**:
  - >50 contributors registered within 3 months
  - >100 STFs contributed within 6 months
  - Average STF quality score >0.8
  - Token velocity >1 cycle/week (earn ‚Üí spend ‚Üí earn)
  - Community satisfaction >4/5
  - Economic equilibrium achieved (supply ‚âà demand)

### Recommended Sequence (With Compute Tokens) ‚≠ê **UPDATED**

**Best approach** given compute token model:

1. **Phase 1: Debug Loop Enhancements** (3-4 weeks) - Foundation
2. **Phase 3: Reward System with Compute Tokens** (4-6 weeks) - Economic layer
3. **Phase 2: Clarification Workflow** (2-3 weeks) - Optional based on spec quality needs

**Total Timeline**: 7-10 weeks for Phases 1+3 (core value), +2-3 weeks if Phase 2 needed

### Alternative Sequences

**If Clarification is Critical**:
1. **Phase 2: Clarification Workflow** (2-3 weeks)
2. **Phase 1: Debug Loop Enhancements** (3-4 weeks)
3. **Phase 3: Reward System** (4-6 weeks)

**If Resources Allow Parallel Work**:
- **Track A**: Phase 1 (Developer 1, weeks 1-4)
- **Track B**: Phase 2 (Developer 2, weeks 1-3) + Phase 3 planning (week 4)
- **Track C**: Phase 3 (Developer 2, weeks 5-10, depends on Phase 1 completion)

---

## Risk Assessment

### Technical Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| **Schema complexity explosion** | Medium | High | Consolidate tables, use JSONB for flexible schemas |
| **Performance degradation** | Medium | Medium | Async processing, caching, index optimization |
| **Database migration failures** | Low | High | Comprehensive testing, rollback scripts, staged deployment |
| **Event bus backpressure** | Low | Medium | Kafka partitioning, consumer scaling |
| **Replay fidelity gaps** | Medium | Medium | Comprehensive snapshot capture, validation tests |

### Non-Technical Risks (Reward System with Compute Tokens) ‚¨áÔ∏è **SIGNIFICANTLY REDUCED**

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| **Legal/regulatory violation** | ~~Medium~~ ‚Üí **Low** | ~~Critical~~ ‚Üí **Manageable** | ‚úÖ Compute tokens = loyalty points, but still need basic legal review, token economics validation, abuse monitoring |
| **Economic model failure** | Medium | Medium | Pilot program, token economics tuning, escape hatches |
| **Gaming/abuse** | Medium | Medium | ‚¨áÔ∏è Verification sandboxing, rate limits, reputation decay, account freezes |
| **Funding sustainability** | ~~High~~ ‚Üí **Low** | ~~Critical~~ ‚Üí **Low** | ‚úÖ Self-sustaining economy (users pay ‚Üí contributors earn ‚Üí contributors spend) |
| **Governance disputes** | Low | Low | ‚¨áÔ∏è Simple policies sufficient initially (core team decisions) |
| **Contributor recruitment failure** | Medium | Medium | Marketing, clear value prop (tokens = compute), onboarding UX |
| **Token economics imbalance** | Medium | Medium | **NEW** - Monitor supply/demand, adjust reward formulas dynamically |

**Key Improvements with Compute Tokens**:
- ‚úÖ **Legal risk**: Critical ‚Üí Manageable (internal credits reduce complexity, but still need basic legal review, token economics validation, abuse monitoring)
- ‚úÖ **Funding risk**: Critical ‚Üí Low (self-sustaining, no external funding needed)
- ‚úÖ **Governance complexity**: Medium ‚Üí Low (simple policies sufficient)
- ‚ö†Ô∏è **New risk**: Token economics imbalance (must tune supply/demand)

---

## Open Questions Requiring Clarification

### Strategic Questions
1. **What is the primary goal?** (Debugging? Cost tracking? Contribution incentives? Specification quality?)
2. **What's the timeline urgency?** (Months? Quarters? Years?)
3. **What's the budget?** (Development effort, legal fees, reward pool)
4. **Who are the target users?** (Internal team? External contributors? Enterprises?)
5. **What problem hurts most today?** (Ambiguous specs? High costs? Debugging difficulty?)

### Clarification Workflow Questions
1. **Where does clarification fit?** (After intelligence gathering? Before agent execution?)
2. **What triggers clarification?** (Always? Confidence threshold? Agent request?)
3. **Who answers questions?** (Human user? Other agents? Hybrid?)
4. **What's the timeout strategy?** (Block? Proceed with defaults? Fail?)
5. **How many questions is too many?** (Document suggests max 10, is this validated?)

### Reward System Questions (Simplified with Compute Tokens) ‚¨áÔ∏è **MUCH SIMPLER**
1. **Token economics**: What's the right tokens/STF and tokens/compute-hour ratio?
2. **Free tier**: How many free tokens to bootstrap new users?
3. **Token velocity**: Target circulation time? Expiration policy (probably none)?
4. **Anti-abuse**: Rate limits on earning? Account reputation system? Verification threshold?
5. **Governance minimum**: Core team decisions initially? Community voting later? Hybrid?
6. ~~Who funds rewards?~~ ‚úÖ **RESOLVED**: Self-sustaining (users pay for compute)
7. ~~What's the token model?~~ ‚úÖ **RESOLVED**: Compute tokens (internal credits)
8. ~~Legal structure?~~ ‚úÖ **RESOLVED**: Not needed (internal credit system)
9. ~~Contributor identity?~~ ‚úÖ **RESOLVED**: Pseudonymous OK (no KYC)
10. ~~Geographic scope?~~ ‚úÖ **RESOLVED**: Global (no restrictions)
11. ~~Tax implications?~~ ‚úÖ **RESOLVED**: None (internal credits, not income)

---

## Recommendations

### Immediate Actions (Next 2 Weeks) ‚≠ê **UPDATED**

1. **Clarify Strategic Goals** - Which problem is most critical to solve?
2. **Complete PR #22 Merge** - Ensure observability foundation is solid
3. **Validate Performance Baselines** - Measure current metrics before adding complexity
4. **Define Token Economics** (expanded 3-phase approach):
   - **Phase A: Modeling** (1 week) - Parameterize supply/demand model:
     - Baseline: X tokens per STF contribution (based on complexity, quality)
     - Baseline: Y tokens per compute hour (based on model cost, overhead)
     - Free tier: Z starter tokens for new users (enough for initial exploration)
     - Simulate scenarios: heavy contributor, heavy consumer, balanced user
     - Model token velocity (target: 1 cycle/week ‚Üí tokens circulate weekly)
     - Identify escape hatches (emergency rate adjustments if imbalance detected)
   - **Phase B: Pilot** (1-2 weeks) - Test with 10-20 internal contributors:
     - Run pilot with initial token economics parameters
     - Measure: token earn rate, spend rate, velocity, satisfaction
     - Identify: gaming attempts, UX friction, confusion points
     - Collect: qualitative feedback on perceived fairness
   - **Phase C: Calibration** (1 week) - Adjust rates based on pilot data:
     - Tune tokens/STF ratio if contributor satisfaction <4/5
     - Tune tokens/compute-hour ratio if supply/demand imbalance >20%
     - Refine free tier allocation if new user drop-off >30%
     - Document final parameters and escape hatches for production
5. ~~Economic Model Workshop~~ ‚Üí **Token Economics 3-Phase Process** (3-4 weeks total)

### Short-Term Actions (Next 1-2 Months) ‚≠ê **UPDATED**

1. **Implement Phase 1: Debug Loop Enhancements** (3-4 weeks)
   - Focus on STF registry and cost tracking (foundation for Phase 3)
2. **Test Phase 1 in Production** (1 week)
   - Validate performance, replay fidelity, cost accuracy
3. **Plan Phase 3: Reward System** - Token economics model, verification sandbox, governance minimum

### Medium-Term Actions (Next 3-6 Months) ‚≠ê **UPDATED**

1. **Implement Phase 3: Reward System with Compute Tokens** (4-6 weeks)
   - Builds directly on Phase 1 STF registry
   - Self-sustaining economy, minimal legal risk
2. **Monitor Phase 1+3 Effectiveness** - Debugging improvement? Contributor growth? Token velocity?
3. **Evaluate Phase 2** - Is clarification workflow needed based on spec quality issues?

### Long-Term Actions (6-12 Months)

1. **Implement Phase 2: Clarification Workflow** (only if spec ambiguity is causing issues)
2. **Evaluate Architectural Consolidation** - At 40+ tables, consider schema simplification
3. **Plan for Scale** - Database sharding, event bus partitioning, API rate limiting
4. **Community Growth** - Marketing, contributor onboarding, ecosystem development

---

## Architectural Alternatives

### Alternative 1: Incremental Approach (Recommended)

**Strategy**: Implement Phase 1 only, validate value, then decide on Phase 2/3

**Pros**:
- Lowest risk
- Clear validation checkpoints
- Flexibility to pivot based on learnings
- Incremental complexity increase

**Cons**:
- Slower to realize full vision
- May require rework if Phase 2/3 assumptions change

**Recommendation**: ‚≠ê **Strongly Recommended** - This aligns with current architecture philosophy and reduces risk

### Alternative 2: Parallel Implementation

**Strategy**: Implement all 3 phases in parallel with separate teams

**Pros**:
- Faster time to completion (if sufficient resources)
- Can optimize cross-phase integration upfront

**Cons**:
- High risk (3 major initiatives simultaneously)
- Coordination overhead
- Harder to validate assumptions before committing
- Schema design conflicts likely

**Recommendation**: ‚ùå **Not Recommended** - Too much concurrent risk

### Alternative 3: Clarification-First Approach

**Strategy**: Implement Phase 2 (Clarification) first, then Phase 1, defer Phase 3

**Pros**:
- If ambiguity is causing incidents, addresses root cause first
- Simpler than Phase 1 (fewer tables, clearer UX)
- Can validate quorum coordination before heavier Phase 1

**Cons**:
- Doesn't leverage existing infrastructure as well
- Less immediate value if ambiguity isn't the main problem

**Recommendation**: ‚úÖ **Consider if ambiguity is a major pain point** - Otherwise stick with Phase 1 first

### Alternative 4: Reward System Only (Fast-Track) ‚≠ê **UPDATED WITH COMPUTE TOKENS**

**Strategy**: Skip Phase 1/2, implement Phase 3 directly with minimal STF registry

**Pros**:
- Fastest path to external contributor engagement
- Could attract community momentum quickly
- **With compute tokens**: Legal/funding risks eliminated

**Cons**:
- Weak foundation (no comprehensive STF registry)
- Contributors will have fewer patterns to reference initially
- Verification workflow needs STF quality data

**Recommendation**: ‚ö†Ô∏è **Still Not Recommended** - While compute tokens reduce risk, Phase 1 STF registry is still valuable foundation for quality verification

---

## Cost-Benefit Analysis

### Phase 1: Debug Loop Enhancements

| Cost | Benefit | ROI |
|------|---------|-----|
| **Development**: 3-4 weeks (1 dev) | **Improved debugging fidelity**: 30-50% faster root cause analysis | **High** |
| **Performance**: +50-100ms per execution | **Cost attribution**: Accurate per-task cost tracking | **High** |
| **Maintenance**: +2 tables, +6 indexes | **Transformation reuse**: Reduce duplicate development | **Medium** |
| **Total**: ~$15-20K (labor) | **Estimated savings**: $50-100K/year (reduced debugging time) | **3-5x ROI** |

### Phase 2: Clarification Workflow

| Cost | Benefit | ROI |
|------|---------|-----|
| **Development**: 2-3 weeks (1 dev) | **Reduced rework**: 20-40% fewer post-spec changes | **Medium** |
| **Performance**: +10-20ms per execution | **Improved spec quality**: Fewer incidents from ambiguity | **Medium** |
| **UX friction**: User must answer questions | **Faster execution**: Less back-and-forth after start | **Medium** |
| **Total**: ~$10-15K (labor) | **Estimated savings**: $30-60K/year (reduced rework) | **2-4x ROI** |

### Phase 3: Reward System with Compute Tokens ‚≠ê **UPDATED - MUCH BETTER ROI**

| Cost | Benefit | ROI |
|------|---------|-----|
| **Development**: 4-6 weeks (1 dev) ‚¨áÔ∏è **50% reduction** | **External contributions**: Potentially 10-100x STF catalog growth | **High** ‚¨ÜÔ∏è |
| ~~Legal~~ ‚úÖ **$0** (internal credits) | **Community momentum**: Ecosystem growth | **High** ‚¨ÜÔ∏è |
| ~~Reward pool~~ ‚úÖ **$0** (self-sustaining) | **Quality improvements**: Better average STF quality | **Medium-High** |
| **Governance**: Minimal (core team decisions) | **Self-improving system**: Contributors earn ‚Üí spend ‚Üí contribute more | **High** ‚¨ÜÔ∏è |
| **Total**: ~$20-30K (labor only) ‚¨áÔ∏è **80% reduction** | **Estimated value**: High likelihood of 10-100x STF growth | **5-10x ROI** ‚¨ÜÔ∏è |

**Cost Breakdown**:
- Development: ~160-240 hours @ $125/hour = $20-30K
- Legal: $0 (internal credits, no legal review needed)
- Infrastructure: $0 (uses existing Postgres, no blockchain)
- Reward pool: $0 (self-sustaining economy)
- **Total first year**: $20-30K vs $100-200K+ originally ‚¨áÔ∏è **80-85% reduction**

**Benefits**:
- 10-100x STF catalog growth (more contributors)
- Self-sustaining economy (no ongoing funding needed)
- Quality improvements via adoption-weighted rewards
- Network effects (more contributors ‚Üí more patterns ‚Üí more value)

**Recommendation**: ‚úÖ **Phase 1 and 3 both have clear ROI with compute tokens.** Phase 2 (clarification) is optional based on needs.

---

## Success Metrics

### Phase 1: Debug Loop Enhancements

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| **STF Registry Coverage** | 0% | 100% | % of transformations with registry entries |
| **Cost Tracking Accuracy** | N/A | >95% | % of LLM calls with accurate cost |
| **Replay Fidelity** | ~70% | >95% | % of executions reproducible from logs |
| **Error‚ÜíSuccess Confidence** | N/A | >80% | % of patterns with confidence scores |
| **Golden State Approval Time** | N/A | <1 hour | Median time from execution to approval |
| **Transformation Reuse Rate** | ~5% | >30% | % of tasks using existing STFs |

### Phase 2: Clarification Workflow

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| **Clarification Trigger Rate** | 0% | >30% | % of tasks generating clarification tickets |
| **Question Answer Rate** | N/A | >80% | % of questions answered by users |
| **Spec Improvement Correlation** | N/A | >0.7 | Correlation between clarification and spec quality |
| **Rework Reduction** | Baseline | -20% | % reduction in post-spec change requests |
| **User Satisfaction** | N/A | >4/5 | User rating of clarification UX |
| **Assumption Burn-Down Rate** | N/A | >50%/month | % of provisional assumptions replaced with answers |

### Phase 3: Reward System with Compute Tokens ‚≠ê **UPDATED**

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| **Contributor Registration** | 0 | >50 (3 months) | Total registered contributors |
| **STF Contributions** | 0 | >100 (6 months) | Total STFs contributed |
| **STF Quality Score** | N/A | >0.8 | Average quality score of contributed STFs |
| **Token Distribution** | 0 tokens | N/A | Total tokens earned by contributors |
| **Token Velocity** | N/A | >1 cycle/week | Earn ‚Üí spend ‚Üí earn circulation time |
| **Community Satisfaction** | N/A | >4/5 | Contributor satisfaction rating |
| **Economic Equilibrium** | N/A | ¬±20% | Token supply vs demand balance |
| **Adoption Rate** | 0% | >20% | % of tasks using contributed STFs |

---

## Conclusion ‚≠ê **UPDATED WITH COMPUTE TOKEN MODEL**

The proposal represents a **bold and comprehensive vision** for evolving OmniClaude into a self-improving, economically sustainable, and highly observable system. With **compute tokens** as the reward currency, the three initiatives now have **significantly improved feasibility**, particularly Phase 3 which transforms from "high-risk, defer" to "medium-risk, feasible after Phase 1".

### Key Takeaways ‚≠ê **UPDATED**

1. **OmniClaude already has 40-50% of the debug loop features** - Existing observability infrastructure is strong
2. **Phase 1 (Debug Loop) is a natural evolution** - Low risk, clear value, aligns with current architecture
3. **Phase 2 (Clarification) is feasible but requires UX validation** - Medium risk, moderate value
4. **Phase 3 (Reward System) is NOW FEASIBLE with compute tokens** ‚¨ÜÔ∏è - Legal/funding risks eliminated, 80% cost reduction

**Compute Token Model Changes Everything**:
- ‚úÖ Legal risk: Critical ‚Üí Manageable (internal credits simplify but basic review still needed)
- ‚úÖ Timeline: 8-12 weeks ‚Üí 4-6 weeks (50% reduction)
- ‚úÖ Cost: $100-200K+ ‚Üí $20-30K (80-85% reduction)
- ‚úÖ Funding: External sources needed ‚Üí Self-sustaining economy
- ‚úÖ Complexity: High governance ‚Üí Simple core team decisions

### Recommended Path Forward ‚≠ê **UPDATED**

**Immediate** (Next 2 weeks):
1. Clarify strategic goals and priorities
2. Complete PR #22 merge
3. Validate performance baselines
4. **Define token economics** (tokens per STF, tokens per compute hour, free tier)

**Short-term** (Next 1-2 months):
1. **Implement Phase 1: Debug Loop Enhancements** (STF registry + cost tracking)
2. Measure impact and validate assumptions
3. **Plan Phase 3: Reward System** (token economics, verification sandbox, governance minimum)

**Medium-term** (Next 3-6 months):
1. **Implement Phase 3: Reward System with Compute Tokens** (builds on Phase 1)
2. Monitor effectiveness (contributor growth, token velocity, STF quality)
3. **Evaluate Phase 2** (clarification workflow) based on spec quality issues

**Long-term** (6-12 months):
1. **Implement Phase 2 only if** clarification workflow is needed
2. Monitor system complexity and plan consolidation if needed
3. Community growth and ecosystem development

### Final Recommendation ‚≠ê **SIGNIFICANTLY CHANGED**

‚úÖ **Proceed with Phase 1 (Debug Loop Enhancements)** - Clear value, low risk, natural fit, foundation for Phase 3

‚úÖ **Proceed with Phase 3 (Reward System with Compute Tokens)** ‚¨ÜÔ∏è **UPGRADED** - Now feasible after Phase 1, self-sustaining economy, minimal legal risk, 5-10x ROI

ü§î **Evaluate Phase 2 (Clarification Workflow)** - Depends on whether ambiguity is a major pain point (optional)

**Compute tokens transform Phase 3 from "defer indefinitely" to "high priority after Phase 1".**

**Total Timeline for Core Value** (Phases 1 + 3): **7-10 weeks** vs 11-16 weeks originally

**Total Cost for Core Value** (Phases 1 + 3): **$35-50K** vs $115-220K originally (‚¨áÔ∏è **70-80% reduction**)

**Question for you**: What is the **most critical problem** you're trying to solve?
- Debugging difficulty + contributor economy? ‚Üí Phase 1 then Phase 3 (7-10 weeks)
- Specification ambiguity? ‚Üí Phase 2 then Phase 1 (5-7 weeks)
- All three? ‚Üí Phase 1 ‚Üí Phase 3 ‚Üí Phase 2 (9-13 weeks total)

---

**Document Version**: 2.0 ‚≠ê **MAJOR UPDATE - COMPUTE TOKEN MODEL**
**Last Updated**: 2025-11-09
**Status**: Draft for Review
**Next Review**: After token economics defined and strategic goals clarified

**Version History**:
- **v2.0 (2025-11-09)**: Major update - Compute token model analysis added, Phase 3 risk/cost/timeline significantly reduced, recommendations updated
- **v1.0 (2025-11-09)**: Initial analysis with fiat/cryptocurrency reward assumptions
