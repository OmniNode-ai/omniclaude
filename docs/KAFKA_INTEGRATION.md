# Kafka Integration Guide - Agent Observability System

**Version**: 1.0.0
**Created**: 2025-10-20
**Correlation ID**: 7415f61f-ab2f-4961-97f5-a5f5c6499a4d
**Status**: Production-Ready Architecture

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Architecture Overview](#architecture-overview)
3. [Topic Definitions](#topic-definitions)
4. [Producer Implementation (Skills)](#producer-implementation-skills)
5. [Consumer Architecture](#consumer-architecture)
6. [Deployment Guide](#deployment-guide)
7. [Monitoring & Alerting](#monitoring--alerting)
8. [Migration from Direct DB](#migration-from-direct-db)
9. [Troubleshooting](#troubleshooting)
10. [Performance Tuning](#performance-tuning)

---

## Executive Summary

### Why Kafka for Agent Observability?

The shift from direct database writes to Kafka-based event streaming provides:

| Capability | Direct DB | Kafka Event Bus | Improvement |
|-----------|-----------|-----------------|-------------|
| **Agent Latency** | 20-100ms blocking | <5ms async | **95% faster** |
| **Event Replay** | None | Full audit trail | **Time-travel debugging** |
| **Multiple Consumers** | Single purpose | DB + analytics + alerts + ML | **4+ use cases** |
| **Fault Tolerance** | Fails immediately | Buffered, retries | **Zero data loss** |
| **Scalability** | Vertical only | Horizontal | **1M+ events/sec** |
| **Decoupling** | Tight coupling | Producers â†” Consumers | **Independent scaling** |

### Key Architectural Principles

1. **Agents NEVER write to database directly** - All logging flows through Kafka
2. **Skills publish events** - Simple, fast, non-blocking Kafka producers
3. **Consumers handle complexity** - Batch writes, aggregations, alerting, ML
4. **Event-driven design** - Events are source of truth, consumers derive state
5. **Exactly-once semantics** - Transactional consumers prevent data duplication

---

## Architecture Overview

### System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Agent Observability System                      â”‚
â”‚                        (Kafka-Based Architecture)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     Agent Layer (Producers)                      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚ â”‚
â”‚  â”‚  â”‚ Polymorphic  â”‚  â”‚ API Architectâ”‚  â”‚ Performance  â”‚  ...     â”‚ â”‚
â”‚  â”‚  â”‚ Agent (Polly)â”‚  â”‚ Agent        â”‚  â”‚ Agent        â”‚          â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ â”‚
â”‚  â”‚         â”‚                  â”‚                  â”‚                   â”‚ â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ â”‚
â”‚  â”‚                            â”‚                                       â”‚ â”‚
â”‚  â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚ â”‚
â”‚  â”‚                  â”‚ Agent Skills      â”‚                            â”‚ â”‚
â”‚  â”‚                  â”‚ (Kafka Producers) â”‚                            â”‚ â”‚
â”‚  â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                â”‚                                       â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                      Kafka Cluster                               â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  Broker 1      Broker 2      Broker 3     (Replication)   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚                                                             â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  Topics (with partitions):                                 â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”œâ”€ agent-actions (12 partitions)                          â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”œâ”€ agent-routing-decisions (6 partitions)                 â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”œâ”€ agent-transformation-events (6 partitions)             â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”œâ”€ router-performance-metrics (6 partitions)              â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”œâ”€ task-executions (12 partitions)                        â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”œâ”€ code-quality-metrics (6 partitions)                    â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”œâ”€ success-patterns (6 partitions)                        â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€ failure-patterns (6 partitions)                        â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                        â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                  Consumer Groups (Parallel Processing)           â”‚ â”‚
â”‚  â”‚                                                                   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚ â”‚
â”‚  â”‚  â”‚ Database Writer    â”‚  â”‚ Metrics Aggregator â”‚                â”‚ â”‚
â”‚  â”‚  â”‚ (Batch Inserts)    â”‚  â”‚ (Real-time Calc)   â”‚                â”‚ â”‚
â”‚  â”‚  â”‚ - PostgreSQL       â”‚  â”‚ - Redis            â”‚                â”‚ â”‚
â”‚  â”‚  â”‚ - Exactly-once     â”‚  â”‚ - Rollups          â”‚                â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚ â”‚
â”‚  â”‚                                                                   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚ â”‚
â”‚  â”‚  â”‚ Alert Manager      â”‚  â”‚ Pattern Extractor  â”‚                â”‚ â”‚
â”‚  â”‚  â”‚ (Failure Detection)â”‚  â”‚ (ML Learning)      â”‚                â”‚ â”‚
â”‚  â”‚  â”‚ - PagerDuty/Slack  â”‚  â”‚ - Template Gen     â”‚                â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚ â”‚
â”‚  â”‚                                                                   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚ â”‚
â”‚  â”‚  â”‚ Audit Logger       â”‚                                         â”‚ â”‚
â”‚  â”‚  â”‚ (Compliance)       â”‚                                         â”‚ â”‚
â”‚  â”‚  â”‚ - S3 Archival      â”‚                                         â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                      Storage Layer                               â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚ â”‚
â”‚  â”‚  â”‚ PostgreSQL   â”‚  â”‚ Redis        â”‚  â”‚ S3/Object    â”‚          â”‚ â”‚
â”‚  â”‚  â”‚ (Structured) â”‚  â”‚ (Real-time)  â”‚  â”‚ Storage      â”‚          â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
1. Agent executes action (e.g., routing decision)
   â†“
2. Skill invoked (/log-routing-decision)
   â†“
3. Skill serializes event to JSON
   â†“
4. Kafka producer publishes to topic (async, <5ms)
   â†“
5. Kafka persists event to partition (durable, replicated)
   â†“
6. Multiple consumer groups consume event in parallel:
   a. Database Writer â†’ PostgreSQL (batch insert, 100 events/500ms)
   b. Metrics Aggregator â†’ Redis (real-time metrics, 5-sec windows)
   c. Alert Manager â†’ PagerDuty (failure threshold monitoring)
   d. Pattern Extractor â†’ ML pipeline (pattern recognition)
   e. Audit Logger â†’ S3 (compliance archival, Parquet format)
   â†“
7. Consumers commit offsets (exactly-once semantics)
   â†“
8. State available for queries:
   - PostgreSQL: Historical queries, analytics
   - Redis: Real-time dashboards, API queries
   - S3: Audit trails, event replay
```

---

## Topic Definitions

### Topic Naming Convention

**Pattern**: `{domain}-{entity}-{plural}`

Examples:
- `agent-actions` (not `agent-action` or `agents-actions`)
- `agent-routing-decisions` (not `routing-decision`)
- `code-quality-metrics` (not `code-quality-metric`)

### Topic Schema Specifications

#### 1. `agent-actions` Topic

**Purpose**: Log every action an agent takes (tool calls, decisions, errors).

**Partitions**: 12 (based on correlation_id for ordering)
**Retention**: 7 days (debug data, purge after)
**Replication Factor**: 3 (fault tolerance)

**Event Schema** (JSON):

```json
{
  "event_id": "uuid",
  "correlation_id": "uuid",
  "agent_name": "polymorphic-agent",
  "action_type": "tool_call | decision | error | success",
  "action_name": "Read | Write | route_to_agent | etc",
  "action_details": {
    // Action-specific fields (flexible schema)
    "file_path": "/path/to/file.py",
    "lines_read": 150,
    "duration_ms": 45
  },
  "debug_mode": true,
  "duration_ms": 45,
  "timestamp": "2025-10-20T18:30:00.123Z",
  "metadata": {
    "skill_version": "1.0.0",
    "kafka_producer_id": "skill-log-agent-action-001"
  }
}
```

**Partitioning Key**: `correlation_id` (ensures all actions in same trace go to same partition)

**Consumer Groups**:
- `db-writer-group` - Writes to `agent_actions` table
- `metrics-group` - Real-time action count metrics
- `audit-group` - S3 archival for compliance

---

#### 2. `agent-routing-decisions` Topic

**Purpose**: Track agent selection decisions for routing intelligence.

**Partitions**: 6
**Retention**: 30 days
**Replication Factor**: 3

**Event Schema**:

```json
{
  "event_id": "uuid",
  "correlation_id": "uuid",
  "user_request": "optimize database queries",
  "selected_agent": "agent-performance",
  "confidence_score": 0.92,
  "alternatives": [
    {
      "agent_name": "agent-database",
      "confidence_score": 0.85,
      "reason": "Database keyword match"
    },
    {
      "agent_name": "agent-debug",
      "confidence_score": 0.73,
      "reason": "General optimization capability"
    }
  ],
  "reasoning": "High confidence match on 'optimize' and 'performance' triggers",
  "routing_strategy": "enhanced_fuzzy_matching",
  "context": {
    "domain": "database_optimization",
    "previous_agent": "polymorphic-agent",
    "file_context": ["api/database.py"]
  },
  "routing_time_ms": 45,
  "timestamp": "2025-10-20T18:30:00.123Z"
}
```

**Partitioning Key**: `correlation_id`

**Consumer Groups**:
- `db-writer-group` - `agent_routing_decisions` table
- `analytics-group` - Routing accuracy analysis
- `learning-group` - ML model training data

---

#### 3. `agent-transformation-events` Topic

**Purpose**: Track polymorphic agent transformations (identity changes).

**Partitions**: 6
**Retention**: 30 days
**Replication Factor**: 3

**Event Schema**:

```json
{
  "event_id": "uuid",
  "correlation_id": "uuid",
  "source_agent": "polymorphic-agent",
  "target_agent": "agent-performance",
  "transformation_reason": "High confidence routing decision",
  "confidence_score": 0.92,
  "transformation_duration_ms": 45,
  "success": true,
  "context_preserved": {
    "user_request": "optimize database queries",
    "correlation_id": "uuid",
    "session_id": "uuid"
  },
  "timestamp": "2025-10-20T18:30:00.123Z"
}
```

**Partitioning Key**: `correlation_id`

**Consumer Groups**:
- `db-writer-group` - `agent_transformation_events` table
- `monitoring-group` - Transformation success rate tracking
- `alert-group` - Failed transformation alerts

---

#### 4. `router-performance-metrics` Topic

**Purpose**: Router performance metrics for optimization.

**Partitions**: 6
**Retention**: 30 days
**Replication Factor**: 3

**Event Schema**:

```json
{
  "event_id": "uuid",
  "correlation_id": "uuid",
  "query_text": "optimize database queries",
  "routing_duration_ms": 45,
  "cache_hit": false,
  "trigger_match_strategy": "enhanced_fuzzy_matching",
  "confidence_components": {
    "trigger_score": 0.95,
    "context_score": 0.88,
    "capability_score": 0.92,
    "historical_score": 0.90
  },
  "candidates_evaluated": 3,
  "timestamp": "2025-10-20T18:30:00.123Z"
}
```

**Partitioning Key**: `correlation_id`

**Consumer Groups**:
- `db-writer-group` - `router_performance_metrics` table
- `perf-group` - Real-time performance dashboards
- `optimization-group` - Router algorithm optimization

---

#### 5. `task-executions` Topic

**Purpose**: Complete task execution capture for learning.

**Partitions**: 12
**Retention**: 90 days
**Replication Factor**: 3

**Event Schema**:

```json
{
  "event_id": "uuid",
  "correlation_id": "uuid",
  "fingerprint_id": "uuid",
  "task_hash": "sha256_hash",
  "original_prompt": "Create FastAPI CRUD endpoint for users",
  "task_type": "code_generation",
  "domain": "api_development",
  "agent_name": "agent-api-architect",
  "agent_chain": ["polymorphic-agent", "agent-api-architect", "agent-testing"],
  "success": true,
  "quality_score": 0.92,
  "complexity_score": 0.65,
  "steps": [
    {
      "step_number": 1,
      "action": "rag_query",
      "duration_ms": 1200,
      "outcome": "success"
    },
    {
      "step_number": 2,
      "action": "code_generation",
      "files": ["api/users.py"],
      "duration_ms": 850,
      "outcome": "success"
    }
  ],
  "total_duration_ms": 15400,
  "tools_used": ["perform_rag_query", "Write", "Bash"],
  "user_acceptance": true,
  "timestamp": "2025-10-20T18:30:00.123Z"
}
```

**Partitioning Key**: `fingerprint_id` (co-locates similar tasks)

**Consumer Groups**:
- `db-writer-group` - `execution_paths` table
- `learning-group` - Pattern extraction and template generation
- `analytics-group` - Success rate and quality metrics

---

#### 6. `code-quality-metrics` Topic

**Purpose**: Code artifact quality tracking.

**Partitions**: 6
**Retention**: 90 days
**Replication Factor**: 3

**Event Schema**:

```json
{
  "event_id": "uuid",
  "correlation_id": "uuid",
  "path_id": "uuid",
  "fingerprint_id": "uuid",
  "file_path": "api/users.py",
  "artifact_type": "source_code",
  "language": "python",
  "quality_score": 0.92,
  "onex_compliance_score": 0.95,
  "lines_added": 145,
  "lines_modified": 0,
  "lines_deleted": 0,
  "test_coverage": 87.5,
  "tests_passed": 12,
  "tests_failed": 0,
  "lint_passed": true,
  "type_check_passed": true,
  "security_scan_passed": true,
  "onex_node_type": "Effect",
  "complexity_metrics": {
    "cyclomatic_complexity": 5,
    "cognitive_complexity": 3,
    "nesting_depth": 2
  },
  "timestamp": "2025-10-20T18:30:00.123Z"
}
```

**Partitioning Key**: `path_id`

**Consumer Groups**:
- `db-writer-group` - `code_artifacts` table
- `quality-group` - Quality trend analysis
- `alert-group` - Low quality alerts

---

#### 7. `success-patterns` Topic

**Purpose**: Successful execution patterns for reuse.

**Partitions**: 6
**Retention**: 365 days (long-term learning data)
**Replication Factor**: 3

**Event Schema**:

```json
{
  "event_id": "uuid",
  "correlation_id": "uuid",
  "pattern_id": "uuid",
  "fingerprint_id": "uuid",
  "path_id": "uuid",
  "pattern_name": "FastAPI CRUD Endpoint Pattern",
  "pattern_type": "workflow",
  "pattern_template": {
    "steps": [
      {"action": "rag_query", "query": "FastAPI CRUD {{entity}} best practices"},
      {"action": "generate_code", "template": "onex_effect_crud"},
      {"action": "validate", "checks": ["type_safety", "onex_compliance"]}
    ],
    "placeholders": {
      "entity_name": "string",
      "domain": "string"
    }
  },
  "applicability_criteria": {
    "task_types": ["code_generation"],
    "domains": ["api_development"],
    "technologies": ["fastapi", "pydantic"]
  },
  "validation_criteria": {
    "min_quality_score": 0.85,
    "min_test_coverage": 85
  },
  "estimated_duration_ms": 14200,
  "timestamp": "2025-10-20T18:30:00.123Z"
}
```

**Partitioning Key**: `pattern_id`

**Consumer Groups**:
- `db-writer-group` - `success_patterns` table
- `ml-group` - Template extraction and refinement
- `recommendation-group` - Pattern recommendation engine

---

#### 8. `failure-patterns` Topic

**Purpose**: Failure tracking for debugging and prevention.

**Partitions**: 6
**Retention**: 365 days
**Replication Factor**: 3

**Event Schema**:

```json
{
  "event_id": "uuid",
  "correlation_id": "uuid",
  "failure_id": "uuid",
  "fingerprint_id": "uuid",
  "path_id": "uuid",
  "failure_type": "validation_failure",
  "error_category": "type_error",
  "error_message": "Expected str, got int for field 'user_id'",
  "error_stack_trace": "Traceback...",
  "failed_step": {
    "step_number": 4,
    "action": "type_check",
    "tool": "mypy"
  },
  "root_cause": "Inconsistent ID format between legacy and new database records",
  "contributing_factors": ["database_migration_incomplete", "mixed_data_types"],
  "resolution": "Changed product_id field type from str to Union[str, int]",
  "resolution_steps": [
    {"action": "identify_root_cause", "duration_ms": 1200},
    {"action": "update_type_annotations", "files": ["models/product.py"]}
  ],
  "time_to_resolve_ms": 3400,
  "prevention_strategy": {
    "strategy": "Add runtime type coercion in Pydantic validators",
    "automated_fix_possible": true
  },
  "timestamp": "2025-10-20T18:30:00.123Z"
}
```

**Partitioning Key**: `failure_id`

**Consumer Groups**:
- `db-writer-group` - `failure_patterns` table
- `alert-group` - Repeated failure alerts (>3 occurrences)
- `ml-group` - Automated fix generation

---

## Producer Implementation (Skills)

### Skill Template

All agent-tracking skills follow this Kafka producer pattern:

```python
#!/usr/bin/env python3
"""
Skill: /log-routing-decision
Purpose: Publish routing decision events to Kafka
Topic: agent-routing-decisions
"""

import os
import sys
import json
import uuid
from datetime import datetime
from kafka import KafkaProducer
from kafka.errors import KafkaError

# Environment configuration
KAFKA_BROKERS = os.getenv('KAFKA_BROKERS', 'localhost:9092').split(',')
KAFKA_ENABLE_LOGGING = os.getenv('KAFKA_ENABLE_LOGGING', 'true').lower() == 'true'
DEBUG = os.getenv('DEBUG', 'false').lower() == 'true'

# Topic configuration
TOPIC_NAME = 'agent-routing-decisions'
PARTITION_KEY_FIELD = 'correlation_id'

def create_kafka_producer():
    """Create Kafka producer with proper configuration."""
    return KafkaProducer(
        bootstrap_servers=KAFKA_BROKERS,
        value_serializer=lambda v: json.dumps(v).encode('utf-8'),
        key_serializer=lambda k: k.encode('utf-8') if k else None,
        acks='all',  # Wait for all replicas to acknowledge
        retries=3,  # Retry on failure
        max_in_flight_requests_per_connection=1,  # Ensure ordering
        compression_type='snappy',  # Fast compression
        linger_ms=10,  # Small batching for low latency
        batch_size=16384,  # Batch size in bytes
    )

def publish_routing_decision(
    agent: str,
    confidence: float,
    strategy: str,
    latency_ms: int,
    user_request: str,
    reasoning: str,
    correlation_id: str,
    alternatives: list = None
):
    """
    Publish routing decision event to Kafka.

    Returns:
        dict: Result with success status and event_id
    """
    if not KAFKA_ENABLE_LOGGING:
        return {
            "success": True,
            "message": "Kafka logging disabled (KAFKA_ENABLE_LOGGING=false)",
            "event_id": None
        }

    try:
        # Create event payload
        event_id = str(uuid.uuid4())
        event = {
            "event_id": event_id,
            "correlation_id": correlation_id,
            "user_request": user_request,
            "selected_agent": agent,
            "confidence_score": confidence,
            "alternatives": alternatives or [],
            "reasoning": reasoning,
            "routing_strategy": strategy,
            "context": {},  # Can be enriched by caller
            "routing_time_ms": latency_ms,
            "timestamp": datetime.utcnow().isoformat() + 'Z'
        }

        # Create producer and publish
        producer = create_kafka_producer()

        # Partition by correlation_id for ordering
        partition_key = event[PARTITION_KEY_FIELD]

        # Async publish with callback
        future = producer.send(
            TOPIC_NAME,
            key=partition_key,
            value=event
        )

        # Wait for acknowledgment (with timeout)
        record_metadata = future.get(timeout=5)

        # Flush to ensure delivery
        producer.flush()
        producer.close()

        if DEBUG:
            print(f"âœ… Published routing decision to Kafka", file=sys.stderr)
            print(f"   Topic: {record_metadata.topic}", file=sys.stderr)
            print(f"   Partition: {record_metadata.partition}", file=sys.stderr)
            print(f"   Offset: {record_metadata.offset}", file=sys.stderr)

        return {
            "success": True,
            "event_id": event_id,
            "topic": record_metadata.topic,
            "partition": record_metadata.partition,
            "offset": record_metadata.offset,
            "latency_ms": latency_ms
        }

    except KafkaError as e:
        print(f"âŒ Kafka publish failed: {e}", file=sys.stderr)
        return {
            "success": False,
            "error": str(e),
            "event_id": None
        }
    except Exception as e:
        print(f"âŒ Unexpected error: {e}", file=sys.stderr)
        return {
            "success": False,
            "error": str(e),
            "event_id": None
        }

def main():
    """CLI entry point for skill execution."""
    import argparse

    parser = argparse.ArgumentParser(description='Log routing decision to Kafka')
    parser.add_argument('--agent', required=True, help='Selected agent name')
    parser.add_argument('--confidence', type=float, required=True, help='Confidence score (0.0-1.0)')
    parser.add_argument('--strategy', required=True, help='Routing strategy used')
    parser.add_argument('--latency-ms', type=int, required=True, help='Routing latency in ms')
    parser.add_argument('--user-request', required=True, help='Original user request')
    parser.add_argument('--reasoning', required=True, help='Why this agent was selected')
    parser.add_argument('--correlation-id', required=True, help='Correlation ID')
    parser.add_argument('--alternatives', help='JSON array of alternatives')

    args = parser.parse_args()

    # Parse alternatives if provided
    alternatives = json.loads(args.alternatives) if args.alternatives else None

    # Publish event
    result = publish_routing_decision(
        agent=args.agent,
        confidence=args.confidence,
        strategy=args.strategy,
        latency_ms=args.latency_ms,
        user_request=args.user_request,
        reasoning=args.reasoning,
        correlation_id=args.correlation_id,
        alternatives=alternatives
    )

    # Output result
    print(json.dumps(result, indent=2))
    sys.exit(0 if result['success'] else 1)

if __name__ == '__main__':
    main()
```

### Skill Configuration

**Common Environment Variables** (all skills):

```bash
# Kafka connection
KAFKA_BROKERS=localhost:9092,localhost:9093,localhost:9094  # Comma-separated

# Feature flags
KAFKA_ENABLE_LOGGING=true  # Master switch for Kafka logging
DEBUG=false  # Verbose debugging output

# Producer tuning
KAFKA_COMPRESSION=snappy  # snappy, gzip, lz4, or none
KAFKA_BATCH_SIZE=16384  # Bytes per batch
KAFKA_LINGER_MS=10  # Max time to wait for batch
KAFKA_RETRIES=3  # Retry attempts on failure
KAFKA_ACKS=all  # Wait for all replicas (strongest durability)
```

**Skill-Specific Configuration**:

Each skill specifies:
- `TOPIC_NAME` - Which Kafka topic to publish to
- `PARTITION_KEY_FIELD` - Event field used for partitioning (e.g., `correlation_id`)
- Event schema validation rules

---

## Consumer Architecture

### Consumer Groups

Each consumer group processes all events independently:

1. **db-writer-group** - Database persistence
2. **metrics-group** - Real-time metrics aggregation
3. **alert-group** - Failure detection and alerting
4. **ml-group** - Pattern extraction and learning
5. **audit-group** - Compliance logging and archival

### Consumer Implementation Template

```python
#!/usr/bin/env python3
"""
Consumer: Database Writer
Purpose: Consume events from all topics and persist to PostgreSQL
Consumer Group: db-writer-group
"""

import os
import json
from kafka import KafkaConsumer
from kafka.errors import KafkaError
import psycopg2
from psycopg2.extras import execute_batch

# Kafka configuration
KAFKA_BROKERS = os.getenv('KAFKA_BROKERS', 'localhost:9092').split(',')
CONSUMER_GROUP_ID = 'db-writer-group'
TOPICS = [
    'agent-actions',
    'agent-routing-decisions',
    'agent-transformation-events',
    'router-performance-metrics',
    'task-executions',
    'code-quality-metrics',
    'success-patterns',
    'failure-patterns'
]

# Database configuration
DB_CONFIG = {
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': int(os.getenv('POSTGRES_PORT', 5436)),
    'database': os.getenv('POSTGRES_DATABASE', 'omninode_bridge'),
    'user': os.getenv('POSTGRES_USER', 'postgres'),
    'password': os.getenv('POSTGRES_PASSWORD')
}

# Batch configuration
BATCH_SIZE = 100  # Events per batch insert
MAX_LATENCY_MS = 500  # Max time to wait for batch

def create_consumer():
    """Create Kafka consumer with proper configuration."""
    return KafkaConsumer(
        *TOPICS,
        bootstrap_servers=KAFKA_BROKERS,
        group_id=CONSUMER_GROUP_ID,
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        key_deserializer=lambda k: k.decode('utf-8') if k else None,
        auto_offset_reset='earliest',  # Start from beginning if no offset
        enable_auto_commit=False,  # Manual commit for exactly-once semantics
        max_poll_records=BATCH_SIZE,  # Batch size for processing
        session_timeout_ms=30000,  # Consumer heartbeat timeout
        max_poll_interval_ms=300000  # Max time between polls
    )

def process_batch(consumer, conn, cursor):
    """
    Process a batch of events with transactional writes.

    Returns:
        int: Number of events processed
    """
    messages = consumer.poll(timeout_ms=MAX_LATENCY_MS, max_records=BATCH_SIZE)

    if not messages:
        return 0

    events_processed = 0

    try:
        # Start transaction
        conn.autocommit = False

        # Process events by topic
        for topic_partition, records in messages.items():
            topic = topic_partition.topic

            for record in records:
                event = record.value

                # Route to appropriate handler
                if topic == 'agent-routing-decisions':
                    insert_routing_decision(cursor, event)
                elif topic == 'agent-transformation-events':
                    insert_transformation_event(cursor, event)
                elif topic == 'router-performance-metrics':
                    insert_performance_metric(cursor, event)
                elif topic == 'task-executions':
                    insert_task_execution(cursor, event)
                elif topic == 'code-quality-metrics':
                    insert_code_quality(cursor, event)
                elif topic == 'success-patterns':
                    insert_success_pattern(cursor, event)
                elif topic == 'failure-patterns':
                    insert_failure_pattern(cursor, event)
                elif topic == 'agent-actions':
                    insert_agent_action(cursor, event)

                events_processed += 1

        # Commit database transaction
        conn.commit()

        # Commit Kafka offsets (exactly-once semantics)
        consumer.commit()

        return events_processed

    except Exception as e:
        # Rollback on error
        conn.rollback()
        print(f"âŒ Batch processing failed: {e}")
        raise

def insert_routing_decision(cursor, event):
    """Insert routing decision into database."""
    cursor.execute("""
        INSERT INTO agent_routing_decisions (
            id, user_request, selected_agent, confidence_score,
            alternatives, reasoning, routing_strategy, context,
            routing_time_ms, correlation_id, created_at
        ) VALUES (
            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
        )
        ON CONFLICT (id) DO NOTHING
    """, (
        event['event_id'],
        event['user_request'],
        event['selected_agent'],
        event['confidence_score'],
        json.dumps(event.get('alternatives', [])),
        event['reasoning'],
        event['routing_strategy'],
        json.dumps(event.get('context', {})),
        event['routing_time_ms'],
        event['correlation_id'],
        event['timestamp']
    ))

def main():
    """Main consumer loop."""
    print("ðŸš€ Starting Database Writer Consumer")
    print(f"   Topics: {', '.join(TOPICS)}")
    print(f"   Consumer Group: {CONSUMER_GROUP_ID}")
    print(f"   Batch Size: {BATCH_SIZE}")

    # Create Kafka consumer
    consumer = create_consumer()

    # Database connection with connection pooling
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()

    try:
        total_processed = 0

        while True:
            # Process batch
            events_processed = process_batch(consumer, conn, cursor)

            if events_processed > 0:
                total_processed += events_processed
                print(f"âœ… Processed {events_processed} events (total: {total_processed})")

    except KeyboardInterrupt:
        print("\nâ¹ï¸  Shutting down consumer gracefully...")
    finally:
        consumer.close()
        cursor.close()
        conn.close()
        print(f"   Total events processed: {total_processed}")

if __name__ == '__main__':
    main()
```

### Consumer Deployment

**Docker Compose Configuration**:

```yaml
version: '3.8'

services:
  # Database Writer Consumer
  db-writer-consumer:
    build:
      context: ./consumers/db-writer
      dockerfile: Dockerfile
    environment:
      KAFKA_BROKERS: kafka1:9092,kafka2:9092,kafka3:9092
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DATABASE: omninode_bridge
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      CONSUMER_GROUP_ID: db-writer-group
      BATCH_SIZE: 100
      MAX_LATENCY_MS: 500
    depends_on:
      - kafka1
      - kafka2
      - kafka3
      - postgres
    restart: unless-stopped
    deploy:
      replicas: 3  # Scale for throughput
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

  # Metrics Aggregator Consumer
  metrics-consumer:
    build:
      context: ./consumers/metrics-aggregator
      dockerfile: Dockerfile
    environment:
      KAFKA_BROKERS: kafka1:9092,kafka2:9092,kafka3:9092
      REDIS_HOST: redis
      REDIS_PORT: 6379
      CONSUMER_GROUP_ID: metrics-group
      WINDOW_SIZE_SECONDS: 5
    depends_on:
      - kafka1
      - redis
    restart: unless-stopped

  # Alert Manager Consumer
  alert-consumer:
    build:
      context: ./consumers/alert-manager
      dockerfile: Dockerfile
    environment:
      KAFKA_BROKERS: kafka1:9092,kafka2:9092,kafka3:9092
      PAGERDUTY_API_KEY: ${PAGERDUTY_API_KEY}
      SLACK_WEBHOOK_URL: ${SLACK_WEBHOOK_URL}
      CONSUMER_GROUP_ID: alert-group
      FAILURE_THRESHOLD: 3
    depends_on:
      - kafka1
    restart: unless-stopped

  # Pattern Extractor Consumer
  ml-consumer:
    build:
      context: ./consumers/pattern-extractor
      dockerfile: Dockerfile
    environment:
      KAFKA_BROKERS: kafka1:9092,kafka2:9092,kafka3:9092
      ML_MODEL_PATH: /models/pattern-extraction
      CONSUMER_GROUP_ID: ml-group
    depends_on:
      - kafka1
    restart: unless-stopped
    volumes:
      - ./models:/models:ro

  # Audit Logger Consumer
  audit-consumer:
    build:
      context: ./consumers/audit-logger
      dockerfile: Dockerfile
    environment:
      KAFKA_BROKERS: kafka1:9092,kafka2:9092,kafka3:9092
      S3_BUCKET: agent-observability-audit
      S3_PREFIX: events/
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      CONSUMER_GROUP_ID: audit-group
      COMPRESSION: parquet
    depends_on:
      - kafka1
    restart: unless-stopped
```

---

## Deployment Guide

### Prerequisites

1. **Kafka Cluster**: 3+ brokers for production (replication factor 3)
2. **ZooKeeper Ensemble**: 3+ nodes for Kafka coordination
3. **PostgreSQL**: Version 12+ with pg_vector extension
4. **Redis**: For real-time metrics caching
5. **S3/Object Storage**: For audit trail archival

### Step-by-Step Deployment

#### Step 1: Deploy Kafka Cluster

```bash
# Using Docker Compose
cd infrastructure/kafka
docker-compose up -d

# Verify cluster health
docker-compose exec kafka1 kafka-broker-api-versions --bootstrap-server kafka1:9092
```

**Kafka Configuration** (`server.properties`):

```properties
# Broker configuration
broker.id=1
listeners=PLAINTEXT://0.0.0.0:9092
advertised.listeners=PLAINTEXT://kafka1:9092
zookeeper.connect=zk1:2181,zk2:2181,zk3:2181

# Replication
default.replication.factor=3
min.insync.replicas=2
unclean.leader.election.enable=false

# Log retention
log.retention.hours=168  # 7 days default
log.retention.bytes=-1  # Unlimited
log.segment.bytes=1073741824  # 1GB segments

# Performance tuning
num.network.threads=8
num.io.threads=16
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
```

#### Step 2: Create Topics

```bash
# Script to create all topics with proper configuration
#!/bin/bash

BOOTSTRAP_SERVERS="kafka1:9092,kafka2:9092,kafka3:9092"
REPLICATION_FACTOR=3

# Function to create topic
create_topic() {
    local TOPIC=$1
    local PARTITIONS=$2
    local RETENTION_MS=$3

    kafka-topics --create \
        --bootstrap-server $BOOTSTRAP_SERVERS \
        --topic $TOPIC \
        --partitions $PARTITIONS \
        --replication-factor $REPLICATION_FACTOR \
        --config retention.ms=$RETENTION_MS \
        --config compression.type=snappy \
        --config min.insync.replicas=2

    echo "âœ… Created topic: $TOPIC ($PARTITIONS partitions, ${RETENTION_MS}ms retention)"
}

# Create all topics
create_topic "agent-actions" 12 604800000  # 7 days
create_topic "agent-routing-decisions" 6 2592000000  # 30 days
create_topic "agent-transformation-events" 6 2592000000  # 30 days
create_topic "router-performance-metrics" 6 2592000000  # 30 days
create_topic "task-executions" 12 7776000000  # 90 days
create_topic "code-quality-metrics" 6 7776000000  # 90 days
create_topic "success-patterns" 6 31536000000  # 365 days
create_topic "failure-patterns" 6 31536000000  # 365 days

echo "âœ… All topics created successfully"

# Verify topics
kafka-topics --list --bootstrap-server $BOOTSTRAP_SERVERS
```

#### Step 3: Deploy Consumers

```bash
# Build consumer Docker images
cd consumers
docker build -t agent-db-writer:latest -f db-writer/Dockerfile .
docker build -t agent-metrics:latest -f metrics-aggregator/Dockerfile .
docker build -t agent-alerts:latest -f alert-manager/Dockerfile .
docker build -t agent-ml:latest -f pattern-extractor/Dockerfile .
docker build -t agent-audit:latest -f audit-logger/Dockerfile .

# Deploy with Docker Compose
docker-compose -f docker-compose.consumers.yml up -d

# Verify consumers are running
docker-compose ps
docker-compose logs -f db-writer-consumer
```

#### Step 4: Update Agent Skills

```bash
# Update skills to use Kafka producers
cd /Volumes/PRO-G40/Code/omniclaude/skills/agent-tracking

# Update each skill script to use Kafka template
# (See Producer Implementation section above)

# Test skill locally
export KAFKA_BROKERS=localhost:9092
export DEBUG=true

/log-routing-decision \
  --agent test-agent \
  --confidence 0.9 \
  --strategy test \
  --latency-ms 50 \
  --user-request "test request" \
  --reasoning "test" \
  --correlation-id $(uuidgen)
```

#### Step 5: Verify End-to-End Flow

```bash
# 1. Publish test event
./test-publish.sh

# 2. Check Kafka topic has event
kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic agent-routing-decisions \
  --from-beginning \
  --max-messages 1

# 3. Verify database write
psql -h localhost -p 5436 -U postgres -d omninode_bridge -c \
  "SELECT * FROM agent_routing_decisions ORDER BY created_at DESC LIMIT 1;"

# 4. Check Redis metrics
redis-cli GET "metrics:routing:count:5min"

# 5. Verify audit logs in S3
aws s3 ls s3://agent-observability-audit/events/ --recursive | tail -10
```

---

## Monitoring & Alerting

### Kafka Metrics to Monitor

**Broker Metrics**:
- `kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec` - Ingest rate
- `kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec` - Data rate
- `kafka.controller:type=KafkaController,name=ActiveControllerCount` - Controller status
- `kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions` - Replication lag

**Topic Metrics**:
- `kafka.log:type=Log,name=Size,topic=agent-routing-decisions` - Topic size
- `kafka.server:type=BrokerTopicMetrics,name=TotalProduceRequestsPerSec,topic=*` - Producer load

**Consumer Metrics**:
- `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=*,attribute=records-lag-max` - Consumer lag
- `kafka.consumer:type=consumer-coordinator-metrics,client-id=*,attribute=commit-latency-avg` - Commit latency

### Prometheus Configuration

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'kafka-brokers'
    static_configs:
      - targets:
        - kafka1:9092
        - kafka2:9092
        - kafka3:9092
    metrics_path: /metrics

  - job_name: 'kafka-consumers'
    static_configs:
      - targets:
        - db-writer-consumer:8080
        - metrics-consumer:8080
        - alert-consumer:8080
```

### Grafana Dashboards

**Dashboard 1: Kafka Cluster Health**
- Broker status and uptime
- Topic partition distribution
- Replication factor compliance
- Disk usage and throughput

**Dashboard 2: Event Flow**
- Events per second by topic
- Producer latency (p50, p95, p99)
- Consumer lag by consumer group
- End-to-end latency tracking

**Dashboard 3: Consumer Performance**
- Events processed per consumer
- Database write batch sizes
- Error rates and retries
- Consumer offset lag

### Alerting Rules

```yaml
# alertmanager.yml
groups:
  - name: kafka-observability
    rules:
      # High consumer lag
      - alert: ConsumerLagHigh
        expr: kafka_consumer_lag > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Consumer {{ $labels.consumer_group }} has high lag"
          description: "Lag is {{ $value }} events for {{ $labels.topic }}"

      # Under-replicated partitions
      - alert: UnderReplicatedPartitions
        expr: kafka_server_replicamanager_underreplicatedpartitions > 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Kafka has under-replicated partitions"
          description: "{{ $value }} partitions are under-replicated"

      # Producer errors
      - alert: ProducerErrorRateHigh
        expr: rate(kafka_producer_errors_total[5m]) > 0.01
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High producer error rate"
          description: "Producer errors at {{ $value }}/sec"

      # Database write failures
      - alert: DatabaseWriteFailures
        expr: rate(consumer_db_write_errors_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database write failures detected"
          description: "Consumer failing to write to database"
```

---

## Migration from Direct DB

### Migration Strategy

**Phase 1: Dual-Write Period (2 weeks)**

1. Deploy Kafka infrastructure
2. Create topics
3. Deploy consumers with logging only (no DB writes yet)
4. Update skills to write to **both** Kafka and database directly
5. Compare outputs for consistency
6. Monitor for data loss or duplication

**Phase 2: Kafka-Primary (1 week)**

1. Make Kafka the primary source
2. Direct DB writes become fallback only
3. Consumers begin writing to database
4. Validate data consistency
5. Monitor consumer lag and performance

**Phase 3: Kafka-Only (Production)**

1. Remove direct DB write code from skills
2. All writes go through Kafka â†’ Consumers â†’ DB
3. Full monitoring and alerting enabled
4. Backfill historical data from database to Kafka (optional)

### Migration Script

```python
#!/usr/bin/env python3
"""
Migration script: Validate Kafka vs Direct DB writes
Purpose: Ensure data consistency during dual-write period
"""

import psycopg2
from kafka import KafkaConsumer
import json

def compare_routing_decisions():
    """Compare routing decisions from Kafka vs DB."""

    # Connect to DB
    conn = psycopg2.connect(
        host='localhost',
        port=5436,
        database='omninode_bridge',
        user='postgres',
        password='omninode-bridge-postgres-dev-2024'
    )
    cursor = conn.cursor()

    # Get recent DB entries
    cursor.execute("""
        SELECT id, selected_agent, confidence_score, created_at
        FROM agent_routing_decisions
        WHERE created_at > NOW() - INTERVAL '1 hour'
        ORDER BY created_at DESC
    """)
    db_entries = {row[0]: row for row in cursor.fetchall()}

    # Consume Kafka topic
    consumer = KafkaConsumer(
        'agent-routing-decisions',
        bootstrap_servers=['localhost:9092'],
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        auto_offset_reset='earliest',
        enable_auto_commit=False
    )

    kafka_entries = {}
    for message in consumer:
        event = message.value
        kafka_entries[event['event_id']] = event

        if len(kafka_entries) >= len(db_entries):
            break

    # Compare
    missing_in_kafka = set(db_entries.keys()) - set(kafka_entries.keys())
    missing_in_db = set(kafka_entries.keys()) - set(db_entries.keys())

    print(f"âœ… DB entries: {len(db_entries)}")
    print(f"âœ… Kafka entries: {len(kafka_entries)}")
    print(f"âš ï¸  Missing in Kafka: {len(missing_in_kafka)}")
    print(f"âš ï¸  Missing in DB: {len(missing_in_db)}")

    if missing_in_kafka:
        print("\nâŒ Events in DB but not Kafka:")
        for event_id in list(missing_in_kafka)[:10]:
            print(f"   {event_id}: {db_entries[event_id]}")

    if missing_in_db:
        print("\nâŒ Events in Kafka but not DB:")
        for event_id in list(missing_in_db)[:10]:
            print(f"   {event_id}: {kafka_entries[event_id]}")

    # Data consistency check
    common_events = set(db_entries.keys()) & set(kafka_entries.keys())
    inconsistent = 0

    for event_id in common_events:
        db_row = db_entries[event_id]
        kafka_event = kafka_entries[event_id]

        if db_row[1] != kafka_event['selected_agent']:
            print(f"âš ï¸  Data mismatch for {event_id}")
            inconsistent += 1

    print(f"\nâœ… Data consistency: {len(common_events) - inconsistent}/{len(common_events)} events match")

    consumer.close()
    cursor.close()
    conn.close()

if __name__ == '__main__':
    compare_routing_decisions()
```

---

## Troubleshooting

### Common Issues

#### Issue 1: Consumer Lag Increasing

**Symptoms**:
- Consumer offset lag > 10,000 events
- Database writes delayed by minutes

**Diagnosis**:
```bash
# Check consumer lag
kafka-consumer-groups --bootstrap-server localhost:9092 \
  --describe --group db-writer-group

# Check consumer logs
docker logs db-writer-consumer --tail 100
```

**Solutions**:
1. **Scale consumers**: Increase consumer replicas
   ```bash
   docker-compose up -d --scale db-writer-consumer=5
   ```
2. **Increase batch size**: More events per DB transaction
   ```bash
   export BATCH_SIZE=500  # Increase from 100
   ```
3. **Database indexing**: Ensure proper indexes on write tables
4. **Consumer tuning**: Increase `max_poll_records` and `fetch_max_bytes`

---

#### Issue 2: Events Not Appearing in Database

**Symptoms**:
- Kafka events published successfully
- Database remains empty

**Diagnosis**:
```bash
# Verify events in Kafka
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic agent-routing-decisions \
  --from-beginning --max-messages 10

# Check consumer is running
docker ps | grep db-writer-consumer

# Check consumer logs for errors
docker logs db-writer-consumer | grep ERROR
```

**Solutions**:
1. **Restart consumer**: May have crashed silently
   ```bash
   docker-compose restart db-writer-consumer
   ```
2. **Check database connection**: Verify credentials and network
   ```bash
   psql -h localhost -p 5436 -U postgres -d omninode_bridge -c "SELECT 1;"
   ```
3. **Reset consumer offset**: Start from beginning
   ```bash
   kafka-consumer-groups --bootstrap-server localhost:9092 \
     --group db-writer-group --reset-offsets --to-earliest \
     --topic agent-routing-decisions --execute
   ```

---

#### Issue 3: Kafka Disk Full

**Symptoms**:
- Kafka brokers crashing
- Producer errors: `NOT_ENOUGH_REPLICAS`

**Diagnosis**:
```bash
# Check disk usage
docker exec kafka1 df -h /var/lib/kafka

# Check topic sizes
kafka-log-dirs --bootstrap-server localhost:9092 --describe
```

**Solutions**:
1. **Reduce retention**: Shorten retention period
   ```bash
   kafka-configs --bootstrap-server localhost:9092 \
     --entity-type topics --entity-name agent-actions \
     --alter --add-config retention.ms=259200000  # 3 days
   ```
2. **Delete old data**: Manual cleanup
   ```bash
   kafka-delete-records --bootstrap-server localhost:9092 \
     --offset-json-file offsets-to-delete.json
   ```
3. **Expand storage**: Add disk space to Kafka brokers
4. **Enable compression**: Use snappy or lz4 compression

---

#### Issue 4: High Producer Latency

**Symptoms**:
- Skills taking >100ms to publish
- User-facing latency impact

**Diagnosis**:
```bash
# Check producer metrics
# (Exposed via skill HTTP endpoint if implemented)
curl http://skill-server:8080/metrics | grep producer_latency
```

**Solutions**:
1. **Tune producer config**: Reduce `linger_ms`, increase `batch_size`
   ```python
   producer = KafkaProducer(
       linger_ms=0,  # Send immediately
       acks=1,  # Wait for leader only (faster)
       compression_type='lz4'  # Faster compression
   )
   ```
2. **Connection pooling**: Reuse producer instances
3. **Async fire-and-forget**: Don't wait for acknowledgment in non-critical paths
   ```python
   future = producer.send(topic, value=event)
   # Don't call future.get() - just return immediately
   ```

---

## Performance Tuning

### Producer Optimization

**For Low Latency** (<5ms publish):
```python
producer = KafkaProducer(
    linger_ms=0,  # No batching
    acks=1,  # Wait for leader only
    compression_type='none',  # No compression
    buffer_memory=33554432  # 32MB buffer
)
```

**For High Throughput** (1000+ events/sec):
```python
producer = KafkaProducer(
    linger_ms=100,  # Wait for batch
    acks=1,  # Leader acknowledgment
    compression_type='snappy',  # Fast compression
    batch_size=65536,  # 64KB batches
    buffer_memory=67108864  # 64MB buffer
)
```

### Consumer Optimization

**For Low Lag** (real-time processing):
```python
consumer = KafkaConsumer(
    max_poll_records=100,  # Small batches
    max_poll_interval_ms=10000,  # Poll frequently
    session_timeout_ms=10000  # Fast rebalance
)
```

**For High Throughput** (batch processing):
```python
consumer = KafkaConsumer(
    max_poll_records=1000,  # Large batches
    fetch_min_bytes=1048576,  # Wait for 1MB
    fetch_max_wait_ms=500  # Max 500ms wait
)
```

### Database Write Optimization

```python
# Use COPY for bulk inserts (10x faster than INSERT)
from io import StringIO

def bulk_insert_routing_decisions(cursor, events):
    """Fast bulk insert using COPY."""

    # Prepare CSV data
    csv_buffer = StringIO()
    for event in events:
        row = [
            event['event_id'],
            event['user_request'],
            event['selected_agent'],
            str(event['confidence_score']),
            json.dumps(event.get('alternatives', [])),
            # ... other fields
        ]
        csv_buffer.write('\t'.join(row) + '\n')

    # Rewind buffer
    csv_buffer.seek(0)

    # COPY from buffer
    cursor.copy_from(
        csv_buffer,
        'agent_routing_decisions',
        columns=['id', 'user_request', 'selected_agent', 'confidence_score', 'alternatives', ...],
        sep='\t'
    )
```

---

## Conclusion

This Kafka integration provides a production-ready, scalable foundation for agent observability. Key benefits:

- âš¡ **<5ms latency**: Agents never block on I/O
- ðŸ”„ **Event replay**: Complete audit trail with time-travel debugging
- ðŸ“Š **Multiple consumers**: Same events power DB, analytics, alerts, ML
- ðŸš€ **Scalable**: Handles 1M+ events/sec horizontally
- ðŸ›¡ï¸ **Fault-tolerant**: Zero data loss with proper configuration

**Next Steps**:
1. Deploy Kafka cluster (3+ brokers)
2. Create topics with proper configuration
3. Update agent skills to use Kafka producers
4. Deploy consumer groups (DB writer, metrics, alerts, ML, audit)
5. Enable monitoring (Prometheus + Grafana)
6. Migrate from direct DB writes (dual-write â†’ Kafka-primary â†’ Kafka-only)

**Questions or Issues**: See troubleshooting section or contact the infrastructure team.

---

**Document Version**: 1.0.0
**Last Updated**: 2025-10-20
**Authors**: Polymorphic Agent (Polly) with Kafka Architecture Team
**Status**: Production-Ready Architecture
