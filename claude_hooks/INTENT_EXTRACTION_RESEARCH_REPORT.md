# Intent Extraction Research Report
**Structured Intent Classification for Enhanced Polymorphic Agent Selection**

**Date**: 2025-10-10
**Status**: Research Complete
**Purpose**: Enhance agent selection with structured intent extraction

---

## Executive Summary

This report evaluates structured intent extraction libraries and approaches to enhance the polymorphic agent selection system. Based on comprehensive research including benchmarks, documentation, and performance analysis, we recommend a **hybrid approach** combining **lightweight sklearn-based intent classification** (<1ms) with **Instructor for complex extraction** (when needed).

### Key Findings

1. **Sub-1ms intent classification is achievable** using embeddings + cosine similarity + sklearn
2. **Instructor** is the most mature library (3M downloads/month, 15+ provider support)
3. **Pydantic AI** is emerging as a strong agent framework (v1.0 in Sept 2025)
4. **LangExtract** is optimized for document extraction, not real-time classification
5. **Traditional ML** (sklearn) outperforms deep learning for fast, simple classification

### Recommendations

1. **Primary**: Fast sklearn-based intent classifier for agent selection (<1ms target)
2. **Secondary**: Instructor for complex structured extraction (when intent is ambiguous)
3. **Enhancement**: Add meta-trigger support for "polymorphic agent" keyword
4. **UX**: Implement agent self-announcement with emoji and colored output

---

## 1. Library Comparison Matrix

### Overview Table

| Library | Primary Use | Latency | Validation | Provider Support | Trust Score | Best For |
|---------|-------------|---------|------------|------------------|-------------|----------|
| **Instructor** | Structured extraction | 2.4-6.5s | Pydantic | 15+ (OpenAI, Anthropic, Gemini, Ollama, etc.) | 9/10 | Complex extraction, retries |
| **Pydantic AI** | Agent framework | 1-3s (estimated) | Pydantic | Multi-provider | 10/10 | Agent workflows, streaming |
| **LangExtract** | Document extraction | 2-5s | Custom | Google Gemini, OpenAI, Ollama | 8/10 | Long documents, grounding |
| **Marvin** | Natural interfaces | 2-4s | Pydantic | Multi-provider (via Pydantic AI) | 7/10 | Simple extraction tasks |
| **sklearn + embeddings** | Intent classification | <1ms | Custom | Local/API | 9/10 | **Real-time agent selection** |

### Detailed Analysis

#### **Instructor** ðŸ† Most Popular
- **Downloads**: 3M/month
- **GitHub Stars**: 11k+
- **Architecture**: Pydantic-based validation with automatic retries
- **Performance Benchmarks**:
  - NER task: 2.438s (p95 latency)
  - Reliability: 1.000 (perfect)
  - F1 Score: Competitive
- **Strengths**:
  - Mature, production-ready
  - Excellent documentation via Context7
  - Strong validation with retry mechanisms
  - Multi-provider support (15+ LLM providers)
  - Async support
- **Weaknesses**:
  - Higher latency (2-6s for LLM calls)
  - Overkill for simple classification
  - Requires API calls (cost, latency)
- **Use Case**: Complex structured extraction when lightweight classification fails

#### **Pydantic AI** ðŸ†• Emerging Leader
- **Status**: v1.0 released September 2025
- **Architecture**: Agent framework with structured outputs
- **Performance**: Not yet benchmarked extensively
- **Strengths**:
  - Built by Pydantic team (trusted source)
  - Native streaming support
  - Partial validation during streaming
  - Agent-first design
  - Validation layer for OpenAI/Google/Anthropic SDKs
- **Weaknesses**:
  - Still new (limited production usage)
  - Higher latency than traditional ML
  - Requires LLM API calls
- **Use Case**: Building agent workflows with structured outputs

#### **LangExtract** ðŸ“„ Document Specialist
- **Provider**: Google
- **Architecture**: LLM-orchestrated chunking and extraction
- **Performance**:
  - Handles docs up to 147k chars
  - Parallel processing (20 workers)
  - 2-5s latency (document-dependent)
- **Strengths**:
  - Excellent for long documents
  - Source grounding (maps to exact text)
  - Interactive HTML visualization
  - Optimized for Gemini models
- **Weaknesses**:
  - Designed for documents, not real-time classification
  - Higher latency due to chunking/synthesis
  - Overkill for prompt analysis
- **Use Case**: Extracting structured data from large documents (not suitable for agent selection)

#### **Marvin** ðŸŽ¨ Simplicity Focus
- **Architecture**: High-level functions (cast, extract, classify, generate)
- **Version**: 3.0 (ported to Pydantic AI)
- **Performance**:
  - NER F1 Score: 0.798 (best in benchmarks)
  - Reliability: 1.000
  - 2-4s latency
- **Strengths**:
  - Simple, intuitive API
  - Built on Pydantic AI (modern)
  - Good for prototyping
- **Weaknesses**:
  - Less mature than Instructor
  - Limited documentation
  - Requires LLM API calls
- **Use Case**: Rapid prototyping and simple extraction tasks

#### **sklearn + Embeddings** âš¡ Speed Champion
- **Architecture**: Traditional ML (SVM, RandomForest) + sentence embeddings
- **Performance**:
  - **<1ms classification** (demonstrated in production)
  - Local inference (no API calls)
  - O(1) dot product operations
- **Approach**:
  1. Pre-compute embeddings for each agent's training examples
  2. Average embeddings into single vector per intent/agent
  3. Use cosine similarity for classification
  4. Optional: Train sklearn classifier (SVM/RandomForest) on embeddings
- **Strengths**:
  - **Extremely fast** (<1ms target achievable)
  - No API costs
  - Local deployment
  - High reliability
  - F1 scores >70% typical
- **Weaknesses**:
  - Requires training data
  - Less flexible than LLMs
  - Fixed set of intents/agents
- **Use Case**: **Primary recommendation for agent selection**

---

## 2. Performance Analysis

### Benchmark Results Summary

**From GitHub benchmark repository** (stephenleo/llm-structured-output-benchmarks):

| Task | Library | Latency (p95) | Reliability | Notes |
|------|---------|---------------|-------------|-------|
| **Named Entity Recognition** | Instructor | 2.438s | 1.000 | Best reliability |
| | Marvin | 3.124s | 1.000 | Best F1 (0.798) |
| | OpenAI Native | 2.856s | 1.000 | Best precision (0.834) |
| | Outlines | 6.573s | 0.979 | Slowest |
| **Multi-label Classification** | Fructose | 1.285s | 1.000 | Fastest reliable |
| | LlamaIndex | 0.853s | 0.990 | Fastest overall |
| | Outlines | 7.606s | 0.985 | Slowest |
| **Synthetic Data Generation** | LlamaIndex | 1.003s | 1.000 | Best combo |
| | Instructor | 1.437s | 1.000 | Reliable |
| | Outlines | 3.577s | 0.990 | Slower |

### Intent Classification Speed Targets

**Target for Polymorphic Agent Selection**: <500ms total pipeline

Current 3-stage detection:
- **Stage 1**: Pattern detection (~1ms) âœ…
- **Stage 2**: Trigger matching (~5ms) âœ…
- **Stage 3**: AI selection (~2500ms) âŒ Too slow

**Proposed enhancement**:
- **Stage 1**: Pattern detection (~1ms) âœ…
- **Stage 2**: Trigger matching (~5ms) âœ…
- **Stage 2.5**: **Fast intent classification (~1-10ms)** âœ… NEW
- **Stage 3**: Instructor structured extraction (~2000ms) - fallback only

### Sub-1ms Intent Classification Architecture

Based on research into production systems achieving <1ms:

1. **Preprocessing** (offline):
   - Collect training examples for each agent
   - Generate embeddings using lightweight model (sentence-transformers)
   - Average embeddings per agent â†’ single vector per agent
   - Optional: Train sklearn classifier on embeddings

2. **Runtime Classification** (<1ms):
   - Embed user prompt (10-50ms for sentence-transformers)
   - Compute cosine similarity vs agent vectors (O(1) per agent)
   - Return top match with confidence score
   - Threshold: confidence >0.7 = use agent, <0.7 = fallback to AI

3. **Models for Embeddings**:
   - **sentence-transformers/all-MiniLM-L6-v2**: 384 dims, fast, good quality
   - **sentence-transformers/all-mpnet-base-v2**: 768 dims, slower, best quality
   - **OpenAI text-embedding-3-small**: 1536 dims, API-based, high quality

### Latency Breakdown by Approach

| Approach | Component | Latency | Total | Use Case |
|----------|-----------|---------|-------|----------|
| **Current (AI only)** | RTX 5090 vLLM | ~2500ms | ~2500ms | Universal fallback |
| **Proposed (sklearn)** | Embed prompt | 10-50ms | ~50ms | Primary agent selection |
| | Cosine similarity | <1ms | | |
| | Classification | <1ms | | |
| **Proposed (Instructor)** | LLM API call | 1000-3000ms | ~2000ms | Complex ambiguous cases |
| | Pydantic validation | 10-50ms | | |
| | Retry (if needed) | +1000ms | | |

---

## 3. Architectural Recommendations

### Proposed 4-Stage Detection Pipeline

```
User Prompt
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: Pattern Detection (~1ms)          â”‚
â”‚ - Check for @agent-name syntax             â”‚
â”‚ - Confidence: 1.0 if match                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ [No match]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: Trigger Matching (~5ms)           â”‚
â”‚ - Word-boundary keyword matching           â”‚
â”‚ - Confidence: 0.7-0.9 based on matches     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ [No confident match]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2.5: Intent Classification (<50ms)    â”‚ â­ NEW
â”‚ - Embed prompt with sentence-transformers  â”‚
â”‚ - Cosine similarity vs agent embeddings    â”‚
â”‚ - sklearn classifier (optional boost)      â”‚
â”‚ - Confidence: 0.0-1.0                      â”‚
â”‚ - Extract: task_type, domain, urgency      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ [Confidence <0.7 OR ambiguous]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: Structured Extraction (~2000ms)    â”‚
â”‚ - Instructor-based complex extraction      â”‚
â”‚ - RTX 5090 vLLM fallback                   â”‚
â”‚ - Full intent structure with reasoning     â”‚
â”‚ - Confidence: AI-generated                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Intent Structure (Structured Extraction)

When Stage 2.5 or Stage 3 is invoked, extract:

```python
from pydantic import BaseModel, Field
from typing import Literal, Optional

class IntentExtraction(BaseModel):
    """Structured intent for agent selection."""

    # Primary classification
    task_type: Literal[
        "testing",
        "debugging",
        "coding",
        "refactoring",
        "documentation",
        "analysis",
        "workflow",
        "parallel_execution",
        "unknown"
    ] = Field(description="Primary task category")

    # Domain classification
    domain: Literal[
        "backend",
        "frontend",
        "database",
        "api",
        "infrastructure",
        "security",
        "performance",
        "testing",
        "general",
        "unknown"
    ] = Field(description="Technical domain")

    # Urgency/priority
    urgency: Literal["low", "medium", "high", "critical"] = Field(
        default="medium",
        description="Task urgency level"
    )

    # Complexity
    complexity: Literal["simple", "moderate", "complex"] = Field(
        description="Task complexity estimation"
    )

    # Agent selection
    recommended_agent: str = Field(
        description="Recommended agent name (e.g., 'agent-testing')"
    )

    # Confidence and reasoning
    confidence: float = Field(
        ge=0.0, le=1.0,
        description="Confidence in agent selection (0.0-1.0)"
    )

    reasoning: Optional[str] = Field(
        default=None,
        description="Explanation of agent selection"
    )

    # Keywords detected
    keywords: list[str] = Field(
        default_factory=list,
        description="Key trigger words detected"
    )

    # Requires parallel execution?
    parallel_execution_needed: bool = Field(
        default=False,
        description="Whether task requires parallel agent coordination"
    )
```

### Integration Points

1. **hybrid_agent_selector.py** - Add Stage 2.5 intent classifier
2. **agent_detector.py** - Keep pattern/trigger detection pure
3. **intent_classifier.py** - NEW: Fast sklearn-based classifier
4. **instructor_extractor.py** - NEW: Instructor-based fallback
5. **user-prompt-submit.sh** - Add meta-trigger support
6. **agent_announcer.py** - NEW: Colored agent self-announcement

---

## 4. Implementation Plan

### Phase 1: Fast Intent Classifier (Primary)

**Goal**: Add Stage 2.5 with <50ms intent classification

**Files to Create**:
- `lib/intent_classifier.py` - sklearn-based fast classifier
- `lib/embedding_generator.py` - Generate/cache embeddings
- `data/agent_training_examples.yaml` - Training data per agent

**Files to Modify**:
- `lib/hybrid_agent_selector.py` - Add Stage 2.5 call
- `lib/correlation_manager.py` - Track intent classification metrics

**Implementation Steps**:

1. **Create Training Data** (1 hour)
   ```yaml
   # data/agent_training_examples.yaml
   agent-testing:
     examples:
       - "write pytest tests for the API"
       - "help me test the authentication flow"
       - "create unit tests for user service"
       - "improve test coverage in the database layer"
     domain: testing
     keywords: [test, pytest, unittest, coverage, mock]

   agent-debug:
     examples:
       - "debug this memory leak"
       - "investigate why the query is slow"
       - "troubleshoot the failing CI pipeline"
       - "analyze this IndexError"
     domain: debugging
     keywords: [debug, error, bug, issue, troubleshoot, investigate]
   ```

2. **Implement Fast Classifier** (3 hours)
   ```python
   # lib/intent_classifier.py
   from sentence_transformers import SentenceTransformer
   from sklearn.ensemble import RandomForestClassifier
   import numpy as np
   from typing import Optional, Tuple

   class FastIntentClassifier:
       """Ultra-fast intent classification using embeddings + sklearn."""

       def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
           self.embedder = SentenceTransformer(model_name)
           self.agent_embeddings = {}  # agent_name -> avg_embedding
           self.classifier = None
           self.agents = []

       def train(self, training_data: dict):
           """Train on agent examples."""
           # Pre-compute embeddings for each agent
           for agent_name, examples in training_data.items():
               embeddings = self.embedder.encode(examples)
               avg_embedding = np.mean(embeddings, axis=0)
               self.agent_embeddings[agent_name] = avg_embedding
               self.agents.append(agent_name)

           # Optional: Train sklearn classifier
           X = [emb for emb in self.agent_embeddings.values()]
           y = list(self.agent_embeddings.keys())
           self.classifier = RandomForestClassifier(n_estimators=100)
           # ... sklearn training

       def classify(
           self,
           prompt: str,
           confidence_threshold: float = 0.7
       ) -> Optional[Tuple[str, float]]:
           """Classify prompt to agent in <50ms."""
           # Embed prompt
           prompt_embedding = self.embedder.encode([prompt])[0]

           # Compute cosine similarities
           similarities = {}
           for agent_name, agent_embedding in self.agent_embeddings.items():
               similarity = np.dot(prompt_embedding, agent_embedding) / (
                   np.linalg.norm(prompt_embedding) * np.linalg.norm(agent_embedding)
               )
               similarities[agent_name] = similarity

           # Get top match
           best_agent = max(similarities, key=similarities.get)
           confidence = similarities[best_agent]

           # Threshold check
           if confidence >= confidence_threshold:
               return (best_agent, confidence)
           return None
   ```

3. **Integrate into Pipeline** (2 hours)
   ```python
   # lib/hybrid_agent_selector.py (modifications)

   from lib.intent_classifier import FastIntentClassifier

   class HybridAgentSelector:
       def __init__(self):
           self.detector = AgentDetector()
           self.intent_classifier = FastIntentClassifier()
           self.ai_selector = AIAgentSelector()

           # Load and train intent classifier
           training_data = self._load_training_data()
           self.intent_classifier.train(training_data)

       def select_agent(self, prompt: str, context: dict) -> Tuple[str, float, str]:
           """4-stage selection with intent classification."""

           # Stage 1: Pattern detection
           agent = self.detector.detect_agent(prompt)
           if agent:
               return (agent, 1.0, "pattern_match")

           # Stage 2: Trigger matching
           agent = self.detector._detect_by_triggers(prompt)
           if agent:
               return (agent, 0.85, "trigger_match")

           # Stage 2.5: Fast intent classification (NEW)
           result = self.intent_classifier.classify(prompt)
           if result:
               agent, confidence = result
               return (agent, confidence, "intent_classification")

           # Stage 3: AI selection (fallback)
           if self._should_use_ai():
               return self._ai_selection(prompt, context)

           return (None, 0.0, "no_agent")
   ```

**Performance Target**: Stage 2.5 adds 10-50ms (embeddings + classification)

### Phase 2: Instructor Fallback (Secondary)

**Goal**: Add structured extraction for ambiguous cases

**Files to Create**:
- `lib/instructor_extractor.py` - Instructor-based extraction
- `lib/intent_models.py` - Pydantic models for intent

**Implementation Steps**:

1. **Install Dependencies**
   ```bash
   pip install instructor sentence-transformers
   ```

2. **Create Pydantic Models** (30 min)
   ```python
   # lib/intent_models.py
   from pydantic import BaseModel, Field
   from typing import Literal, Optional

   class IntentExtraction(BaseModel):
       """Full intent structure for complex cases."""
       task_type: Literal["testing", "debugging", "coding", "refactoring", ...]
       domain: Literal["backend", "frontend", "database", ...]
       urgency: Literal["low", "medium", "high", "critical"]
       recommended_agent: str
       confidence: float = Field(ge=0.0, le=1.0)
       reasoning: Optional[str] = None
       parallel_execution_needed: bool = False
   ```

3. **Implement Instructor Wrapper** (2 hours)
   ```python
   # lib/instructor_extractor.py
   import instructor
   from openai import OpenAI
   from lib.intent_models import IntentExtraction

   class InstructorExtractor:
       """Structured intent extraction using Instructor."""

       def __init__(self, model: str = "gpt-4o-mini"):
           self.client = instructor.from_openai(OpenAI())
           self.model = model

       def extract_intent(
           self,
           prompt: str,
           available_agents: list[str]
       ) -> IntentExtraction:
           """Extract structured intent using LLM."""

           system_prompt = f"""
           You are an agent router. Analyze the user's prompt and determine:
           - Task type (testing, debugging, coding, etc.)
           - Technical domain
           - Which agent should handle it

           Available agents:
           {self._format_agent_list(available_agents)}
           """

           intent = self.client.chat.completions.create(
               model=self.model,
               response_model=IntentExtraction,
               messages=[
                   {"role": "system", "content": system_prompt},
                   {"role": "user", "content": prompt}
               ],
               max_retries=2
           )

           return intent
   ```

**Performance Target**: 1-3s for Instructor extraction (fallback only)

### Phase 3: Meta-Trigger Support

**Goal**: Recognize "polymorphic agent" keyword and auto-select agent

**Files to Modify**:
- `hooks/user-prompt-submit.sh` - Detect meta-trigger
- `lib/agent_detector.py` - Add meta-trigger pattern

**Implementation**:

```python
# lib/agent_detector.py additions

META_TRIGGER_PATTERNS = [
    r"polymorphic\s+agent",
    r"use\s+polymorphic\s+agent",
    r"select\s+agent",
    r"choose\s+agent",
    r"which\s+agent"
]

def detect_meta_trigger(self, prompt: str) -> bool:
    """Detect if user wants agent auto-selection."""
    prompt_lower = prompt.lower()
    for pattern in self.META_TRIGGER_PATTERNS:
        if re.search(pattern, prompt_lower):
            return True
    return False
```

```bash
# hooks/user-prompt-submit.sh additions

# Check for meta-trigger
if echo "$PROMPT" | grep -qiE "polymorphic\s+agent|select\s+agent"; then
    # Force intent classification
    FORCE_INTENT_CLASSIFICATION=true
    log_event "Meta-trigger detected - forcing intent classification"
fi
```

### Phase 4: Agent Self-Announcement

**Goal**: Visual indicator when agent is invoked (emoji + color)

**Files to Create**:
- `lib/agent_announcer.py` - Format agent announcements

**Files to Modify**:
- `hooks/user-prompt-submit.sh` - Add colored output

**Implementation**:

```python
# lib/agent_announcer.py

from typing import Dict
import sys

AGENT_EMOJIS = {
    "agent-testing": "ðŸ§ª",
    "agent-debug": "ðŸ›",
    "agent-code-generator": "âš¡",
    "agent-workflow-coordinator": "ðŸŽ¯",
    "agent-parallel-dispatcher": "âš™ï¸",
    "agent-debug-intelligence": "ðŸ”",
}

# ANSI color codes
COLORS = {
    "agent-testing": "\033[96m",      # Cyan
    "agent-debug": "\033[91m",         # Light red
    "agent-code-generator": "\033[93m", # Yellow
    "agent-workflow-coordinator": "\033[95m", # Magenta
    "agent-parallel-dispatcher": "\033[92m",  # Light green
    "agent-debug-intelligence": "\033[94m",   # Light blue
}
RESET = "\033[0m"

def announce_agent(
    agent_name: str,
    confidence: float,
    method: str,
    file=sys.stderr
) -> None:
    """Print colored agent announcement with emoji."""
    emoji = AGENT_EMOJIS.get(agent_name, "ðŸ¤–")
    color = COLORS.get(agent_name, "\033[97m")  # Default: white

    announcement = (
        f"{color}{emoji} Agent Activated: {agent_name}{RESET}\n"
        f"{color}â”œâ”€ Confidence: {confidence:.2f}{RESET}\n"
        f"{color}â”œâ”€ Method: {method}{RESET}\n"
        f"{color}â””â”€ Ready to assist!{RESET}\n"
    )

    print(announcement, file=file, flush=True)
```

```bash
# hooks/user-prompt-submit.sh additions

# If agent detected, announce it
if [ -n "$AGENT_DETECTED" ]; then
    python3 -c "
from lib.agent_announcer import announce_agent
announce_agent('$AGENT_DETECTED', $CONFIDENCE, '$METHOD')
"
fi
```

**Example Output**:
```
ðŸ§ª Agent Activated: agent-testing
â”œâ”€ Confidence: 0.95
â”œâ”€ Method: intent_classification
â””â”€ Ready to assist!
```

---

## 5. Performance Validation Plan

### Benchmarks to Run

1. **Stage 2.5 Latency** (Target: <50ms)
   - Measure embedding generation time
   - Measure classification time
   - Test with various prompt lengths

2. **Classification Accuracy** (Target: >85%)
   - Create test dataset (100+ prompts)
   - Compare Stage 2.5 vs human labels
   - Compare Stage 2.5 vs AI selection (Stage 3)

3. **End-to-End Pipeline** (Target: <500ms without AI)
   - Stage 1: <2ms
   - Stage 2: <10ms
   - Stage 2.5: <50ms
   - Stage 3 (fallback): ~2000ms

### Test Scenarios

```yaml
test_prompts:
  - prompt: "write pytest tests for the authentication module"
    expected_agent: "agent-testing"
    expected_method: "trigger_match"  # or "intent_classification"

  - prompt: "investigate this memory leak in the cache service"
    expected_agent: "agent-debug"
    expected_method: "intent_classification"

  - prompt: "create a REST API for user management"
    expected_agent: "agent-code-generator"
    expected_method: "intent_classification"

  - prompt: "polymorphic agent help me fix this bug"
    expected_agent: "agent-debug"
    expected_method: "meta_trigger + intent_classification"
```

---

## 6. Alternative Approaches Considered

### Why Not LangExtract?

**Pros**:
- Google-backed, well-documented
- Excellent for document extraction

**Cons**:
- âŒ Designed for long documents, not prompts
- âŒ Higher latency (2-5s for chunking/synthesis)
- âŒ Overkill for simple agent selection
- âŒ Requires Gemini API calls

**Verdict**: Not suitable for real-time agent selection

### Why Not Pure Instructor?

**Pros**:
- Mature, production-ready
- Excellent validation and retries

**Cons**:
- âŒ 2-6s latency for every prompt (too slow)
- âŒ API costs on every invocation
- âŒ Overkill for simple classification

**Verdict**: Use as fallback only (Stage 3)

### Why Not Pydantic AI Agents?

**Pros**:
- Modern, agent-first design
- Streaming support

**Cons**:
- âŒ Still new (v1.0 in Sept 2025)
- âŒ Higher latency than traditional ML
- âŒ Requires LLM API calls

**Verdict**: Consider for future agent framework upgrades, not for fast classification

### Why sklearn + Embeddings? âœ…

**Pros**:
- âœ… **Sub-1ms classification demonstrated in production**
- âœ… No API costs
- âœ… Local inference
- âœ… Highly reliable (F1 >70%)
- âœ… Fast enough to not impact UX

**Cons**:
- Requires training data (we have agent definitions)
- Less flexible than LLMs (acceptable tradeoff)

**Verdict**: **Best fit for Stage 2.5 fast intent classification**

---

## 7. Migration Strategy

### Rollout Plan

**Week 1: Foundation**
- Create training data from agent definitions
- Implement `FastIntentClassifier`
- Unit tests for classification

**Week 2: Integration**
- Integrate Stage 2.5 into `HybridAgentSelector`
- Add performance monitoring
- Benchmark latency and accuracy

**Week 3: Enhancement**
- Implement meta-trigger support
- Add agent self-announcement
- Create user documentation

**Week 4: Validation**
- Run end-to-end tests
- Collect user feedback
- Tune confidence thresholds

**Week 5: Optional Instructor Fallback**
- Implement Instructor extraction
- Connect to Stage 3 fallback
- Final benchmarks

### Backward Compatibility

- Existing 3-stage pipeline remains functional
- Stage 2.5 is inserted, not replacing anything
- All existing agent definitions work unchanged
- Environment variables control feature flags:
  - `ENABLE_INTENT_CLASSIFICATION=true` (default)
  - `ENABLE_INSTRUCTOR_FALLBACK=false` (optional)
  - `ENABLE_META_TRIGGERS=true` (default)
  - `ENABLE_AGENT_ANNOUNCEMENTS=true` (default)

---

## 8. Cost Analysis

### Current System (AI-Only)

- **RTX 5090 vLLM**: Free (local)
- **Latency**: ~2.5s per prompt
- **Cost**: $0

### Proposed System (Hybrid)

**Stage 2.5 (sklearn)**:
- **Model**: sentence-transformers (local)
- **Latency**: 10-50ms
- **Cost**: $0

**Stage 3 Fallback (Instructor + OpenAI)**:
- **Model**: gpt-4o-mini
- **Latency**: 1-3s
- **Cost**: ~$0.0001 per intent extraction (rarely needed)
- **Frequency**: 5-10% of prompts (when intent is ambiguous)

**Monthly Cost Estimate**:
- 1000 prompts/day
- 95% handled by Stage 1/2/2.5 (free)
- 5% fallback to Instructor (50 prompts/day)
- 50 * 30 * $0.0001 = **$0.15/month**

**Verdict**: Negligible cost, massive speed improvement

---

## 9. Risk Assessment

### Technical Risks

**Risk 1: sklearn Classification Accuracy**
- **Likelihood**: Medium
- **Impact**: High
- **Mitigation**: Benchmark against human labels, tune thresholds, keep AI fallback

**Risk 2: Embedding Model Latency**
- **Likelihood**: Low
- **Impact**: Medium
- **Mitigation**: Use lightweight model (all-MiniLM-L6-v2), cache embeddings

**Risk 3: Training Data Quality**
- **Likelihood**: Medium
- **Impact**: Medium
- **Mitigation**: Use existing agent definitions + trigger words as training data

**Risk 4: False Positives**
- **Likelihood**: Medium
- **Impact**: Low
- **Mitigation**: Confidence thresholds, monitor metrics, allow user override

### Operational Risks

**Risk 1: Increased Complexity**
- **Likelihood**: High
- **Impact**: Medium
- **Mitigation**: Comprehensive tests, clear documentation, feature flags

**Risk 2: Maintenance Burden**
- **Likelihood**: Medium
- **Impact**: Medium
- **Mitigation**: Automated retraining, monitoring, fallback to existing system

---

## 10. Success Metrics

### KPIs to Track

1. **Agent Selection Latency** (Target: <100ms avg)
   - Stage 1 pattern: <2ms
   - Stage 2 trigger: <10ms
   - Stage 2.5 intent: <50ms
   - Stage 3 AI: <3000ms (fallback only)

2. **Classification Accuracy** (Target: >90%)
   - Compare predicted agent vs actual agent used
   - Compare Stage 2.5 vs AI selection

3. **User Experience**
   - Time to first agent activation: <100ms
   - Agent announcement visibility: 100% (if enabled)
   - Meta-trigger success rate: >95%

4. **Cost Efficiency**
   - API calls reduced: >90% (Stage 2.5 vs pure AI)
   - Monthly cost: <$1

5. **Reliability**
   - Agent selection availability: 99.9%
   - Fallback activation rate: <10%

---

## 11. Recommended Next Actions

### Immediate (This Week)
1. âœ… **Review this research report** with team
2. â­ **Approve hybrid approach** (sklearn + Instructor fallback)
3. ðŸŽ¯ **Create agent training data** (data/agent_training_examples.yaml)
4. âš¡ **Implement FastIntentClassifier** prototype
5. ðŸ§ª **Run initial benchmarks** on latency and accuracy

### Short-Term (Next 2 Weeks)
6. ðŸ”Œ **Integrate Stage 2.5** into HybridAgentSelector
7. ðŸŽ¨ **Implement agent announcements** with emoji and color
8. ðŸ·ï¸ **Add meta-trigger support** for "polymorphic agent" keyword
9. ðŸ“Š **Set up monitoring** for performance metrics
10. ðŸ“ **Update user documentation** with new features

### Long-Term (Next Month)
11. ðŸ” **Optional: Add Instructor fallback** for complex cases
12. ðŸ§¬ **Optimize embeddings** (model selection, caching strategies)
13. ðŸ“ˆ **Collect user feedback** and iterate
14. ðŸŽ“ **Train team** on new intent classification system

---

## 12. Conclusion

Based on comprehensive research into structured intent extraction libraries and performance benchmarks, we recommend a **hybrid approach** that combines:

1. **Fast sklearn-based intent classification** (<50ms) as primary Stage 2.5
2. **Instructor-based structured extraction** (1-3s) as optional Stage 3 fallback
3. **Meta-trigger support** for "polymorphic agent" keyword
4. **Agent self-announcements** with emoji and colored output

This approach delivers:
- âœ… **10-50x speed improvement** over pure AI selection
- âœ… **>90% classification accuracy** (target)
- âœ… **<$1/month cost** (negligible)
- âœ… **Zero breaking changes** to existing system
- âœ… **Enhanced UX** with visual agent feedback

### Final Recommendation

**Proceed with hybrid sklearn + Instructor approach, starting with Phase 1 (Fast Intent Classifier) this week.**

---

**Report Author**: Claude Code (Agent-Research)
**Research Date**: 2025-10-10
**Libraries Evaluated**: Instructor, Pydantic AI, LangExtract, Marvin, sklearn, sentence-transformers
**Performance Benchmarks**: stephenleo/llm-structured-output-benchmarks, production case studies
**Documentation Sources**: Context7, GitHub, Medium, technical blogs

---

**Appendix: Key Resources**

- [Instructor GitHub](https://github.com/instructor-ai/instructor)
- [Pydantic AI Documentation](https://ai.pydantic.dev/)
- [LangExtract GitHub](https://github.com/google/langextract)
- [LLM Structured Output Benchmarks](https://github.com/stephenleo/llm-structured-output-benchmarks)
- [Fast Intent Classification Article](https://medium.com/@durgeshrathod.777/intent-classification-in-1ms-how-we-built-a-lightning-fast-classifier-with-embeddings-db76bfb6d964)
- [spaCy NLP Documentation](https://spacy.io/)
- [sentence-transformers](https://www.sbert.net/)

**Status**: âœ… **RESEARCH COMPLETE - READY FOR IMPLEMENTATION**
