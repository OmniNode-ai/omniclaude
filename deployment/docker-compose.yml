# OmniClaude Consolidated Docker Compose Stack
# ==============================================================================
# Single compose file with environment-specific configuration via .env files
#
# Usage:
#   Development: docker-compose --env-file .env.dev up -d
#   Testing:     docker-compose --env-file .env.test up -d --profile test
#   Production:  docker-compose --env-file .env.prod up -d
#
# See deployment/README.md for detailed instructions
# ==============================================================================
#
# PostgreSQL Environment Variable Convention:
# -------------------------------------------
# This compose file uses TWO sets of PostgreSQL variables with distinct purposes:
#
# 1. APP_POSTGRES_* â†’ Local application database (containerized PostgreSQL)
#    - Used by: app, postgres services
#    - Database: omniclaude (local, containerized)
#    - Example: APP_POSTGRES_USER=omniclaude, APP_POSTGRES_PASSWORD=<set_in_env>
#
# 2. POSTGRES_* â†’ Shared bridge database (remote infrastructure)
#    - Used by: agent-observability-consumer, routing-adapter, archon-router-consumer, test-runner
#    - Database: omninode_bridge (shared, 192.168.86.200:5436)
#    - Example: POSTGRES_USER=postgres, POSTGRES_PASSWORD=<set_in_env>
#
# WHY TWO SETS?
# - Local app database (APP_POSTGRES_*): Application-specific data, runs in Docker
# - Shared bridge database (POSTGRES_*): Cross-service observability, agent routing,
#   manifest injections, and all event-driven intelligence tracking
#
# See deployment/README.md for detailed configuration guide.
# ==============================================================================

# Project name - ensures containers show as "omniclaude" instead of "deployment"
name: omniclaude

services:
  # ============================================================================
  # Application Service - OmniClaude
  # ============================================================================
  app:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
        VERSION: ${VERSION:-0.1.0}
    image: omniclaude:${VERSION:-latest}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_app
    restart: ${RESTART_POLICY:-unless-stopped}
    ports:
      - "${APP_PORT:-8000}:8000"
      - "${APP_METRICS_PORT:-8001}:8001"
    environment:
      # Database configuration - LOCAL APP DATABASE
      # Uses APP_POSTGRES_* variables for local containerized PostgreSQL
      - DATABASE_URL=postgresql://${APP_POSTGRES_USER:-omniclaude}:${APP_POSTGRES_PASSWORD:?APP_POSTGRES_PASSWORD must be set}@${APP_POSTGRES_HOST:-postgres}:${APP_POSTGRES_INTERNAL_PORT:-5432}/${APP_POSTGRES_DATABASE:-omniclaude}
      - POSTGRES_HOST=${APP_POSTGRES_HOST:-postgres}
      - POSTGRES_PORT=${APP_POSTGRES_INTERNAL_PORT:-5432}
      - POSTGRES_USER=${APP_POSTGRES_USER:-omniclaude}
      - POSTGRES_PASSWORD=${APP_POSTGRES_PASSWORD:?APP_POSTGRES_PASSWORD must be set}
      - POSTGRES_DATABASE=${APP_POSTGRES_DATABASE:-omniclaude}

      # Valkey configuration (Redis-compatible)
      - REDIS_URL=redis://${VALKEY_HOST:-valkey}:${VALKEY_INTERNAL_PORT:-6379}/${VALKEY_DB:-0}

      # Application configuration
      - APP_ENV=${ENVIRONMENT:-production}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - SECRET_KEY=${SECRET_KEY:?SECRET_KEY must be set}

      # OpenTelemetry configuration
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://${OTEL_COLLECTOR_HOST:-otel-collector}:${OTEL_COLLECTOR_PORT:-4317}
      - OTEL_SERVICE_NAME=${OTEL_SERVICE_NAME:-omniclaude}
      - OTEL_TRACES_EXPORTER=${OTEL_TRACES_EXPORTER:-otlp}
      - OTEL_METRICS_EXPORTER=${OTEL_METRICS_EXPORTER:-otlp}
      - OTEL_LOGS_EXPORTER=${OTEL_LOGS_EXPORTER:-otlp}

      # Prometheus metrics
      - PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus
    volumes:
      - app_data:/app/data
      - ./logs:/app/logs
    networks:
      - app_network
      - monitoring_network
    depends_on:
      postgres:
        condition: service_healthy
      valkey:
        condition: service_healthy
      otel-collector:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '${APP_CPU_LIMIT:-2.0}'
          memory: ${APP_MEMORY_LIMIT:-2G}
        reservations:
          cpus: '${APP_CPU_RESERVATION:-0.5}'
          memory: ${APP_MEMORY_RESERVATION:-512M}
    profiles:
      - ${PROFILE_APP:-default}

  # ============================================================================
  # Agent Observability Consumer Service
  # ============================================================================
  agent-observability-consumer:
    build:
      context: ..
      dockerfile: deployment/Dockerfile.consumer
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
        VERSION: ${VERSION:-0.1.0}
    image: omniclaude-agent-consumer:${VERSION:-latest}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_agent_consumer
    restart: ${RESTART_POLICY:-unless-stopped}
    environment:
      # Kafka configuration
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS:?KAFKA_BOOTSTRAP_SERVERS must be set}
      - KAFKA_GROUP_ID=${KAFKA_CONSUMER_GROUP_ID:-agent-observability-postgres}

      # PostgreSQL configuration - SHARED BRIDGE DATABASE
      # Uses POSTGRES_* variables for remote omninode_bridge database (192.168.86.200:5436)
      - POSTGRES_HOST=${POSTGRES_HOST:?POSTGRES_HOST must be set}
      - POSTGRES_PORT=${POSTGRES_PORT:?POSTGRES_PORT must be set}
      - POSTGRES_DATABASE=${POSTGRES_DATABASE:?POSTGRES_DATABASE must be set}
      - POSTGRES_USER=${POSTGRES_USER:?POSTGRES_USER must be set}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:?POSTGRES_PASSWORD must be set}

      # Batch processing configuration
      - BATCH_SIZE=${CONSUMER_BATCH_SIZE:-100}
      - BATCH_TIMEOUT_MS=${CONSUMER_BATCH_TIMEOUT_MS:-1000}

      # Health check and logging
      - HEALTH_CHECK_PORT=${CONSUMER_HEALTH_PORT:-8080}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    extra_hosts:
      # DNS resolution for remote shared infrastructure
      - "omninode-bridge-redpanda:${REMOTE_INFRASTRUCTURE_IP:?REMOTE_INFRASTRUCTURE_IP must be set}"
      - "omninode-bridge-postgres:${REMOTE_INFRASTRUCTURE_IP:?REMOTE_INFRASTRUCTURE_IP must be set}"
      - "omninode-bridge-consul:${REMOTE_INFRASTRUCTURE_IP:?REMOTE_INFRASTRUCTURE_IP must be set}"
    networks:
      - app_network
      - omninode-bridge-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${CONSUMER_HEALTH_PORT:-8080}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '${CONSUMER_CPU_LIMIT:-0.5}'
          memory: ${CONSUMER_MEMORY_LIMIT:-512M}
        reservations:
          cpus: '${CONSUMER_CPU_RESERVATION:-0.1}'
          memory: ${CONSUMER_MEMORY_RESERVATION:-128M}
    profiles:
      - ${PROFILE_CONSUMER:-default}

  # ============================================================================
  # Routing Adapter Service (Event-Driven Agent Routing)
  # ============================================================================
  routing-adapter:
    build:
      context: ..
      dockerfile: deployment/Dockerfile.routing-adapter
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
        VERSION: ${VERSION:-0.1.0}
    image: omniclaude-routing-adapter:${VERSION:-latest}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_routing_adapter
    restart: ${RESTART_POLICY:-unless-stopped}
    ports:
      - "${ROUTING_ADAPTER_PORT:-8070}:8070"
    environment:
      # Kafka configuration
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS:?KAFKA_BOOTSTRAP_SERVERS must be set}
      - KAFKA_GROUP_ID=${KAFKA_ROUTING_GROUP_ID:-agent-routing-service}
      - KAFKA_REQUEST_TOPIC=${KAFKA_ROUTING_REQUEST_TOPIC:-agent.routing.requested.v1}
      - KAFKA_COMPLETED_TOPIC=${KAFKA_ROUTING_COMPLETED_TOPIC:-agent.routing.completed.v1}
      - KAFKA_FAILED_TOPIC=${KAFKA_ROUTING_FAILED_TOPIC:-agent.routing.failed.v1}

      # PostgreSQL configuration - SHARED BRIDGE DATABASE
      # Uses POSTGRES_* variables for remote omninode_bridge database (192.168.86.200:5436)
      - POSTGRES_HOST=${POSTGRES_HOST:?POSTGRES_HOST must be set}
      - POSTGRES_PORT=${POSTGRES_PORT:?POSTGRES_PORT must be set}
      - POSTGRES_DATABASE=${POSTGRES_DATABASE:?POSTGRES_DATABASE must be set}
      - POSTGRES_USER=${POSTGRES_USER:?POSTGRES_USER must be set}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:?POSTGRES_PASSWORD must be set}

      # Agent registry configuration
      - AGENT_REGISTRY_PATH=${AGENT_REGISTRY_PATH:-/agent-definitions/agent-registry.yaml}
      - AGENT_DEFINITIONS_PATH=${AGENT_DEFINITIONS_PATH:-/agent-definitions}

      # Service configuration
      - ROUTING_ADAPTER_PORT=8070
      - HEALTH_CHECK_PORT=8070
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - ROUTING_TIMEOUT_MS=${ROUTING_TIMEOUT_MS:-5000}
      - CACHE_TTL_SECONDS=${ROUTING_CACHE_TTL:-3600}
      - MAX_RECOMMENDATIONS=${MAX_RECOMMENDATIONS:-5}
    volumes:
      - ${AGENT_DEFINITIONS_HOST_PATH:-$HOME/.claude/agent-definitions}:/agent-definitions:ro
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8070/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '${ROUTING_ADAPTER_CPU_LIMIT:-0.5}'
          memory: ${ROUTING_ADAPTER_MEMORY_LIMIT:-512M}
        reservations:
          cpus: '${ROUTING_ADAPTER_CPU_RESERVATION:-0.1}'
          memory: ${ROUTING_ADAPTER_MEMORY_RESERVATION:-128M}
    profiles:
      - ${PROFILE_ROUTING_ADAPTER:-default}

  # ============================================================================
  # Agent Router Event Consumer (Kafka-based Agent Routing)
  # ============================================================================
  archon-router-consumer:
    build:
      context: ..
      dockerfile: agents/services/Dockerfile.router-consumer
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
        VERSION: ${VERSION:-0.2.0}
    image: omniclaude-archon-router-consumer:${VERSION:-latest}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_archon_router_consumer
    restart: ${RESTART_POLICY:-unless-stopped}
    environment:
      # Kafka configuration
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS:?KAFKA_BOOTSTRAP_SERVERS must be set}
      - KAFKA_GROUP_ID=${KAFKA_ROUTER_GROUP_ID:-agent-router-service}

      # PostgreSQL configuration - SHARED BRIDGE DATABASE
      # Uses POSTGRES_* variables for remote omninode_bridge database (192.168.86.200:5436)
      - POSTGRES_HOST=${POSTGRES_HOST:?POSTGRES_HOST must be set}
      - POSTGRES_PORT=${POSTGRES_PORT:?POSTGRES_PORT must be set}
      - POSTGRES_DATABASE=${POSTGRES_DATABASE:?POSTGRES_DATABASE must be set}
      - POSTGRES_USER=${POSTGRES_USER:?POSTGRES_USER must be set}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:?POSTGRES_PASSWORD must be set}

      # Agent registry configuration
      - REGISTRY_PATH=${AGENT_REGISTRY_PATH:-/agent-definitions/agent-registry.yaml}
      - AGENT_DEFINITIONS_PATH=/agent-definitions

      # Health check configuration
      - HEALTH_CHECK_PORT=${ARCHON_ROUTER_HEALTH_PORT:-8070}

      # Service configuration
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    extra_hosts:
      # DNS resolution for remote shared infrastructure
      - "omninode-bridge-redpanda:${REMOTE_INFRASTRUCTURE_IP:?REMOTE_INFRASTRUCTURE_IP must be set}"
      - "omninode-bridge-postgres:${REMOTE_INFRASTRUCTURE_IP:?REMOTE_INFRASTRUCTURE_IP must be set}"
      - "omninode-bridge-consul:${REMOTE_INFRASTRUCTURE_IP:?REMOTE_INFRASTRUCTURE_IP must be set}"
    volumes:
      - ${AGENT_DEFINITIONS_HOST_PATH:-$HOME/.claude/agent-definitions}:/agent-definitions:ro
    networks:
      - app_network
      - omninode-bridge-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${ARCHON_ROUTER_HEALTH_PORT:-8070}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '${ARCHON_ROUTER_CPU_LIMIT:-0.5}'
          memory: ${ARCHON_ROUTER_MEMORY_LIMIT:-512M}
        reservations:
          cpus: '${ARCHON_ROUTER_CPU_RESERVATION:-0.1}'
          memory: ${ARCHON_ROUTER_MEMORY_RESERVATION:-128M}
    profiles:
      - ${PROFILE_ARCHON_ROUTER:-default}

  # ============================================================================
  # PostgreSQL Database Service (App-specific)
  # ============================================================================
  postgres:
    image: postgres:${POSTGRES_VERSION:-16-alpine}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_postgres
    restart: ${RESTART_POLICY:-unless-stopped}
    ports:
      - "${POSTGRES_EXTERNAL_PORT:-5432}:5432"
    environment:
      # Local App Database Configuration
      # Uses APP_POSTGRES_* variables for local containerized PostgreSQL
      - POSTGRES_USER=${APP_POSTGRES_USER:-omniclaude}
      - POSTGRES_PASSWORD=${APP_POSTGRES_PASSWORD:?APP_POSTGRES_PASSWORD must be set}
      - POSTGRES_DB=${APP_POSTGRES_DATABASE:-omniclaude}
      - POSTGRES_INITDB_ARGS=${POSTGRES_INITDB_ARGS:---encoding=UTF-8 --lc-collate=C --lc-ctype=C}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ${POSTGRES_INIT_SCRIPT:-../scripts/init-db.sh}:/docker-entrypoint-initdb.d/init-db.sh:ro
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${APP_POSTGRES_USER:-omniclaude} -d ${APP_POSTGRES_DATABASE:-omniclaude}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '${POSTGRES_CPU_LIMIT:-1.0}'
          memory: ${POSTGRES_MEMORY_LIMIT:-1G}
        reservations:
          cpus: '${POSTGRES_CPU_RESERVATION:-0.25}'
          memory: ${POSTGRES_MEMORY_RESERVATION:-256M}
    profiles:
      - ${PROFILE_POSTGRES:-default}

  # ============================================================================
  # Redpanda (Kafka-compatible broker) - Test Environment Only
  # ============================================================================
  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:${REDPANDA_VERSION:-v23.3.3}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_redpanda
    command:
      - redpanda
      - start
      - --kafka-addr
      - internal://0.0.0.0:9092,external://0.0.0.0:29092
      - --advertise-kafka-addr
      - internal://redpanda:9092,external://localhost:29092
      - --pandaproxy-addr
      - internal://0.0.0.0:8082,external://0.0.0.0:28082
      - --advertise-pandaproxy-addr
      - internal://redpanda:8082,external://localhost:28082
      - --schema-registry-addr
      - internal://0.0.0.0:8081,external://0.0.0.0:28081
      - --rpc-addr
      - redpanda:33145
      - --advertise-rpc-addr
      - redpanda:33145
      - --mode
      - dev-container
      - --smp
      - '1'
      - --default-log-level=${REDPANDA_LOG_LEVEL:-info}
    ports:
      - "${REDPANDA_KAFKA_PORT:-29092}:29092"
      - "${REDPANDA_PROXY_PORT:-28082}:28082"
      - "${REDPANDA_SCHEMA_PORT:-28081}:28081"
      - "${REDPANDA_ADMIN_PORT:-9644}:9644"
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "rpk cluster health | grep -q 'Healthy:.*true'"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    profiles:
      - test

  # ============================================================================
  # Redpanda Console (Kafka UI) - Test/Debug Only
  # ============================================================================
  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:${REDPANDA_CONSOLE_VERSION:-v2.3.8}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_console
    entrypoint: /bin/sh
    command: -c 'echo "$$CONSOLE_CONFIG_FILE" > /tmp/config.yml; /app/console'
    environment:
      CONFIG_FILEPATH: /tmp/config.yml
      CONSOLE_CONFIG_FILE: |
        kafka:
          brokers: ["${KAFKA_CONSOLE_BROKER:-redpanda:9092}"]
          schemaRegistry:
            enabled: true
            urls: ["http://${KAFKA_CONSOLE_SCHEMA_HOST:-redpanda}:8081"]
        redpanda:
          adminApi:
            enabled: true
            urls: ["http://${KAFKA_CONSOLE_ADMIN_HOST:-redpanda}:9644"]
    ports:
      - "${REDPANDA_CONSOLE_PORT:-8080}:8080"
    networks:
      - app_network
    depends_on:
      - redpanda
    profiles:
      - test
      - debug

  # ============================================================================
  # Test Runner (runs pytest in isolated container) - Test Only
  # ============================================================================
  test-runner:
    build:
      context: ..
      dockerfile: deployment/Dockerfile.test-runner
    container_name: ${CONTAINER_PREFIX:-omniclaude}_test_runner
    environment:
      - KAFKA_BROKERS=${KAFKA_BOOTSTRAP_SERVERS:-redpanda:9092}
      # PostgreSQL configuration - SHARED BRIDGE DATABASE
      # Uses POSTGRES_* variables for testing against shared database
      - POSTGRES_HOST=${POSTGRES_HOST:-postgres}
      - POSTGRES_PORT=${POSTGRES_INTERNAL_PORT:-5432}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:?POSTGRES_PASSWORD must be set}
      - POSTGRES_DATABASE=${POSTGRES_DATABASE:-omninode_bridge}
      - PYTHONUNBUFFERED=1
      - DEBUG=${DEBUG:-false}
    depends_on:
      redpanda:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - app_network
    volumes:
      - ../tests:/app/tests:ro
      - ../agents:/app/agents:ro
      - ../skills:/app/skills:ro
      - ./test-results:/app/test-results
    command: >
      sh -c "
        echo 'ðŸ§ª Waiting for services to be ready...' &&
        sleep 5 &&
        echo 'ðŸ§ª Running unit tests...' &&
        pytest tests/test_kafka_logging.py -v --tb=short --junitxml=/app/test-results/unit.xml &&
        echo 'ðŸ§ª Running integration tests...' &&
        pytest tests/test_kafka_consumer.py -v --tb=short --junitxml=/app/test-results/integration.xml &&
        echo 'ðŸ§ª Running e2e tests...' &&
        pytest tests/test_e2e_agent_logging.py -v --tb=short --junitxml=/app/test-results/e2e.xml &&
        echo 'ðŸ§ª Running performance tests...' &&
        pytest tests/test_logging_performance.py -v --tb=short -m performance --junitxml=/app/test-results/performance.xml &&
        echo 'âœ… All tests completed!'
      "
    profiles:
      - test

  # ============================================================================
  # Valkey Cache Service (Redis-compatible)
  # ============================================================================
  valkey:
    image: valkey/valkey:${VALKEY_VERSION:-7-alpine}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_valkey
    restart: ${RESTART_POLICY:-unless-stopped}
    ports:
      - "${VALKEY_PORT:-6379}:6379"
    command: >
      valkey-server
      --maxmemory ${VALKEY_MAXMEMORY:-512mb}
      --maxmemory-policy ${VALKEY_MAXMEMORY_POLICY:-allkeys-lru}
      --appendonly yes
      --appendfsync ${VALKEY_APPENDFSYNC:-everysec}
    volumes:
      - valkey_data:/data
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '${VALKEY_CPU_LIMIT:-0.5}'
          memory: ${VALKEY_MEMORY_LIMIT:-512M}
        reservations:
          cpus: '${VALKEY_CPU_RESERVATION:-0.1}'
          memory: ${VALKEY_MEMORY_RESERVATION:-128M}
    profiles:
      - ${PROFILE_VALKEY:-default}

  # ============================================================================
  # Prometheus Monitoring Service
  # ============================================================================
  prometheus:
    image: prom/prometheus:${PROMETHEUS_VERSION:-v2.48.0}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_prometheus
    restart: ${RESTART_POLICY:-unless-stopped}
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION:-30d}'
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    networks:
      - monitoring_network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '${PROMETHEUS_CPU_LIMIT:-1.0}'
          memory: ${PROMETHEUS_MEMORY_LIMIT:-1G}
        reservations:
          cpus: '${PROMETHEUS_CPU_RESERVATION:-0.25}'
          memory: ${PROMETHEUS_MEMORY_RESERVATION:-256M}
    profiles:
      - ${PROFILE_MONITORING:-default}

  # ============================================================================
  # Grafana Visualization Service
  # ============================================================================
  grafana:
    image: grafana/grafana:${GRAFANA_VERSION:-10.2.0}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_grafana
    restart: ${RESTART_POLICY:-unless-stopped}
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:?GRAFANA_ADMIN_PASSWORD must be set}
      - GF_INSTALL_PLUGINS=${GRAFANA_PLUGINS:-grafana-clock-panel,grafana-simple-json-datasource}
      - GF_SERVER_ROOT_URL=${GRAFANA_ROOT_URL:-http://localhost:3000}
      - GF_ANALYTICS_REPORTING_ENABLED=${GRAFANA_ANALYTICS:-false}
      - GF_ANALYTICS_CHECK_FOR_UPDATES=${GRAFANA_CHECK_UPDATES:-false}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - monitoring_network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '${GRAFANA_CPU_LIMIT:-1.0}'
          memory: ${GRAFANA_MEMORY_LIMIT:-512M}
        reservations:
          cpus: '${GRAFANA_CPU_RESERVATION:-0.25}'
          memory: ${GRAFANA_MEMORY_RESERVATION:-128M}
    profiles:
      - ${PROFILE_MONITORING:-default}

  # ============================================================================
  # OpenTelemetry Collector Service
  # ============================================================================
  otel-collector:
    image: otel/opentelemetry-collector-contrib:${OTEL_VERSION:-0.91.0}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_otel_collector
    restart: ${RESTART_POLICY:-unless-stopped}
    ports:
      - "${OTEL_GRPC_PORT:-4317}:4317"
      - "${OTEL_HTTP_PORT:-4318}:4318"
      - "${OTEL_METRICS_PORT:-8888}:8888"
      - "${OTEL_HEALTH_PORT:-13133}:13133"
    command: ["--config=/etc/otel-collector-config.yml"]
    volumes:
      - ./monitoring/otel/otel-collector-config.yml:/etc/otel-collector-config.yml:ro
    networks:
      - app_network
      - monitoring_network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:13133/"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '${OTEL_CPU_LIMIT:-1.0}'
          memory: ${OTEL_MEMORY_LIMIT:-512M}
        reservations:
          cpus: '${OTEL_CPU_RESERVATION:-0.25}'
          memory: ${OTEL_MEMORY_RESERVATION:-128M}
    profiles:
      - ${PROFILE_MONITORING:-default}

  # ============================================================================
  # Jaeger Distributed Tracing Service
  # ============================================================================
  jaeger:
    image: jaegertracing/all-in-one:${JAEGER_VERSION:-1.52}
    container_name: ${CONTAINER_PREFIX:-omniclaude}_jaeger
    restart: ${RESTART_POLICY:-unless-stopped}
    ports:
      - "${JAEGER_UI_PORT:-16686}:16686"
      - "${JAEGER_COLLECTOR_PORT:-14268}:14268"
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - monitoring_network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:14269/"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '${JAEGER_CPU_LIMIT:-0.5}'
          memory: ${JAEGER_MEMORY_LIMIT:-512M}
        reservations:
          cpus: '${JAEGER_CPU_RESERVATION:-0.1}'
          memory: ${JAEGER_MEMORY_RESERVATION:-128M}
    profiles:
      - ${PROFILE_MONITORING:-default}

# ============================================================================
# Networks - Segregated for security
# ============================================================================
networks:
  app_network:
    external: true
    name: deployment_app_network
  monitoring_network:
    driver: bridge
    ipam:
      config:
        - subnet: ${MONITORING_NETWORK_SUBNET:-172.31.0.0/16}

  # External networks for cross-repository communication
  # These networks are created by omninode_bridge and allow native
  # connectivity to PostgreSQL, Kafka/Redpanda, and other shared services
  # running on 192.168.86.200 without requiring /etc/hosts configuration
  omninode-bridge-network:
    external: true
    name: omninode-bridge-network
  omninode_bridge_omninode-bridge-network:
    external: true
    name: omninode_bridge_omninode-bridge-network

# ============================================================================
# Volumes - Persistent data storage
# ============================================================================
volumes:
  app_data:
    driver: local
  postgres_data:
    driver: local
  redpanda_data:
    driver: local
  valkey_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
