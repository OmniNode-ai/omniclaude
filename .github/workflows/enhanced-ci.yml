name: Enhanced CI Pipeline with ONEX Validation

# Comprehensive CI pipeline for OmniClaude with ONEX architecture compliance
# Includes unit tests, integration tests, hooks testing, agent framework validation,
# performance benchmarks, and infrastructure validation

on:
  push:
    branches: [main, develop, feature/**]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  PYTHON_VERSION: '3.12'

jobs:
  # ============================================================================
  # Code Quality and Type Safety (Strict)
  # ============================================================================
  quality-strict:
    name: Code Quality (Strict Mode)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        run: pip install --no-cache-dir uv

      - name: Load cached venv
        id: cached-uv-dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ hashFiles('**/uv.lock') }}

      - name: Configure Git authentication for private repositories
        run: |
          git config --global url."https://x-access-token:${{ secrets.GH_PAT }}@github.com/".insteadOf "https://github.com/"
        continue-on-error: true

      - name: Install dependencies
        if: steps.cached-uv-dependencies.outputs.cache-hit != 'true'
        run: |
          # Install dependencies - continue even if private repos fail (tests use mocks)
          uv sync --frozen --group dev || {
            echo "Warning: Some dependencies failed to install (likely private repos)"
            echo "Continuing with available dependencies - tests use mocks"
            uv sync --group dev 2>&1 | grep -v "omnibase" || true
          }

      - name: Run Black (code formatting check)
        run: |
          echo "::group::Black Formatting Check"
          # Explicitly scan only project source directories to avoid .venv
          uv run black --check --diff \
            --extend-exclude="/(\.venv|\.git|__pycache__|\.pytest_cache|\.mypy_cache|dist|build|_archive)/" \
            src/ tests/
          echo "::endgroup::"

      - name: Run Ruff (linting - strict)
        run: |
          echo "::group::Ruff Linting"
          # Scan only project source directories, aligned with pre-commit config
          uv run ruff check src/ tests/ --show-source --show-fixes
          echo "::endgroup::"

      - name: Run type checking (mypy - strict mode)
        run: |
          echo "::group::MyPy Type Checking (Strict)"
          uv run pip install mypy types-PyYAML types-aiofiles types-redis
          # Strict type checking - this MUST pass for CI to succeed
          uv run mypy src/omniclaude/ \
            --strict \
            --show-error-codes \
            --show-column-numbers \
            --pretty \
            --ignore-missing-imports \
            --disallow-untyped-defs \
            --warn-redundant-casts \
            --warn-unused-ignores \
            --no-implicit-optional
          echo "::endgroup::"

      - name: Upload type checking report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: mypy-report
          path: .mypy_cache/

  # ============================================================================
  # ONEX Architecture Compliance Validation
  # ============================================================================
  onex-validation:
    name: ONEX Architecture Compliance
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        run: pip install --no-cache-dir uv

      - name: Configure Git authentication for private repositories
        run: |
          git config --global url."https://x-access-token:${{ secrets.GH_PAT }}@github.com/".insteadOf "https://github.com/"

      - name: Install dependencies
        run: |
          uv sync --frozen || {
            echo "Warning: Some dependencies failed - continuing with available packages"
            true
          }

      - name: Validate ONEX Naming Conventions
        run: |
          echo "::group::ONEX Naming Convention Validation"
          # Check for proper ONEX node naming: Node<Name><Type>
          echo "Validating ONEX node class naming conventions..."

          # Find all Python files in src/omniclaude/
          NODE_FILES=$(find src/omniclaude/ -name "node_*.py" -type f 2>/dev/null || true)
          if [ -z "$NODE_FILES" ]; then
            echo "No ONEX node files found in src/omniclaude/ - skipping validation"
            echo "This is expected for schema-only phase (OMN-1399)"
          else
            echo "$NODE_FILES" | while read -r file; do
              echo "Checking $file..."

              # Validate Effect nodes
              if [[ "$file" == *"_effect.py" ]]; then
                if ! grep -q "class Node.*Effect" "$file"; then
                  echo "ERROR: $file should define a class named Node<Name>Effect"
                  exit 1
                else
                  echo "$file: ONEX Effect naming convention passed"
                fi
              fi

              # Validate Compute nodes
              if [[ "$file" == *"_compute.py" ]]; then
                if ! grep -q "class Node.*Compute" "$file"; then
                  echo "ERROR: $file should define a class named Node<Name>Compute"
                  exit 1
                else
                  echo "$file: ONEX Compute naming convention passed"
                fi
              fi

              # Validate Reducer nodes
              if [[ "$file" == *"_reducer.py" ]]; then
                if ! grep -q "class Node.*Reducer" "$file"; then
                  echo "ERROR: $file should define a class named Node<Name>Reducer"
                  exit 1
                else
                  echo "$file: ONEX Reducer naming convention passed"
                fi
              fi

              # Validate Orchestrator nodes
              if [[ "$file" == *"_orchestrator.py" ]]; then
                if ! grep -q "class Node.*Orchestrator" "$file"; then
                  echo "ERROR: $file should define a class named Node<Name>Orchestrator"
                  exit 1
                else
                  echo "$file: ONEX Orchestrator naming convention passed"
                fi
              fi
            done
          fi

          echo "All ONEX naming conventions validated successfully"
          echo "::endgroup::"

      - name: Validate ONEX Contract Usage
        run: |
          echo "::group::ONEX Contract Validation"
          echo "Validating ONEX contract definitions..."

          # Check for ModelContract usage in nodes
          NODE_FILES=$(find src/omniclaude/ -name "node_*.py" -type f 2>/dev/null || true)
          if [ -z "$NODE_FILES" ]; then
            echo "No ONEX node files found in src/omniclaude/ - skipping contract validation"
            echo "This is expected for schema-only phase (OMN-1399)"
          else
            echo "$NODE_FILES" | while read -r file; do
              if ! grep -q "ModelContract" "$file" 2>/dev/null; then
                echo "WARNING: $file may be missing ModelContract usage"
              else
                echo "$file: Uses ModelContract"
              fi
            done
          fi

          echo "Contract validation completed"
          echo "::endgroup::"

      - name: Check ONEX Method Signatures
        run: |
          echo "::group::ONEX Method Signature Validation"
          echo "Validating ONEX method signatures..."

          # Validate Effect nodes have execute_effect method
          EFFECT_FILES=$(find src/omniclaude/ -name "*_effect.py" -type f 2>/dev/null || true)
          if [ -z "$EFFECT_FILES" ]; then
            echo "No ONEX Effect node files found in src/omniclaude/ - skipping method validation"
            echo "This is expected for schema-only phase (OMN-1399)"
          else
            echo "$EFFECT_FILES" | while read -r file; do
              if grep -q "class Node.*Effect" "$file"; then
                if ! grep -q "async def execute_effect" "$file"; then
                  echo "WARNING: $file Effect node should implement async def execute_effect()"
                else
                  echo "$file: Implements execute_effect()"
                fi
              fi
            done
          fi

          echo "Method signature validation completed"
          echo "::endgroup::"

  # ============================================================================
  # Unit Tests (Fast, Isolated) - Split across 10 parallel jobs
  # ============================================================================
  unit-tests:
    name: Unit Tests (Split ${{ matrix.split }}/10)
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Prevent infinite hangs
    strategy:
      fail-fast: false  # Don't cancel other jobs if one fails
      matrix:
        split: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        run: pip install --no-cache-dir uv

      - name: Configure Git authentication for private repositories
        run: |
          git config --global url."https://x-access-token:${{ secrets.GH_PAT }}@github.com/".insteadOf "https://github.com/"

      - name: Install dependencies
        run: |
          uv sync --frozen --group dev || {
            echo "Warning: Some dependencies failed - continuing with available packages"
            true
          }

      - name: Install pytest-split for parallel test execution
        run: |
          uv run pip install pytest-split

      - name: Run unit tests (Split ${{ matrix.split }}/10)
        run: |
          echo "::group::Unit Tests - Split ${{ matrix.split }}/10"
          # Add explicit timeout to prevent test hangs
          # Use pytest-split to distribute tests evenly across 10 splits
          # Generate unique coverage file per split to avoid overwrites

          # Check if tests directory exists and has test files
          if [ -d "tests" ] && [ -n "$(find tests/ -name 'test_*.py' -type f 2>/dev/null)" ]; then
            timeout 600 uv run pytest -o addopts="" \
              tests/ \
              -v \
              --ignore=tests/integration/ \
              -m "not integration" \
              --splits 10 \
              --group ${{ matrix.split }} \
              --cov=src/omniclaude \
              --cov-report=term-missing \
              --junitxml=junit-unit-split-${{ matrix.split }}.xml || {
                echo "::error::Unit tests exceeded 10 minute timeout or failed"
                exit 1
              }

            # Rename coverage file to be unique for this split
            mv .coverage .coverage.${{ matrix.split }}
          else
            echo "No test files found in tests/ - creating empty results"
            echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="unit" tests="0" errors="0" failures="0" skipped="0"></testsuite></testsuites>' > junit-unit-split-${{ matrix.split }}.xml
            touch .coverage.${{ matrix.split }}
          fi
          echo "::endgroup::"

      - name: Check for resource leaks
        if: always()
        run: |
          echo "::group::Resource Leak Detection"
          # Check for unclosed file descriptors
          lsof -p $$ 2>/dev/null | head -20 || echo "lsof not available, skipping"
          # Check for zombie processes
          ps aux | grep -i defunct || echo "No zombie processes found"
          echo "::endgroup::"

      - name: Upload coverage artifact for merge
        uses: actions/upload-artifact@v4
        with:
          name: coverage-unit-split-${{ matrix.split }}
          path: .coverage.${{ matrix.split }}
          retention-days: 1

      - name: Upload unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results-split-${{ matrix.split }}
          path: junit-unit-split-${{ matrix.split }}.xml

      - name: Upload failure artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-failure-logs-split-${{ matrix.split }}
          path: |
            logs/
            tmp/
            *.log
            .pytest_cache/

  # ============================================================================
  # Merge Coverage from Parallel Unit Tests
  # ============================================================================
  merge-unit-coverage:
    name: Merge Unit Test Coverage
    runs-on: ubuntu-latest
    needs: unit-tests
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install coverage
        run: pip install coverage[toml]

      - name: Download all coverage artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: coverage-unit-split-*
          path: coverage-files/

      - name: Combine coverage files
        run: |
          echo "::group::Combining Coverage Files"
          # Move all coverage files to current directory
          find coverage-files/ -name '.coverage.*' -exec mv {} . \;
          ls -la .coverage.*

          # Combine all coverage files
          coverage combine .coverage.*

          # Generate XML and term reports
          coverage xml -o coverage-unit.xml
          coverage report

          echo "::endgroup::"

      - name: Upload merged coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage-unit.xml
          flags: unittests
          name: unit-tests-merged
          fail_ci_if_error: false

      - name: Upload merged coverage artifact
        uses: actions/upload-artifact@v4
        with:
          name: coverage-unit-merged
          path: |
            coverage-unit.xml
            .coverage

  # ============================================================================
  # Integration Tests (with PostgreSQL and Redis)
  # ============================================================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Prevent infinite hangs
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        run: pip install --no-cache-dir uv

      - name: Configure Git authentication for private repositories
        run: |
          git config --global url."https://x-access-token:${{ secrets.GH_PAT }}@github.com/".insteadOf "https://github.com/"

      - name: Install dependencies
        run: |
          uv sync --frozen --group dev || {
            echo "Warning: Some dependencies failed - continuing with available packages"
            true
          }

      - name: Initialize test database
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
          PGPASSWORD: test
        run: |
          # Wait for PostgreSQL to be ready (with timeout)
          timeout 60 bash -c 'until psql -h localhost -U test -d test_db -c '\''\q'\''; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done' || {
            echo "::error::PostgreSQL failed to become ready within 60 seconds"
            exit 1
          }

          # Run init script as bash (not as SQL input)
          bash scripts/init-db.sh || true

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/0
        run: |
          echo "::group::Integration Tests"
          # Add explicit timeout to prevent test hangs

          # Check if integration tests directory exists
          if [ -d "tests/integration" ] && [ -n "$(find tests/integration/ -name 'test_*.py' -type f 2>/dev/null)" ]; then
            timeout 900 uv run pytest \
              tests/integration/ \
              -v \
              -m integration \
              --cov=src/omniclaude \
              --cov-report=xml \
              --cov-report=term-missing \
              --junitxml=junit-integration.xml || {
                echo "::error::Integration tests exceeded 15 minute timeout or failed"
                exit 1
              }
          else
            echo "No integration tests found in tests/integration/ - skipping"
            echo "Integration tests will be added in a future PR"
            # Create empty junit file for artifact upload
            echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="integration" tests="0" errors="0" failures="0" skipped="0"></testsuite></testsuites>' > junit-integration.xml
          fi
          echo "::endgroup::"

      - name: Check for resource leaks
        if: always()
        run: |
          echo "::group::Resource Leak Detection"
          # Check database connections
          psql -h localhost -U test -d test_db -c "SELECT COUNT(*) as active_connections FROM pg_stat_activity;" || echo "Cannot check DB connections"
          # Check for unclosed file descriptors
          lsof -p $$ 2>/dev/null | head -20 || echo "lsof not available, skipping"
          # Check for zombie processes
          ps aux | grep -i defunct || echo "No zombie processes found"
          echo "::endgroup::"

      - name: Upload integration test coverage
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: integration
          name: integration-tests

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: junit-integration.xml

      - name: Upload failure artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-failure-logs
          path: |
            logs/
            tmp/
            *.log
            .pytest_cache/

  # ============================================================================
  # Hooks System Tests
  # ============================================================================
  hooks-tests:
    name: Hooks System Tests
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: hooks_test
          POSTGRES_PASSWORD: hooks_test
          POSTGRES_DB: hooks_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        run: pip install --no-cache-dir uv

      - name: Configure Git authentication for private repositories
        run: |
          git config --global url."https://x-access-token:${{ secrets.GH_PAT }}@github.com/".insteadOf "https://github.com/"

      - name: Install dependencies
        run: |
          uv sync --frozen --group dev || {
            echo "Warning: Some dependencies failed - continuing with available packages"
            true
          }

      - name: Initialize hooks database
        env:
          POSTGRES_USER: hooks_test
          POSTGRES_PASSWORD: hooks_test
          POSTGRES_DB: hooks_test_db
          PGPASSWORD: hooks_test
        run: |
          # Wait for PostgreSQL
          until psql -h localhost -U hooks_test -d hooks_test_db -c '\q'; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done

          # Run init script as bash (not as SQL input)
          bash scripts/init-db.sh || true

      - name: Run hooks system tests
        env:
          DATABASE_URL: postgresql://hooks_test:hooks_test@localhost:5432/hooks_test_db
        run: |
          echo "::group::Hooks System Tests"
          # Check if tests/hooks/ exists (hooks tests in new structure)
          if [ -d "tests/hooks" ] && [ -n "$(find tests/hooks/ -name 'test_*.py' -type f 2>/dev/null)" ]; then
            uv run pytest \
              tests/hooks/ \
              -v \
              --cov=src/omniclaude \
              --cov-report=xml \
              --cov-report=term-missing \
              --junitxml=junit-hooks.xml
          else
            echo "No hooks tests found in tests/hooks/ - skipping"
            echo "Hooks tests will be added in a future PR"
            # Create empty junit file for artifact upload
            echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="hooks" tests="0" errors="0" failures="0" skipped="0"></testsuite></testsuites>' > junit-hooks.xml
          fi
          echo "::endgroup::"

      - name: Upload hooks test coverage
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: hooks
          name: hooks-tests

      - name: Upload hooks test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: hooks-test-results
          path: junit-hooks.xml

  # ============================================================================
  # Agent Framework Tests
  # ============================================================================
  agent-framework-tests:
    name: Agent Framework Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        run: pip install --no-cache-dir uv

      - name: Configure Git authentication for private repositories
        run: |
          git config --global url."https://x-access-token:${{ secrets.GH_PAT }}@github.com/".insteadOf "https://github.com/"

      - name: Install dependencies
        run: |
          uv sync --frozen --group dev || {
            echo "Warning: Some dependencies failed - continuing with available packages"
            true
          }

      - name: Run agent framework tests
        run: |
          echo "::group::Agent Framework Tests"
          # Check if agent framework tests exist in new structure
          AGENT_TESTS=""
          for test_file in tests/test_enhanced_router.py tests/test_quality_gates.py tests/test_performance_thresholds.py; do
            if [ -f "$test_file" ]; then
              AGENT_TESTS="$AGENT_TESTS $test_file"
            fi
          done

          if [ -n "$AGENT_TESTS" ]; then
            uv run pytest \
              $AGENT_TESTS \
              -v \
              --cov=src/omniclaude \
              --cov-report=xml \
              --cov-report=term-missing \
              --junitxml=junit-agent-framework.xml
          else
            echo "No agent framework tests found - skipping"
            echo "Agent framework tests will be added in a future PR when ONEX nodes are implemented"
            # Create empty junit file for artifact upload
            echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="agent-framework" tests="0" errors="0" failures="0" skipped="0"></testsuite></testsuites>' > junit-agent-framework.xml
          fi
          echo "::endgroup::"

      - name: Upload agent framework coverage
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: agent-framework
          name: agent-framework-tests

      - name: Upload agent framework test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: agent-framework-test-results
          path: junit-agent-framework.xml

  # ============================================================================
  # Performance Benchmarks
  # ============================================================================
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        run: pip install --no-cache-dir uv

      - name: Configure Git authentication for private repositories
        run: |
          git config --global url."https://x-access-token:${{ secrets.GH_PAT }}@github.com/".insteadOf "https://github.com/"

      - name: Install dependencies
        run: |
          uv sync --frozen --group dev || {
            echo "Warning: Some dependencies failed - continuing with available packages"
            true
          }

      - name: Run performance benchmarks
        run: |
          echo "::group::Performance Benchmarks"

          BENCHMARKS_RUN=0

          # Run template cache benchmark if exists
          if [ -f "tests/benchmark_template_cache.py" ]; then
            uv run python tests/benchmark_template_cache.py
            BENCHMARKS_RUN=$((BENCHMARKS_RUN + 1))
          fi

          # Run parallel execution benchmark if exists
          if [ -f "tests/parallel_benchmark.py" ]; then
            uv run python tests/parallel_benchmark.py
            BENCHMARKS_RUN=$((BENCHMARKS_RUN + 1))
          fi

          # Run general benchmarks if exists
          if [ -f "tests/benchmark_improvements.py" ]; then
            uv run python tests/benchmark_improvements.py
            BENCHMARKS_RUN=$((BENCHMARKS_RUN + 1))
          fi

          if [ $BENCHMARKS_RUN -eq 0 ]; then
            echo "No performance benchmark scripts found - skipping"
            echo "Performance benchmarks will be added in a future PR"
            # Create placeholder files for artifact upload
            echo '{"benchmarks": [], "status": "no_benchmarks_found"}' > benchmark-results.json
            echo "No benchmarks run - benchmark scripts not yet implemented" > performance-report.txt
          fi

          echo "::endgroup::"

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-benchmarks
          path: |
            benchmark-results.json
            performance-report.txt

  # ============================================================================
  # Database Schema Validation
  # ============================================================================
  database-validation:
    name: Database Schema Validation
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: schema_test
          POSTGRES_PASSWORD: schema_test
          POSTGRES_DB: schema_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Validate database initialization script
        env:
          PGPASSWORD: schema_test
          POSTGRES_USER: schema_test
          POSTGRES_DB: schema_test_db
        run: |
          echo "::group::Database Schema Validation"

          # Wait for PostgreSQL
          until psql -h localhost -U schema_test -d schema_test_db -c '\q'; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done

          # Run init script as bash (not as SQL input)
          bash scripts/init-db.sh

          # Validate tables were created
          echo "Validating agent_routing_decisions table..."
          psql -h localhost -U schema_test -d schema_test_db -c '\d agent_routing_decisions'

          echo "Validating agent_transformation_events table..."
          psql -h localhost -U schema_test -d schema_test_db -c '\d agent_transformation_events'

          echo "Validating router_performance_metrics table..."
          psql -h localhost -U schema_test -d schema_test_db -c '\d router_performance_metrics'

          # Validate indexes
          echo "Validating indexes..."
          psql -h localhost -U schema_test -d schema_test_db -c '\di'

          echo "Database schema validation passed"
          echo "::endgroup::"

  # ============================================================================
  # Docker Compose Stack Validation
  # ============================================================================
  docker-compose-validation:
    name: Docker Compose Stack Validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Validate docker compose.yml
        run: |
          echo "::group::Docker Compose Validation"

          # Validate compose file syntax (use v2 syntax: docker compose)
          docker compose config --quiet

          echo "docker compose.yml syntax is valid"

          # Validate services are defined
          echo "Checking defined services..."
          docker compose config --services

          echo "::endgroup::"

      - name: Build Docker Compose stack
        run: |
          echo "::group::Building Docker Compose Stack"
          docker compose build --parallel
          echo "::endgroup::"

      - name: Start stack and validate health
        run: |
          echo "::group::Stack Health Validation"

          # Start services
          docker compose up -d

          # Wait for services to be healthy
          echo "Waiting for services to be healthy..."
          sleep 30

          # Check service health
          docker compose ps

          # Validate PostgreSQL
          docker compose exec -T postgres pg_isready -U omniclaude || echo "PostgreSQL not ready"

          # Validate Redis
          docker compose exec -T redis redis-cli ping || echo "Redis not ready"

          # Check app health endpoint
          curl -f http://localhost:8000/health || echo "App health check failed"

          echo "::endgroup::"

      - name: Cleanup
        if: always()
        run: |
          docker compose down -v

  # ============================================================================
  # Build and Push Docker Image
  # ============================================================================
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs:
      - quality-strict
      - onex-validation
      - merge-unit-coverage
      - integration-tests
      - hooks-tests
      - agent-framework-tests
      - database-validation
      - docker-compose-validation
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,prefix=,format=short
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./deployment/Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILD_DATE=${{ steps.meta.outputs.created }}
            VCS_REF=${{ github.sha }}
            VERSION=${{ steps.meta.outputs.version }}

  # ============================================================================
  # Test Summary Report
  # ============================================================================
  test-summary:
    name: Test Summary Report
    runs-on: ubuntu-latest
    needs:
      - merge-unit-coverage
      - integration-tests
      - hooks-tests
      - agent-framework-tests
      - performance-benchmarks
    if: always()
    steps:
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: test-results/

      - name: Generate test summary
        run: |
          echo "# OmniClaude CI Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # List all artifacts
          ls -la test-results/

          echo "All tests completed" >> $GITHUB_STEP_SUMMARY
