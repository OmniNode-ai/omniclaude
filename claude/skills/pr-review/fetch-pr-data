#!/bin/bash
set -euo pipefail

# ============================================================================
# PR Review Data Fetcher - OPTIMIZED VERSION
# ============================================================================
# Fetches PR review data from 4 GitHub API endpoints with smart limits,
# caching, and graceful degradation for large PRs.
#
# Optimizations:
#   - 5-minute cache to avoid redundant API calls
#   - --limit flag for large PRs (only recent + bot comments)
#   - Summary-only mode for extremely large datasets
#   - Parallel fetching with error recovery
#   - Statistics tracking
# ============================================================================

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
CACHE_DIR="${TMPDIR:-/tmp}/pr-review-cache"
CACHE_TTL=300  # 5 minutes
MAX_COMMENTS_DEFAULT=1000  # Default limit before summarization

# Fetch resolved review threads via GraphQL
# Returns JSON: [{path, line, isResolved, commentIds: [...]}]
fetch_resolved_threads() {
    local repo_name="$1"
    local pr_number="$2"

    # Split owner/repo
    local owner="${repo_name%%/*}"
    local repo="${repo_name#*/}"

    # GraphQL query to fetch review threads with resolution status
    gh api graphql -f query='
        query($owner: String!, $repo: String!, $pr: Int!) {
            repository(owner: $owner, name: $repo) {
                pullRequest(number: $pr) {
                    reviewThreads(first: 100) {
                        nodes {
                            isResolved
                            isOutdated
                            path
                            line
                            originalLine
                            resolvedBy {
                                login
                            }
                            comments(first: 50) {
                                nodes {
                                    id
                                    databaseId
                                    body
                                    createdAt
                                    author {
                                        login
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    ' -f owner="$owner" -f repo="$repo" -F pr="$pr_number" 2>/dev/null | jq '[
        .data.repository.pullRequest.reviewThreads.nodes[] |
        {
            path: .path,
            line: (.line // .originalLine),
            is_resolved: .isResolved,
            is_outdated: .isOutdated,
            resolved_by: (.resolvedBy.login // null),
            comment_ids: [.comments.nodes[].databaseId],
            first_comment_body: (.comments.nodes[0].body // ""),
            first_comment_author: (.comments.nodes[0].author.login // "")
        }
    ]' 2>/dev/null || echo '[]'
}

# Function to print colored status (to stderr to keep stdout clean for JSON)
print_status() {
    echo -e "${BLUE}[PR-FETCH]${NC} $1" >&2
}

print_success() {
    echo -e "${GREEN}✓${NC} $1" >&2
}

print_warning() {
    echo -e "${YELLOW}⚠${NC} $1" >&2
}

print_error() {
    echo -e "${RED}✗${NC} $1" >&2
}

# Usage
usage() {
    cat << EOF
Usage: $(basename "$0") <PR_NUMBER> [OPTIONS]

Fetch PR review data from GitHub API (4 endpoints) with smart caching.

Arguments:
  PR_NUMBER    PR number (e.g., 20) or full GitHub URL

Options:
  --limit NUM       Limit comments per endpoint (default: no limit)
                    For large PRs, fetches recent + bot comments only
  --summary-only    Statistics only, no comment bodies (for huge PRs)
  --no-cache        Skip cache, always fetch fresh data
  --cache-only      Use cached data only, fail if not cached

Examples:
  $(basename "$0") 20                    # Full fetch (cached)
  $(basename "$0") 88 --limit 50         # Large PR, recent 50 only
  $(basename "$0") 88 --summary-only     # Statistics only

Output:
  JSON object with 4 arrays + summary statistics

Exit codes:
  0 - Success
  1 - Missing dependencies or arguments
  2 - GitHub API error
  3 - Cache miss (with --cache-only)
EOF
    exit 1
}

# Check dependencies
check_dependencies() {
    local missing_deps=()

    if ! command -v gh &> /dev/null; then
        missing_deps+=("gh (GitHub CLI)")
    fi

    if ! command -v jq &> /dev/null; then
        missing_deps+=("jq")
    fi

    if [ ${#missing_deps[@]} -ne 0 ]; then
        print_error "Missing required dependencies:"
        for dep in "${missing_deps[@]}"; do
            echo "  - $dep" >&2
        done
        echo "" >&2
        echo "Install with:" >&2
        echo "  brew install gh jq" >&2
        exit 1
    fi
}

# Extract PR number from argument (handles URLs)
extract_pr_number() {
    local input="$1"

    # If it's a URL, extract the PR number
    if [[ "$input" =~ pull/([0-9]+) ]]; then
        echo "${BASH_REMATCH[1]}"
    # If it's already a number, use it
    elif [[ "$input" =~ ^[0-9]+$ ]]; then
        echo "$input"
    else
        print_error "Invalid PR number or URL: $input"
        exit 1
    fi
}

# Get repository name (owner/repo format)
get_repo_name() {
    gh repo view --json nameWithOwner -q .nameWithOwner 2>/dev/null || {
        print_error "Failed to get repository name. Are you in a git repository?"
        exit 2
    }
}

# Check cache validity
check_cache() {
    local cache_file="$1"

    if [[ ! -f "$cache_file" ]]; then
        return 1  # Cache miss
    fi

    # Check age
    local cache_age=$(($(date +%s) - $(stat -f %m "$cache_file" 2>/dev/null || stat -c %Y "$cache_file" 2>/dev/null)))

    if [[ $cache_age -gt $CACHE_TTL ]]; then
        print_warning "Cache expired (${cache_age}s old, TTL: ${CACHE_TTL}s)"
        return 1  # Cache expired
    fi

    print_success "Cache hit (${cache_age}s old)"
    return 0  # Cache valid
}

# Fetch with smart limits
fetch_endpoint_smart() {
    local endpoint="$1"
    local limit="$2"
    local summary_only="$3"

    if [[ "$summary_only" == "true" ]]; then
        # Summary only: count + metadata, no bodies
        gh api "$endpoint" --jq '[.[] | {author: .user.login, created_at: .created_at // .submitted_at}] | {count: length, authors: [.[].author] | unique, latest: .[0].created_at}' 2>/dev/null || echo '{"count": 0, "authors": [], "latest": null}'
    elif [[ -n "$limit" ]] && [[ "$limit" -gt 0 ]]; then
        # Limited fetch: recent + bots
        gh api "$endpoint" --paginate --jq "
            [.[] | select(.user.login | contains(\"bot\") or contains(\"claude\"))] as \$bots |
            [.[] | select(.user.login | contains(\"bot\") or contains(\"claude\") | not)] | sort_by(.created_at // .submitted_at) | reverse | .[0:$limit] as \$recent |
            (\$bots + \$recent) | unique_by(.id)
        " 2>/dev/null || echo '[]'
    else
        # Full fetch
        gh api "$endpoint" --paginate --jq '.' 2>/dev/null || echo '[]'
    fi
}

# Main fetch function
fetch_pr_data() {
    local pr_number="$1"
    local limit="${2:-}"
    local summary_only="${3:-false}"
    local use_cache="${4:-true}"
    local cache_only="${5:-false}"
    local repo_name

    # Setup cache with mode-specific key to prevent serving truncated data as full
    mkdir -p "$CACHE_DIR"
    local fetch_mode="full"
    if [[ "$summary_only" == "true" ]]; then
        fetch_mode="summary"
    elif [[ -n "$limit" ]]; then
        fetch_mode="limited_${limit}"
    fi
    local cache_file="$CACHE_DIR/pr_${pr_number}_${fetch_mode}.json"

    # Check cache
    if [[ "$use_cache" == "true" ]] && check_cache "$cache_file"; then
        cat "$cache_file"
        return 0
    fi

    if [[ "$cache_only" == "true" ]]; then
        print_error "Cache miss and --cache-only specified"
        exit 3
    fi

    print_status "Fetching PR #$pr_number data from all endpoints..."

    # Get repo name
    repo_name=$(get_repo_name)
    print_status "Repository: $repo_name"

    # Create temporary files for parallel fetching
    local tmp_dir=$(mktemp -d)
    local reviews_file="$tmp_dir/reviews.json"
    local inline_file="$tmp_dir/inline.json"
    local pr_comments_file="$tmp_dir/pr_comments.json"
    local issue_comments_file="$tmp_dir/issue_comments.json"
    local resolved_threads_file="$tmp_dir/resolved_threads.json"

    # Fetch in parallel for speed
    print_status "Fetching from 5 endpoints in parallel..."

    if [[ -n "$limit" ]]; then
        print_warning "Limiting to $limit recent comments per endpoint + all bot comments"
    fi

    # 0. Resolved review threads (GraphQL) - always fetch for resolution tracking
    (
        if [[ "$summary_only" != "true" ]]; then
            fetch_resolved_threads "$repo_name" "$pr_number" > "$resolved_threads_file"
        else
            echo '[]' > "$resolved_threads_file"
        fi
        print_success "Resolved Threads API (GraphQL)"
    ) &

    # 1. Formal PR reviews
    (
        if [[ "$summary_only" == "true" ]]; then
            gh api "repos/$repo_name/pulls/$pr_number/reviews" \
                --jq '{count: length, states: [.[].state] | group_by(.) | map({state: .[0], count: length})}' > "$reviews_file" 2>/dev/null || echo '{"count": 0, "states": []}' > "$reviews_file"
        else
            gh api "repos/$repo_name/pulls/$pr_number/reviews" \
                --jq '[.[] | {
                    author: .user.login,
                    state: .state,
                    body: .body,
                    submitted_at: .submitted_at,
                    id: .id
                }]' > "$reviews_file" 2>/dev/null || echo "[]" > "$reviews_file"
        fi
        print_success "Reviews API"
    ) &

    # 2. Inline code review comments
    (
        local endpoint="repos/$repo_name/pulls/$pr_number/comments"
        if [[ "$summary_only" == "true" ]]; then
            gh api "$endpoint" --jq '{count: length, files: [.[].path] | unique | length}' > "$inline_file" 2>/dev/null || echo '{"count": 0, "files": 0}' > "$inline_file"
        elif [[ -n "$limit" ]]; then
            # Fetch all bot comments + recent N human comments
            gh api "$endpoint" --paginate --jq "
                ([.[] | select(.user.login | test(\"bot|claude\"; \"i\"))]) as \$bots |
                ([.[] | select(.user.login | test(\"bot|claude\"; \"i\") | not)] | sort_by(.created_at) | reverse | .[0:${limit}]) as \$recent |
                (\$bots + \$recent) | unique_by(.id) | map({
                    author: .user.login,
                    path: .path,
                    line: .line,
                    body: .body,
                    created_at: .created_at,
                    id: .id
                })
            " > "$inline_file" 2>/dev/null || echo "[]" > "$inline_file"
        else
            gh api "$endpoint" --paginate \
                --jq '[.[] | {
                    author: .user.login,
                    path: .path,
                    line: .line,
                    body: .body,
                    created_at: .created_at,
                    id: .id
                }]' > "$inline_file" 2>/dev/null || echo "[]" > "$inline_file"
        fi
        print_success "Inline Comments API"
    ) &

    # 3. PR conversation comments
    (
        if [[ "$summary_only" == "true" ]]; then
            gh pr view "$pr_number" --json comments --jq '{count: (.comments | length)}' > "$pr_comments_file" 2>/dev/null || echo '{"count": 0}' > "$pr_comments_file"
        elif [[ -n "$limit" ]]; then
            gh pr view "$pr_number" --json comments --jq "
                ([.comments[] | select(.author.login | test(\"bot|claude\"; \"i\"))]) as \$bots |
                ([.comments[] | select(.author.login | test(\"bot|claude\"; \"i\") | not)] | sort_by(.createdAt) | reverse | .[0:${limit}]) as \$recent |
                (\$bots + \$recent) | unique_by(.id) | map({
                    author: .author.login,
                    body: .body,
                    created_at: .createdAt,
                    id: .id
                })
            " > "$pr_comments_file" 2>/dev/null || echo "[]" > "$pr_comments_file"
        else
            gh pr view "$pr_number" --json comments \
                --jq '[.comments[] | {
                    author: .author.login,
                    body: .body,
                    created_at: .createdAt,
                    id: .id
                }]' > "$pr_comments_file" 2>/dev/null || echo "[]" > "$pr_comments_file"
        fi
        print_success "PR Comments API"
    ) &

    # 4. Issue comments (WHERE CLAUDE CODE BOT POSTS!)
    (
        local endpoint="repos/$repo_name/issues/$pr_number/comments"
        if [[ "$summary_only" == "true" ]]; then
            gh api "$endpoint" --jq '{count: length, has_bot: ([.[].user.login] | any(test(\"bot|claude\"; \"i\")))}' > "$issue_comments_file" 2>/dev/null || echo '{"count": 0, "has_bot": false}' > "$issue_comments_file"
        elif [[ -n "$limit" ]]; then
            gh api "$endpoint" --paginate --jq "
                ([.[] | select(.user.login | test(\"bot|claude\"; \"i\"))]) as \$bots |
                ([.[] | select(.user.login | test(\"bot|claude\"; \"i\") | not)] | sort_by(.created_at) | reverse | .[0:${limit}]) as \$recent |
                (\$bots + \$recent) | unique_by(.id) | map({
                    author: .user.login,
                    body: .body,
                    created_at: .created_at,
                    id: .id
                })
            " > "$issue_comments_file" 2>/dev/null || echo "[]" > "$issue_comments_file"
        else
            # Capture stderr to detect GitHub API errors
            local api_output
            if api_output=$(gh api "$endpoint" --paginate \
                --jq '[.[] | {
                    author: .user.login,
                    body: .body,
                    created_at: .created_at,
                    id: .id
                }]' 2>&1); then
                echo "$api_output" > "$issue_comments_file"
            else
                # GitHub API error - surface to user and exit with error code
                echo "[]" > "$issue_comments_file"
                print_error "GitHub API error fetching issue comments: $api_output"
                print_error "Endpoint: $endpoint"
                rm -rf "$tmp_dir"
                exit 2
            fi
        fi
        print_success "Issue Comments API (Claude bot comments!)"
    ) &

    # Wait for all parallel fetches to complete
    wait

    print_status "All endpoints fetched successfully"

    # Combine into single JSON object
    print_status "Combining results..."

    # Use file-based approach to avoid "Argument list too long" errors
    local combined_file="$tmp_dir/combined.json"

    # Create combined JSON using slurpfile instead of --argjson (handles large data)
    jq -n \
        --arg pr_num "$pr_number" \
        --arg repo "$repo_name" \
        --arg limit "$limit" \
        --arg summary_only "$summary_only" \
        --slurpfile reviews "$reviews_file" \
        --slurpfile inline "$inline_file" \
        --slurpfile pr_comments "$pr_comments_file" \
        --slurpfile issue_comments "$issue_comments_file" \
        --slurpfile resolved_threads "$resolved_threads_file" \
        '{
            pr_number: ($pr_num | tonumber),
            repository: $repo,
            fetched_at: (now | todate),
            fetch_mode: (if $summary_only == "true" then "summary" elif $limit != "" then "limited" else "full" end),
            limit: (if $limit != "" then ($limit | tonumber) else null end),
            reviews: $reviews[0],
            inline_comments: $inline[0],
            pr_comments: $pr_comments[0],
            issue_comments: $issue_comments[0],
            resolved_threads: $resolved_threads[0],
            summary: {
                total_reviews: (if $reviews[0] | type == "array" then ($reviews[0] | length) elif $reviews[0].count? then $reviews[0].count else 0 end),
                total_inline_comments: (if $inline[0] | type == "array" then ($inline[0] | length) elif $inline[0].count? then $inline[0].count else 0 end),
                total_pr_comments: (if $pr_comments[0] | type == "array" then ($pr_comments[0] | length) elif $pr_comments[0].count? then $pr_comments[0].count else 0 end),
                total_issue_comments: (if $issue_comments[0] | type == "array" then ($issue_comments[0] | length) elif $issue_comments[0].count? then $issue_comments[0].count else 0 end),
                total_resolved_threads: (if $resolved_threads[0] | type == "array" then [$resolved_threads[0][] | select(.is_resolved == true)] | length else 0 end),
                total_unresolved_threads: (if $resolved_threads[0] | type == "array" then [$resolved_threads[0][] | select(.is_resolved == false)] | length else 0 end),
                total_all_comments: (
                    (if $reviews[0] | type == "array" then ($reviews[0] | length) elif $reviews[0].count? then $reviews[0].count else 0 end) +
                    (if $inline[0] | type == "array" then ($inline[0] | length) elif $inline[0].count? then $inline[0].count else 0 end) +
                    (if $pr_comments[0] | type == "array" then ($pr_comments[0] | length) elif $pr_comments[0].count? then $pr_comments[0].count else 0 end) +
                    (if $issue_comments[0] | type == "array" then ($issue_comments[0] | length) elif $issue_comments[0].count? then $issue_comments[0].count else 0 end)
                )
            }
        }' > "$combined_file"

    local output
    output=$(cat "$combined_file")

    # Cache the output
    if [[ "$use_cache" == "true" ]]; then
        echo "$output" > "$cache_file"
        print_success "Cached to: $cache_file"
    fi

    # Output result
    echo "$output"

    # Cleanup
    rm -rf "$tmp_dir"
}

# Main
main() {
    local pr_number=""
    local limit=""
    local summary_only=false
    local use_cache=true
    local cache_only=false

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            --limit)
                limit="$2"
                shift 2
                ;;
            --summary-only)
                summary_only=true
                shift
                ;;
            --no-cache)
                use_cache=false
                shift
                ;;
            --cache-only)
                cache_only=true
                shift
                ;;
            -h|--help)
                usage
                ;;
            *)
                if [[ -z "$pr_number" ]]; then
                    pr_number="$1"
                else
                    print_error "Unknown argument: $1"
                    usage
                fi
                shift
                ;;
        esac
    done

    if [[ -z "$pr_number" ]]; then
        usage
    fi

    check_dependencies

    pr_number=$(extract_pr_number "$pr_number")

    fetch_pr_data "$pr_number" "$limit" "$summary_only" "$use_cache" "$cache_only"
}

main "$@"
