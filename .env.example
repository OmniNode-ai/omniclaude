# OmniClaude Environment Configuration
# ==============================================================================
# Copy this file to .env and fill in your actual API keys
# Never commit .env to version control!
#
# Setup Instructions:
# 1. cp .env.example .env
# 2. Edit .env with your actual API keys
# 3. source .env (or use direnv)
# 4. Verify: echo $GEMINI_API_KEY
#
# See SECURITY_KEY_ROTATION.md for detailed setup and rotation procedures
# ==============================================================================

# ------------------------------------------------------------------------------
# Google Gemini API Configuration
# ------------------------------------------------------------------------------
# Used by: Multi-provider support, AI quorum validation
# Get your key: https://console.cloud.google.com/apis/credentials
# Required for: Gemini Pro, Gemini Flash, Gemini 2.5 Flash providers
# Permissions: Enable "Generative Language API"
# Rate Limits: Check Google Cloud Console for your tier
GEMINI_API_KEY=your_gemini_api_key_here

# Google API Key (Pydantic AI Integration)
# Note: This should be the same as GEMINI_API_KEY for Pydantic AI compatibility
GOOGLE_API_KEY=your_gemini_api_key_here

# ------------------------------------------------------------------------------
# Z.ai API Configuration
# ------------------------------------------------------------------------------
# Used by: GLM model support, high-concurrency operations
# Get your key: https://z.ai/dashboard (or appropriate Z.ai portal)
# Required for: Z.ai provider (GLM-4.5-Air, GLM-4.5, GLM-4.6)
# Rate Limits:
#   - GLM-4.5-Air: 5 concurrent requests
#   - GLM-4.5: 20 concurrent requests
#   - GLM-4.6: 10 concurrent requests
ZAI_API_KEY=your_zai_api_key_here

# ==============================================================================
# PostgreSQL Configuration
# ==============================================================================
# Used by: Claude Code hooks, agent tracking, pattern traceability
# Purpose: Stores hook events, agent detections, tool usage, correlation tracking
# Database: PostgreSQL (omninode_bridge)
#
# Primary connection (use this for all new code):
POSTGRES_HOST=192.168.86.200
POSTGRES_PORT=5436
POSTGRES_DB=omninode_bridge
POSTGRES_USER=postgres
POSTGRES_PASSWORD=  # Set your password here
#
# Legacy password aliases (DEPRECATED - update code to use POSTGRES_PASSWORD above)
# These are kept for backward compatibility only
DB_PASSWORD=${POSTGRES_PASSWORD}
OMNINODE_BRIDGE_POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
#
# Deployment Examples:
#   - Local development: POSTGRES_HOST=localhost, POSTGRES_PORT=5436
#   - Remote server: POSTGRES_HOST=192.168.86.200, POSTGRES_PORT=5436 (recommended)
#   - Docker internal: POSTGRES_HOST=omniclaude_postgres, POSTGRES_PORT=5432
#
# SECURITY WARNING: Change default password in production!

# ------------------------------------------------------------------------------
# Service URL Configuration
# ------------------------------------------------------------------------------
# Used by: Debug utilities, health checks, pattern tracking, hook integrations
# Purpose: Configure URLs for various backend services
# Default: localhost URLs for local development
#
# Intelligence Service (archon-intelligence-adapter):
INTELLIGENCE_SERVICE_URL=http://localhost:8053

# Main Archon Server:
MAIN_SERVER_URL=http://localhost:8181

# Archon MCP Server:
ARCHON_MCP_URL=http://localhost:8051

# Qdrant Vector Database:
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_URL=http://localhost:6333

# Production deployment:
# - Set these to actual service hostnames/IPs
# - Example: INTELLIGENCE_SERVICE_URL=http://192.168.86.200:8053
# - Example: QDRANT_HOST=192.168.86.200

# ------------------------------------------------------------------------------
# ==============================================================================
# Kafka/Redpanda Configuration
# ==============================================================================
# Used by: Event-driven intelligence, agent tracking, documentation hooks
# Purpose: Central message broker for distributed intelligence and observability
# Admin UI: http://192.168.86.200:8080 (Redpanda Console)
#
# DEPLOYMENT-SPECIFIC CONFIGURATION:
# Kafka uses different ports depending on where your code runs:
#
# 1. DOCKER-INTERNAL (services in docker-compose):
#    Use hostname 'omninode-bridge-redpanda' with port 9092
KAFKA_BOOTSTRAP_SERVERS=omninode-bridge-redpanda:9092
#
# 2. HOST SCRIPTS (running on your local machine):
#    Uncomment this and comment out the line above
# KAFKA_BOOTSTRAP_SERVERS=192.168.86.200:29092
#
# 3. REMOTE SERVER (SSH'd into 192.168.86.200):
#    Uncomment this and comment out the line above
# KAFKA_BOOTSTRAP_SERVERS=localhost:29092
#
# Legacy Configuration (backward compatibility):
KAFKA_INTELLIGENCE_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
#
# Feature Flags:
KAFKA_ENABLE_INTELLIGENCE=true          # Enable event-based intelligence queries
KAFKA_ENABLE_LOGGING=true               # Enable Kafka event logging (default: true)
ENABLE_EVENT_BASED_DISCOVERY=true       # Enable event-first pattern discovery
ENABLE_FILESYSTEM_FALLBACK=true         # Fallback to filesystem on event failure
PREFER_EVENT_PATTERNS=true              # Prefer event patterns over built-in patterns
#
# Performance Configuration:
KAFKA_REQUEST_TIMEOUT_MS=5000           # Request timeout (default: 5000ms)
# Target p95 response time: <100ms
# Target success rate: >95% (with fallback)
#
# Topics:
#
# Intelligence Topics (archon-intelligence service):
#   - dev.archon-intelligence.intelligence.code-analysis-requested.v1
#   - dev.archon-intelligence.intelligence.code-analysis-completed.v1
#   - dev.archon-intelligence.intelligence.code-analysis-failed.v1
#   Purpose: Real-time pattern discovery and intelligence queries
#
# Agent Tracking Topics (agent observability):
#   - agent-routing-decisions: Agent selection and confidence scores
#   - agent-transformation-events: Polymorphic agent transformations
#   - router-performance-metrics: Routing performance analytics
#   - agent-actions: Complete agent tool calls and execution steps
#   Purpose: Complete agent observability with replay capability
#
# Documentation Topics (git hooks):
#   - documentation-changed: Documentation file events (added, updated, deleted)
#   Purpose: Documentation change tracking for downstream processing
KAFKA_DOC_TOPIC=documentation-changed
#
# Troubleshooting:
# - Verify Redpanda running: docker ps | grep redpanda
# - Check topics: docker exec -it omninode-bridge-redpanda rpk topic list
# - Test connectivity: kcat -L -b 192.168.86.200:29092
# - View messages: kcat -C -b 192.168.86.200:29092 -t <topic-name>
#
# Requirements:
# - Redpanda broker running (docker-compose up redpanda)
# - Event topics auto-created on first publish
# - archon-intelligence service running for intelligence queries (port 8053)
# - omniclaude_agent_consumer running for agent tracking (Docker container)

# ------------------------------------------------------------------------------
# ---------------------
#   KAFKA_BOOTSTRAP_SERVERS=omninode-bridge-redpanda:9092
#
# Local Development (Host to Docker):
#   KAFKA_BOOTSTRAP_SERVERS=192.168.86.200:9092
#
# Remote Production Server:
#   KAFKA_BOOTSTRAP_SERVERS=192.168.86.200:9092
#
# Port Reference:
# ---------------
# Port 9092  = Direct Kafka access (recommended for all environments)
#
# Troubleshooting:
# ----------------
# Verify Redpanda running:
#   docker ps | grep redpanda
#
# Check topics:
#   docker exec -it omninode-bridge-redpanda rpk topic list
#
# Test connectivity:
#   kcat -L -b 192.168.86.200:9092
#
# View messages:
#   kcat -C -b 192.168.86.200:9092 -t <topic-name>
#
# Requirements:
# -------------
# - Redpanda broker running (docker-compose up redpanda)
# - Event topics auto-created on first publish
# - archon-intelligence service running for intelligence queries (port 8053)
# - omniclaude_agent_consumer running for agent tracking (Docker container)
#
# Related Documentation:
# ----------------------
# - EVENT_INTELLIGENCE_INTEGRATION_PLAN.md - Integration architecture
# - CLIENT_WORKS_EVIDENCE.md - Functionality validation
# - EVENT_INTELLIGENCE_DEVELOPER_GUIDE.md - Developer integration guide
# - AGENT_OBSERVABILITY_SCHEMA.md - Agent tracking schema documentation
# - docs/KAFKA_PORT_CONFIGURATION.md - Detailed port configuration guide

# ------------------------------------------------------------------------------
# Docker Compose Deployment Configuration
# ------------------------------------------------------------------------------
# Used by: deployment/docker-compose.yml for agent-observability-consumer service
# Purpose: Connects to external omninode-bridge PostgreSQL database
#
# NOTE: PostgreSQL password is configured in the "PostgreSQL Configuration" section above
# The OMNINODE_BRIDGE_POSTGRES_PASSWORD variable is set to ${POSTGRES_PASSWORD} for compatibility

# ------------------------------------------------------------------------------
# Code Generation Database Configuration (OPTIONAL)
# ------------------------------------------------------------------------------
# Used by: Node generation pipeline for template cache metrics and session tracking
# Purpose: Persists generation sessions, artifacts, and template cache analytics
# Database: PostgreSQL (omninode_bridge)
#
# NOTE: This is OPTIONAL. If not set, the system will operate with:
# - In-memory template caching only (no database persistence)
# - Generation will work normally, but metrics won't be persisted
# - A single informational message will be logged at startup
#
# Required for:
# - Template cache metrics persistence
# - Generation session tracking
# - Code generation analytics
#
# Fallback behavior:
# - Template cache works normally (in-memory)
# - No repeated warnings (single info message on initialization)
# - All generation features remain functional
#
# To enable database persistence, uncomment and set:
# POSTGRES_PASSWORD=omninode-bridge-postgres-dev-2024
#
# Connection details (shared with hook intelligence):
# - Host: localhost (POSTGRES_HOST)
# - Port: 5436 (POSTGRES_PORT)
# - Database: omninode_bridge (POSTGRES_DB)
# - User: postgres (POSTGRES_USER)

# ------------------------------------------------------------------------------
# Git Hooks Configuration
# ------------------------------------------------------------------------------
# Used by: Git hooks for documentation validation
# Purpose: Enable/disable documentation validation before push
#
# Note: Kafka topic for documentation changes (KAFKA_DOC_TOPIC) is configured
#       in the "Kafka/Redpanda Configuration" section above
GIT_HOOK_VALIDATE_DOCS=false

# ------------------------------------------------------------------------------
# Manifest Cache Configuration
# ------------------------------------------------------------------------------
# Used by: ManifestInjector (agents/lib/manifest_injector.py)
# Purpose: Configurable TTL for pattern/infrastructure/model caching
# Default: 300 seconds (5 minutes)
#
# Per-query-type TTLs (relative to default):
#   - patterns: 3x default (15 minutes) - patterns are relatively static
#   - infrastructure: 2x default (10 minutes) - changes infrequently
#   - models: 3x default (15 minutes) - relatively static
#   - database_schemas: 1x default (5 minutes)
#   - debug_intelligence: 0.5x default (2.5 minutes) - changes frequently
#
# Performance targets:
#   - 60%+ cache hit rate
#   - <5ms cache query time
#   - 30-50% overall query time reduction
MANIFEST_CACHE_TTL_SECONDS=300

# ------------------------------------------------------------------------------
# Valkey Caching Configuration (Phase 1 Optimization)
# ------------------------------------------------------------------------------
# Used by: IntelligenceCache (agents/lib/intelligence_cache.py)
# Purpose: Distributed, persistent caching for intelligence queries
# Service: archon-valkey (Redis-compatible key-value store)
#
# Architecture:
# - Two-tier caching: Valkey (distributed) → In-memory (local)
# - Valkey checked first for cross-process cache hits
# - In-memory cache used for fastest access within same process
# - Results stored in both caches after query
#
# Configuration:
ENABLE_INTELLIGENCE_CACHE=true

# Valkey connection URL
# Default: redis://:archon_cache_2025@archon-valkey:6379/0 (Docker internal hostname)
# For localhost development: redis://:archon_cache_2025@localhost:6379/0
# For production: Change password via VALKEY_URL environment variable
VALKEY_URL=redis://:archon_cache_2025@archon-valkey:6379/0

# Cache TTLs (seconds) - Per-operation-type configuration:
CACHE_TTL_PATTERNS=300        # 5 minutes - pattern discovery results
CACHE_TTL_INFRASTRUCTURE=3600 # 1 hour - infrastructure topology
CACHE_TTL_SCHEMAS=1800        # 30 minutes - database schemas

# Performance Targets (Phase 1):
# - Query time reduction: 7,500ms → 1,500ms (with 60% cache hit rate)
# - Cache hit rate: >60%
# - Cache lookup time: <10ms p95
# - Archon load reduction: 60%+ (fewer duplicate queries)
#
# Troubleshooting:
# - Verify Valkey running: docker ps | grep archon-valkey
# - Test connection: redis-cli -h localhost -p 6379 ping
# - Check keys: redis-cli -h localhost -p 6379 keys "intelligence:*"
# - Monitor stats: redis-cli -h localhost -p 6379 info stats
#
# Graceful Degradation:
# - If Valkey unavailable, falls back to in-memory cache
# - If both caches miss, queries archon-intelligence service
# - Cache failures logged but don't break intelligence queries
#
# Related Documentation:
# - docs/planning/OMNICLAUDE_INTELLIGENCE_OPTIMIZATION_PLAN.md (Phase 1)
# - agents/lib/intelligence_cache.py (implementation)
# - agents/lib/manifest_injector.py (integration)

# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# Development Repository Paths (OPTIONAL)
# ------------------------------------------------------------------------------
# Used by: Code refiner (agents/lib/code_refiner.py) for production pattern discovery
# Purpose: Enables code refinement with production ONEX patterns from sibling repositories
# Default: Automatically resolves to sibling directories (../omniarchon, ../omninode_bridge)
#
# Only set these if:
# - Repositories are not in standard sibling location
# - Using custom directory structure
# - Running in CI/CD with specific paths
#
# If not set, defaults to:
# - OMNIARCHON_PATH: ../omniarchon (relative to omniclaude)
# - OMNINODE_BRIDGE_PATH: ../omninode_bridge (relative to omniclaude)
#
# OMNIARCHON_PATH=/custom/path/to/omniarchon
# OMNINODE_BRIDGE_PATH=/custom/path/to/omninode_bridge

# ------------------------------------------------------------------------------
# Pattern Quality Filtering (Phase 2 Optimization)
# ------------------------------------------------------------------------------
# Used by: ManifestInjector (agents/lib/manifest_injector.py)
# Purpose: Filter low-quality patterns from manifest injection
# Integration: PatternQualityScorer evaluates patterns across 5 dimensions
#
# Scoring Dimensions:
# 1. Code Completeness (0-1.0): Has meaningful code vs stubs
# 2. Documentation Quality (0-1.0): Docstrings, comments, type hints
# 3. ONEX Compliance (0-1.0): Follows ONEX architecture patterns
# 4. Metadata Richness (0-1.0): Use cases, examples, node types
# 5. Complexity Appropriateness (0-1.0): Complexity matches use case
#
# Configuration:
ENABLE_PATTERN_QUALITY_FILTER=false  # Set to 'true' to enable quality filtering (disabled by default)
MIN_PATTERN_QUALITY=0.5              # 0.0-1.0 threshold (0.5 = fair quality minimum)
#
# Quality Thresholds:
# - 0.9+ = Excellent (production-ready, well-documented)
# - 0.7-0.9 = Good (functional, adequately documented)
# - 0.5-0.7 = Fair (usable but needs improvement)
# - <0.5 = Poor (filtered out by default)
#
# Performance Impact:
# - Non-blocking metric storage (asyncio.create_task)
# - Quality scoring adds ~1-2ms per pattern
# - Metrics stored in pattern_quality_metrics table
#
# Related Documentation:
# - docs/planning/OMNICLAUDE_INTELLIGENCE_OPTIMIZATION_PLAN.md (Phase 2)
# - agents/lib/pattern_quality_scorer.py (implementation)
# - agents/lib/manifest_injector.py (integration)

# ------------------------------------------------------------------------------
# PostgreSQL Database Configuration
# ------------------------------------------------------------------------------
# Used by: Claude Code hooks, agent tracking, pattern traceability
# Purpose: Stores hook events, agent detections, tool usage, correlation tracking
# Database: PostgreSQL (omninode_bridge)
#
# REQUIRED: Set these for your environment (no defaults provided)
# Examples:
#   - Local development: POSTGRES_HOST=localhost, POSTGRES_PORT=5436
#   - Remote server: POSTGRES_HOST=192.168.86.200, POSTGRES_PORT=5436
#   - Docker internal: POSTGRES_HOST=omniclaude_postgres, POSTGRES_PORT=5432
#
POSTGRES_HOST=
# Example: POSTGRES_HOST=localhost or POSTGRES_HOST=192.168.86.200

POSTGRES_PORT=
# Example: POSTGRES_PORT=5436

POSTGRES_DB=
# Example: POSTGRES_DB=omninode_bridge

POSTGRES_USER=
# Example: POSTGRES_USER=postgres

POSTGRES_PASSWORD=
# Example: POSTGRES_PASSWORD=your_secure_password_here
# SECURITY WARNING: Change default in production!

# Legacy alias for backward compatibility
DB_PASSWORD=
# Example: DB_PASSWORD=your_secure_password_here (should match POSTGRES_PASSWORD)

# ------------------------------------------------------------------------------
# Service URL Configuration
# ------------------------------------------------------------------------------
# Used by: Debug utilities, health checks, pattern tracking, hook integrations
# Purpose: Configure URLs for various backend services
# Default: localhost URLs for local development
#
# Intelligence Service (archon-intelligence-adapter):
INTELLIGENCE_SERVICE_URL=http://localhost:8053

# Main Archon Server:
MAIN_SERVER_URL=http://localhost:8181

# Archon MCP Server:
ARCHON_MCP_URL=http://localhost:8051

# Qdrant Vector Database:
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_URL=http://localhost:6333

# Production deployment:
# - Set these to actual service hostnames/IPs
# - Example: INTELLIGENCE_SERVICE_URL=http://192.168.86.200:8053
# - Example: QDRANT_HOST=192.168.86.200

# ------------------------------------------------------------------------------
# Kafka Event Bus Configuration
# ------------------------------------------------------------------------------
# Used by: Event-driven intelligence, agent tracking, documentation hooks
# Purpose: Central message broker for distributed intelligence and observability
# Admin UI: http://localhost:8080 (Redpanda Console)
#
# REQUIRED: Set KAFKA_BOOTSTRAP_SERVERS based on deployment environment
# See detailed port selection guide in the "Kafka Bootstrap Servers Configuration" section below (line ~134)
#
# Quick Reference:
# - Docker internal: omninode-bridge-redpanda:9092
# - Host to Docker (production): localhost:29102 or 192.168.86.200:29102
# - Host to Docker (test): localhost:29092
KAFKA_BOOTSTRAP_SERVERS=
# Example: KAFKA_BOOTSTRAP_SERVERS=192.168.86.200:29102 or localhost:29102

# Feature Flags:
KAFKA_ENABLE_INTELLIGENCE=true          # Enable event-based intelligence queries
KAFKA_ENABLE_LOGGING=true               # Enable Kafka event logging (default: true)
ENABLE_EVENT_BASED_DISCOVERY=true       # Enable event-first pattern discovery
ENABLE_FILESYSTEM_FALLBACK=true         # Fallback to filesystem on event failure
PREFER_EVENT_PATTERNS=true              # Prefer event patterns over built-in patterns

# Performance Configuration:
KAFKA_REQUEST_TIMEOUT_MS=5000           # Request timeout (default: 5000ms)
# Target p95 response time: <100ms
# Target success rate: >95% (with fallback)

# Topics:
#
# Intelligence Topics (archon-intelligence service):
#   - dev.archon-intelligence.intelligence.code-analysis-requested.v1
#   - dev.archon-intelligence.intelligence.code-analysis-completed.v1
#   - dev.archon-intelligence.intelligence.code-analysis-failed.v1
#   Purpose: Real-time pattern discovery and intelligence queries
#
# Agent Tracking Topics (agent observability):
#   - agent-routing-decisions: Agent selection and confidence scores
#   - agent-transformation-events: Polymorphic agent transformations
#   - router-performance-metrics: Routing performance analytics
#   - agent-actions: Complete agent tool calls and execution steps
#   Purpose: Complete agent observability with replay capability
#
# Documentation Topics (git hooks):
#   - documentation-changed: Documentation file events (added, updated, deleted)
#   Purpose: Documentation change tracking for downstream processing
KAFKA_DOC_TOPIC=documentation-changed

# ------------------------------------------------------------------------------
# Kafka Bootstrap Servers Configuration (REQUIRED)
# ------------------------------------------------------------------------------
# CRITICAL: You MUST set this variable based on your deployment environment.
# No default value is provided to prevent misconfiguration.
#
# KAFKA_BOOTSTRAP_SERVERS=
#
# Port Selection Guide:
# ---------------------
# Docker Internal Communication (service-to-service):
#   KAFKA_BOOTSTRAP_SERVERS=omninode-bridge-redpanda:9092
#   ↑ Use port 9092 for containers within Docker network
#
# External Access from Host Machine:
#   KAFKA_BOOTSTRAP_SERVERS=localhost:29102
#   KAFKA_BOOTSTRAP_SERVERS=192.168.86.200:29102
#   ↑ Use port 29102 for production external access
#   ↑ Use port 29092 for test environment external access
#
# Legacy Configuration (backward compatibility):
#   KAFKA_INTELLIGENCE_BOOTSTRAP_SERVERS=<same-as-above>
#
# Common Configurations:
# ---------------------
# Local Development (Docker Compose):
#   KAFKA_BOOTSTRAP_SERVERS=omninode-bridge-redpanda:9092
#
# Local Development (Host to Docker):
#   KAFKA_BOOTSTRAP_SERVERS=localhost:29102
#
# Remote Production Server:
#   KAFKA_BOOTSTRAP_SERVERS=192.168.86.200:29102
#
# Port Reference:
# ---------------
# Port 9092  = Internal Docker network communication
# Port 29092 = External host access (test environment)
# Port 29102 = External host access (production environment)
#
# IMPORTANT: Docker port mapping varies by environment:
# - Test:       0.0.0.0:29092->29092 (see docker-compose.test.yml)
# - Production: Host-specific port mapping (often 29102)
#
# Troubleshooting:
# ----------------
# Verify Redpanda running:
#   docker ps | grep redpanda
#
# Check topics:
#   docker exec -it omninode-bridge-redpanda rpk topic list
#
# Test connectivity (adjust port based on your config):
#   kcat -L -b localhost:29102
#   kcat -L -b 192.168.86.200:29102
#
# View messages:
#   kcat -C -b localhost:29102 -t <topic-name>
#
# Requirements:
# -------------
# - Redpanda broker running (docker-compose up redpanda)
# - Event topics auto-created on first publish
# - archon-intelligence service running for intelligence queries (port 8053)
# - omniclaude_agent_consumer running for agent tracking (Docker container)
#
# Related Documentation:
# ----------------------
# - EVENT_INTELLIGENCE_INTEGRATION_PLAN.md - Integration architecture
# - CLIENT_WORKS_EVIDENCE.md - Functionality validation
# - EVENT_INTELLIGENCE_DEVELOPER_GUIDE.md - Developer integration guide
# - AGENT_OBSERVABILITY_SCHEMA.md - Agent tracking schema documentation
# - docs/KAFKA_PORT_CONFIGURATION.md - Detailed port configuration guide

# ------------------------------------------------------------------------------
# Docker Compose Deployment Configuration
# ------------------------------------------------------------------------------
# Used by: deployment/docker-compose.yml for agent-observability-consumer service
# Purpose: Connects to external omninode-bridge PostgreSQL database
# Database: PostgreSQL (omninode_bridge)
#
# REQUIRED: PostgreSQL password for omninode-bridge database
# SECURITY WARNING: MUST be changed in production!
# Production deployment: Set via environment variable or secrets management
OMNINODE_BRIDGE_POSTGRES_PASSWORD=
# Example: OMNINODE_BRIDGE_POSTGRES_PASSWORD=your_secure_password_here

# ------------------------------------------------------------------------------
# Code Generation Database Configuration (OPTIONAL)
# ------------------------------------------------------------------------------
# Used by: Node generation pipeline for template cache metrics and session tracking
# Purpose: Persists generation sessions, artifacts, and template cache analytics
# Database: PostgreSQL (omninode_bridge)
#
# NOTE: This is OPTIONAL. If not set, the system will operate with:
# - In-memory template caching only (no database persistence)
# - Generation will work normally, but metrics won't be persisted
# - A single informational message will be logged at startup
#
# Required for:
# - Template cache metrics persistence
# - Generation session tracking
# - Code generation analytics
#
# Fallback behavior:
# - Template cache works normally (in-memory)
# - No repeated warnings (single info message on initialization)
# - All generation features remain functional
#
# To enable database persistence, uncomment and set:
# POSTGRES_PASSWORD=omninode-bridge-postgres-dev-2024
#
# Connection details (shared with hook intelligence):
# - Host: localhost (POSTGRES_HOST)
# - Port: 5436 (POSTGRES_PORT)
# - Database: omninode_bridge (POSTGRES_DB)
# - User: postgres (POSTGRES_USER)

# ------------------------------------------------------------------------------
# Kafka Configuration (Documentation Hooks)
# ------------------------------------------------------------------------------
# Used by: Git hooks for documentation change tracking
# Purpose: Publish documentation events for downstream processing
# Topics:
#   - documentation-changed: Documentation file events (added, updated, deleted)
#
# NOTE: KAFKA_BOOTSTRAP_SERVERS is defined in "Kafka Event Bus Configuration" section above
# Do not redefine here - use the same value throughout the file
KAFKA_DOC_TOPIC=documentation-changed

# Optional: Enable/disable git hook validation
# Set to 'true' to validate documentation before push
GIT_HOOK_VALIDATE_DOCS=false

# ------------------------------------------------------------------------------
# Manifest Cache Configuration
# ------------------------------------------------------------------------------
# Used by: ManifestInjector (agents/lib/manifest_injector.py)
# Purpose: Configurable TTL for pattern/infrastructure/model caching
# Default: 300 seconds (5 minutes)
#
# Per-query-type TTLs (relative to default):
#   - patterns: 3x default (15 minutes) - patterns are relatively static
#   - infrastructure: 2x default (10 minutes) - changes infrequently
#   - models: 3x default (15 minutes) - relatively static
#   - database_schemas: 1x default (5 minutes)
#   - debug_intelligence: 0.5x default (2.5 minutes) - changes frequently
#
# Performance targets:
#   - 60%+ cache hit rate
#   - <5ms cache query time
#   - 30-50% overall query time reduction
MANIFEST_CACHE_TTL_SECONDS=300

# ------------------------------------------------------------------------------
# Valkey Caching Configuration (Phase 1 Optimization)
# ------------------------------------------------------------------------------
# Used by: IntelligenceCache (agents/lib/intelligence_cache.py)
# Purpose: Distributed, persistent caching for intelligence queries
# Service: archon-valkey (Redis-compatible key-value store)
#
# Architecture:
# - Two-tier caching: Valkey (distributed) → In-memory (local)
# - Valkey checked first for cross-process cache hits
# - In-memory cache used for fastest access within same process
# - Results stored in both caches after query
#
# Configuration:
ENABLE_INTELLIGENCE_CACHE=true
VALKEY_URL=redis://:archon_cache_2025@archon-valkey:6379/0

# Cache TTLs (seconds) - Per-operation-type configuration:
CACHE_TTL_PATTERNS=300        # 5 minutes - pattern discovery results
CACHE_TTL_INFRASTRUCTURE=3600 # 1 hour - infrastructure topology
CACHE_TTL_SCHEMAS=1800        # 30 minutes - database schemas

# Performance Targets (Phase 1):
# - Query time reduction: 7,500ms → 1,500ms (with 60% cache hit rate)
# - Cache hit rate: >60%
# - Cache lookup time: <10ms p95
# - Archon load reduction: 60%+ (fewer duplicate queries)
#
# Troubleshooting:
# - Verify Valkey running: docker ps | grep archon-valkey
# - Test connection: redis-cli -h localhost -p 6379 ping
# - Check keys: redis-cli -h localhost -p 6379 keys "intelligence:*"
# - Monitor stats: redis-cli -h localhost -p 6379 info stats
#
# Graceful Degradation:
# - If Valkey unavailable, falls back to in-memory cache
# - If both caches miss, queries archon-intelligence service
# - Cache failures logged but don't break intelligence queries
#
# Related Documentation:
# - docs/planning/OMNICLAUDE_INTELLIGENCE_OPTIMIZATION_PLAN.md (Phase 1)
# - agents/lib/intelligence_cache.py (implementation)
# - agents/lib/manifest_injector.py (integration)

# ------------------------------------------------------------------------------
# Kafka Configuration (Event-Based Intelligence)
# ------------------------------------------------------------------------------
# Used by: IntelligenceEventClient (agents/lib/intelligence_event_client.py)
# Purpose: Event-driven pattern discovery from omniarchon intelligence service
# Integration: Replaces hard-coded filesystem paths with dynamic discovery
#
# Configuration:
# NOTE: KAFKA_INTELLIGENCE_BOOTSTRAP_SERVERS uses same value as KAFKA_BOOTSTRAP_SERVERS
# See "Kafka Bootstrap Servers Configuration" section above for port selection guide
# Do not redefine here - use the same value throughout the file
KAFKA_INTELLIGENCE_BOOTSTRAP_SERVERS=
# Example: KAFKA_INTELLIGENCE_BOOTSTRAP_SERVERS=192.168.86.200:29102 (should match KAFKA_BOOTSTRAP_SERVERS)

KAFKA_ENABLE_INTELLIGENCE=true
ENABLE_EVENT_BASED_DISCOVERY=true
ENABLE_FILESYSTEM_FALLBACK=true
PREFER_EVENT_PATTERNS=true
KAFKA_REQUEST_TIMEOUT_MS=5000

# Event Topics (ONEX architecture):
#   - dev.archon-intelligence.intelligence.code-analysis-requested.v1
#   - dev.archon-intelligence.intelligence.code-analysis-completed.v1
#   - dev.archon-intelligence.intelligence.code-analysis-failed.v1

# Port Mapping:
#   - External (host): localhost:9092 or 192.168.86.200:9092
#   - Docker internal: omninode-bridge-redpanda:9092
#   - Redpanda Admin UI: http://localhost:8080

# Requirements:
#   - omniarchon intelligence service running (archon-intelligence on port 8053)
#   - Redpanda broker running (docker-compose up redpanda)
#   - Event topics created (auto-created on first publish)

# Feature Flags:
#   - KAFKA_ENABLE_INTELLIGENCE: Master switch for event-based intelligence
#   - ENABLE_EVENT_BASED_DISCOVERY: Enable event-first pattern discovery
#   - ENABLE_FILESYSTEM_FALLBACK: Fallback to filesystem on event failure
#   - PREFER_EVENT_PATTERNS: Prefer event patterns over built-in patterns

# Performance:
#   - Request timeout: 5000ms default (configurable)
#   - Target p95: <100ms response time
#   - Success rate: >95% (with fallback to filesystem)

# Troubleshooting:
#   - Verify Redpanda running: docker ps | grep redpanda
#   - Check topics: docker exec -it <redpanda-container> rpk topic list
#   - Test connectivity: poetry run python -c "from agents.lib.intelligence_event_client import IntelligenceEventClient; import asyncio; asyncio.run(IntelligenceEventClient().start())"
#   - See CLIENT_WORKS_EVIDENCE.md for validation evidence

# Related Documentation:
#   - EVENT_INTELLIGENCE_INTEGRATION_PLAN.md - Integration architecture
#   - CLIENT_WORKS_EVIDENCE.md - Functionality validation
#   - EVENT_INTELLIGENCE_DEVELOPER_GUIDE.md - Developer integration guide

# ------------------------------------------------------------------------------
# Development Repository Paths (OPTIONAL)
# ------------------------------------------------------------------------------
# Used by: Code refiner (agents/lib/code_refiner.py) for production pattern discovery
# Purpose: Enables code refinement with production ONEX patterns from sibling repositories
# Default: Automatically resolves to sibling directories (../omniarchon, ../omninode_bridge)
#
# Only set these if:
# - Repositories are not in standard sibling location
# - Using custom directory structure
# - Running in CI/CD with specific paths
#
# If not set, defaults to:
# - OMNIARCHON_PATH: ../omniarchon (relative to omniclaude)
# - OMNINODE_BRIDGE_PATH: ../omninode_bridge (relative to omniclaude)
#
# OMNIARCHON_PATH=/custom/path/to/omniarchon
# OMNINODE_BRIDGE_PATH=/custom/path/to/omninode_bridge

# ------------------------------------------------------------------------------
# Security Best Practices
# ------------------------------------------------------------------------------
# 1. Never commit .env files to version control
# 2. Rotate keys every 30-90 days (see SECURITY_KEY_ROTATION.md)
# 3. Use separate keys for development and production
# 4. Enable IP restrictions in provider dashboards
# 5. Set usage quotas to limit damage from leaks
# 6. Monitor API usage regularly
#
# See SECURITY_KEY_ROTATION.md for:
# - How to obtain API keys
# - Step-by-step rotation procedures
# - Testing new keys
# - Troubleshooting common issues
# ------------------------------------------------------------------------------
