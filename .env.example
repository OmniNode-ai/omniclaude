# OmniClaude Environment Configuration
# ==============================================================================
# IMPORTANT: Copy this file to .env and replace placeholder values with actual values
# Never commit .env to version control!
#
# Setup Instructions:
# 1. cp .env.example .env
# 2. Edit .env with your actual API keys and configuration
# 3. source .env (or use direnv)
# 4. Verify: echo $GEMINI_API_KEY
#
# See SECURITY_KEY_ROTATION.md for detailed setup and rotation procedures
# ==============================================================================

# ==============================================================================
# ENVIRONMENT CONFIGURATION
# ==============================================================================
# Environment type: development, test, or production
#
# SECURITY NOTE: This setting affects service binding addresses for security:
# - development: Services bind to 0.0.0.0 (all interfaces) for Docker/debugging
# - production/test: Services bind to 127.0.0.1 (localhost only) for security
#
# To override host binding, set: ROUTING_ADAPTER_HOST=<your-ip>
ENVIRONMENT=development

# Enable debug logging and verbose output
# Set to true for detailed debugging information
DEBUG=false

# Container naming prefix for Docker services
CONTAINER_PREFIX=omniclaude

# ==============================================================================
# EXTERNAL SERVICE DISCOVERY (from omniarchon)
# ==============================================================================
# These services are provided by the omniarchon repository
# Default: Services running on 192.168.86.101
# Update these URLs if services are deployed elsewhere
#
# Archon Intelligence API - Code quality, pattern discovery, RAG queries
ARCHON_INTELLIGENCE_URL=http://192.168.86.101:8053

# Archon Search API - Vector search, semantic search
ARCHON_SEARCH_URL=http://192.168.86.101:8055

# Archon Bridge API - Bridge services between systems
ARCHON_BRIDGE_URL=http://192.168.86.101:8054

# Archon MCP Server - Model Context Protocol server
ARCHON_MCP_URL=http://192.168.86.101:8051

# Legacy alias for intelligence service (for backward compatibility)
INTELLIGENCE_SERVICE_URL=${ARCHON_INTELLIGENCE_URL}

# Archon Server (if different from intelligence)
MAIN_SERVER_URL=http://192.168.86.101:8181

# ==============================================================================
# SHARED INFRASTRUCTURE (from omninode_bridge)
# ==============================================================================
# These services are provided by the omninode_bridge repository
# Default: Infrastructure running on 192.168.86.200
#
# ------------------------------------------------------------------------------
# Kafka/Redpanda Event Bus
# ------------------------------------------------------------------------------
# Central message broker for distributed intelligence and observability
# Admin UI: http://192.168.86.200:8080 (Redpanda Console)
#
# IMPORTANT: Port selection depends on your deployment context
#
# For Docker services (containers in docker-compose):
#   Use hostname 'omninode-bridge-redpanda' with port 9092
#   KAFKA_BOOTSTRAP_SERVERS=omninode-bridge-redpanda:9092
#
# For host scripts (running on your local machine):
#   Use internal Docker network hostname with port 9092
KAFKA_BOOTSTRAP_SERVERS=omninode-bridge-redpanda:9092
#
# For remote server (SSH'd into 192.168.86.200):
#   KAFKA_BOOTSTRAP_SERVERS=localhost:29092
#
# Topics Used:
#   Intelligence: dev.archon-intelligence.intelligence.*
#   Agent Tracking: agent-routing-decisions, agent-transformation-events
#   Documentation: documentation-changed

# Legacy alias for backward compatibility
KAFKA_INTELLIGENCE_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}

# Feature Flags
KAFKA_ENABLE_INTELLIGENCE=true          # Enable event-based intelligence queries
KAFKA_ENABLE_LOGGING=true               # Enable Kafka event logging
KAFKA_ENABLE_LOGGING_EVENTS=true        # Enable logging event publishing to Kafka
ENABLE_EVENT_BASED_DISCOVERY=true       # Enable event-first pattern discovery
ENABLE_FILESYSTEM_FALLBACK=true         # Fallback to filesystem on event failure
PREFER_EVENT_PATTERNS=true              # Prefer event patterns over built-in patterns

# Performance Configuration
KAFKA_REQUEST_TIMEOUT_MS=5000           # Request timeout in milliseconds

# Documentation change tracking topic
KAFKA_DOC_TOPIC=documentation-changed

# ------------------------------------------------------------------------------
# PostgreSQL Database - Local Application
# ------------------------------------------------------------------------------
# Local containerized PostgreSQL for application-specific data
# Used by: app, postgres services in docker-compose.yml
# Database: omniclaude (local, containerized)
#
APP_POSTGRES_HOST=postgres
APP_POSTGRES_PORT=5432
APP_POSTGRES_INTERNAL_PORT=5432
APP_POSTGRES_DATABASE=omniclaude
APP_POSTGRES_USER=omniclaude
APP_POSTGRES_PASSWORD=  # REQUIRED: Set your local app database password

# ------------------------------------------------------------------------------
# PostgreSQL Database - Shared Bridge
# ------------------------------------------------------------------------------
# Shared database for agent tracking, pattern storage, observability
# Used by: routing-adapter, agent-observability-consumer, archon-router-consumer
# Database: omninode_bridge (34+ tables)
#
# Connection details for external access from host:
POSTGRES_HOST=192.168.86.200
POSTGRES_PORT=5436
POSTGRES_DATABASE=omninode_bridge
POSTGRES_USER=postgres
POSTGRES_PASSWORD=  # REQUIRED: Set your actual password in .env (NEVER commit)

# ------------------------------------------------------------------------------
# Password Aliases (DEPRECATED - DO NOT USE)
# ------------------------------------------------------------------------------
# ⚠️ CRITICAL DEPRECATION NOTICE ⚠️
#
# These password aliases are DEPRECATED and will be REMOVED in v2.0 (Q2 2025).
# All code has been migrated to use POSTGRES_PASSWORD exclusively.
#
# DO NOT USE THESE VARIABLES:
#   ❌ POSTGRES_DB (use POSTGRES_DATABASE instead)
#   ❌ DB_PASSWORD (use POSTGRES_PASSWORD instead)
#   ❌ OMNINODE_BRIDGE_POSTGRES_PASSWORD (use POSTGRES_PASSWORD instead)
#
# Why This Matters:
#   - Password rotation is error-prone with multiple aliases
#   - Security audits are more complex with inconsistent naming
#   - Production misconfigurations are more likely with multiple variables
#
# Migration Status (as of 2025-11-08):
#   ✅ All Python files migrated (6 files)
#   ✅ All shell scripts migrated (16 files)
#   ✅ Deprecation warnings added to config/settings.py
#   ✅ Migration guide created: PASSWORD_ALIAS_MIGRATION.md
#
# If You See These Variables in Code:
#   1. File an issue - all code should be migrated
#   2. See PASSWORD_ALIAS_MIGRATION.md for migration guide
#   3. DO NOT add new code using these aliases
#
# For Backward Compatibility (Temporary):
#
# PASSWORD ALIASES (deprecated for consistency and security):
# These aliases point to POSTGRES_PASSWORD but emit warnings when used.
# They will be REMOVED in v2.0 without further notice.
#
DB_PASSWORD=${POSTGRES_PASSWORD}                              # DEPRECATED - Remove in v2.0
OMNINODE_BRIDGE_POSTGRES_PASSWORD=${POSTGRES_PASSWORD}        # DEPRECATED - Remove in v2.0
#
# DATABASE NAME ALIASES (deprecated for consistency):
# This alias points to POSTGRES_DATABASE but emits warnings when used.
# It will be REMOVED in v2.0 without further notice.
#
POSTGRES_DB=${POSTGRES_DATABASE}                              # DEPRECATED - Remove in v2.0

# SECURITY WARNING: Change default password in production!
#
# For Docker services (containers in docker-compose):
#   POSTGRES_HOST=omninode-bridge-postgres
#   POSTGRES_PORT=5432
#
# Tables include:
#   - agent_routing_decisions: Agent selection and confidence scores
#   - agent_manifest_injections: Complete manifest snapshots
#   - agent_execution_logs: Execution lifecycle tracking
#   - pattern_quality_metrics: Pattern quality scores
#   - And 30+ more observability tables

# ------------------------------------------------------------------------------
# PostgreSQL Connection Pool Configuration
# ------------------------------------------------------------------------------
# Used by: skills/_shared/db_helper.py (psycopg2 SimpleConnectionPool)
# Purpose: Manage database connection pooling for efficiency and resource control
#
# Configuration:
#   - Minimum connections (minconn): Always maintained, even when idle
#   - Maximum connections (maxconn): Upper limit for concurrent connections
#
# Sizing Guidelines:
#   Development (low load):
#     POSTGRES_POOL_MIN_SIZE=1   # Minimal resource usage
#     POSTGRES_POOL_MAX_SIZE=5   # Adequate for single-user development
#
#   Production (moderate load):
#     POSTGRES_POOL_MIN_SIZE=2   # Quick connection availability
#     POSTGRES_POOL_MAX_SIZE=10  # Handle concurrent requests
#
#   Production (high load):
#     POSTGRES_POOL_MIN_SIZE=5   # Pre-warmed connections
#     POSTGRES_POOL_MAX_SIZE=20  # Scale with traffic (monitor pg_stat_activity)
#
# Performance Considerations:
#   - Too low: Connection exhaustion under load (requests wait/fail)
#   - Too high: Excessive memory usage, database connection limits
#   - Monitor: PostgreSQL's max_connections setting (default: 100)
#   - Rule of thumb: max_connections / number_of_services = max pool size
#
# Validation:
#   - Both must be >= 1 and <= 100
#   - max_size must be >= min_size (enforced by Pydantic validator)
#
POSTGRES_POOL_MIN_SIZE=1  # Default: 1 (development)
POSTGRES_POOL_MAX_SIZE=5  # Default: 5 (development)

# Troubleshooting:
#   Pool exhaustion: "OperationalError: connection pool exhausted"
#   - Increase POSTGRES_POOL_MAX_SIZE
#   - Check for connection leaks (ensure release_connection() is called)
#   - Monitor active connections: SELECT count(*) FROM pg_stat_activity;

# ==============================================================================
# AI PROVIDER API KEYS
# ==============================================================================
#
# ------------------------------------------------------------------------------
# Google Gemini API Configuration
# ------------------------------------------------------------------------------
# Used by: Multi-provider support, AI quorum validation
# Get your key: https://console.cloud.google.com/apis/credentials
# Required for: Gemini Pro, Gemini Flash, Gemini 2.5 Flash providers
# Permissions: Enable "Generative Language API"
# Rate Limits: Check Google Cloud Console for your tier
GEMINI_API_KEY=  # REQUIRED: Set your actual API key in .env

# Google API Key (Pydantic AI Integration)
# Note: This should be the same as GEMINI_API_KEY for Pydantic AI compatibility
GOOGLE_API_KEY=${GEMINI_API_KEY}

# ------------------------------------------------------------------------------
# Z.ai API Configuration
# ------------------------------------------------------------------------------
# Used by: GLM model support, high-concurrency operations
# Get your key: https://z.ai/dashboard
# Required for: Z.ai provider (GLM-4.5-Air, GLM-4.5, GLM-4.6)
# Rate Limits:
#   - GLM-4.5-Air: 5 concurrent requests
#   - GLM-4.5: 20 concurrent requests
#   - GLM-4.6: 10 concurrent requests
ZAI_API_KEY=  # REQUIRED: Set your actual API key in .env

# ==============================================================================
# INFRASTRUCTURE ADDRESSES
# ==============================================================================
#
# ------------------------------------------------------------------------------
# Remote Infrastructure Configuration
# ------------------------------------------------------------------------------
# REQUIRED: IP address of remote infrastructure server
# Used by: docker-compose extra_hosts for DNS resolution
# Purpose: Enables deployment in different network environments
#
# This IP is used for Docker DNS resolution to shared infrastructure services:
#   - omninode-bridge-redpanda (Kafka/Redpanda)
#   - omninode-bridge-postgres (PostgreSQL)
#   - omninode-bridge-consul (Consul)
#
# Update this value based on your environment:
#   - Development: 192.168.86.200 (default infrastructure server)
#   - Cloud deployment: Your infrastructure server's IP address
#   - CI/CD: Environment-specific IP address
#
# Without this variable set, docker-compose will fail to start services that
# require remote infrastructure connectivity (agent-observability-consumer,
# archon-router-consumer).
REMOTE_INFRASTRUCTURE_IP=192.168.86.200

# vLLM GPU Inference Server (RTX 5090)
# Used by: AI consensus quorum, high-performance inference
VLLM_ENDPOINT=http://192.168.86.201:8001

# Kafka default broker (final fallback in kafka_config.py)
# Used by: Kafka connection fallback when KAFKA_BOOTSTRAP_SERVERS not set
KAFKA_DEFAULT_BROKER=192.168.86.200:29102

# ==============================================================================
# ALERTING & NOTIFICATIONS
# ==============================================================================
#
# ------------------------------------------------------------------------------
# Slack Webhook Configuration
# ------------------------------------------------------------------------------
# Used by: SlackNotifier (agents/lib/slack_notifier.py)
# Purpose: Fail-fast error notifications with throttling
# Get your webhook: https://api.slack.com/messaging/webhooks
#
# Setup Instructions:
#   1. Go to https://api.slack.com/apps
#   2. Create a new app or select existing app
#   3. Enable "Incoming Webhooks"
#   4. Add webhook to your desired channel
#   5. Copy webhook URL and paste below
#
# Features:
#   - Opt-in: Only sends if URL is configured (leave empty to disable)
#   - Throttling: Max 1 notification per error type per throttle window
#   - Non-blocking: Notification failures don't break main flow
#   - Rich context: Error type, message, stack trace, service name, timestamp
#
# Integration Points:
#   - Kafka connection failures (routing_event_client)
#   - Routing failures (agent_router_event_service)
#   - Configuration validation failures (config/settings.py)
#   - Database connection errors
#
SLACK_WEBHOOK_URL=  # OPTIONAL: Set to enable Slack notifications (leave empty to disable)

# Throttle window in seconds (default: 300 = 5 minutes)
# Max 1 notification per error type per window to prevent spam
# Set to 0 to disable throttling (not recommended)
SLACK_NOTIFICATION_THROTTLE_SECONDS=300

# ==============================================================================
# LOCAL SERVICES CONFIGURATION
# ==============================================================================
# Services running locally on your development machine
#
# ------------------------------------------------------------------------------
# Qdrant Vector Database
# ------------------------------------------------------------------------------
# Local vector database for pattern storage and semantic search
# Collections: code_patterns, execution_patterns, workflow_events
#
# Configuration Options:
# 1. Full URL (recommended for production HTTPS):
#    QDRANT_URL=https://qdrant.internal:6333
#
# 2. Host + Port (protocol auto-selected based on ENVIRONMENT):
#    QDRANT_HOST=localhost
#    QDRANT_PORT=6333
#    QDRANT_URL=http://localhost:6333  # Development: http://
#                                       # Production:  https://
#
# URL Resolution Priority:
# - If QDRANT_URL contains http:// or https://, use it directly
# - Otherwise, construct from HOST+PORT with protocol based on ENVIRONMENT
#
# Production HTTPS Setup:
# - Set QDRANT_URL=https://your-qdrant-host:6333
# - Ensure TLS certificate is valid
# - Set ENVIRONMENT=production (enforces HTTPS validation)
#
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_URL=http://localhost:6333

# Health check: curl http://localhost:6333/collections
# Production: curl https://qdrant.internal:6333/collections

# ------------------------------------------------------------------------------
# GTE-Qwen2 Embedding Service
# ------------------------------------------------------------------------------
# Text embedding service for semantic search and pattern matching
# Model: Alibaba-NLP/gte-Qwen2-1.5B-instruct (1536 dimensions)
# Required for: Pattern discovery, semantic search in Qdrant
#
# Service Details:
#   - Port: 8002
#   - VRAM: ~6 GB
#   - Endpoint: OpenAI-compatible API (/v1/embeddings)
#
# NO FALLBACKS: If this service is unavailable, embedding-based queries will fail loudly
EMBEDDING_SERVICE_URL=http://192.168.86.201:8002/v1/embeddings

# Health check: curl http://192.168.86.201:8002/v1/models

# ------------------------------------------------------------------------------
# Valkey Caching (Redis-compatible)
# ------------------------------------------------------------------------------
# Used by: IntelligenceCache (agents/lib/intelligence_cache.py)
# Purpose: Distributed, persistent caching for intelligence queries
# Service: archon-valkey (Redis-compatible key-value store)
#
# Architecture:
#   - Two-tier caching: Valkey (distributed) → In-memory (local)
#   - Valkey checked first for cross-process cache hits
#   - Results stored in both caches after query
#
ENABLE_INTELLIGENCE_CACHE=true

# ------------------------------------------------------------------------------
# Valkey/Redis Authentication
# ------------------------------------------------------------------------------
# Choose configuration based on your environment:
#
# DEVELOPMENT (local trusted network):
#   VALKEY_PASSWORD=  # Empty = no authentication
#   ✓ Acceptable in isolated dev environment with Docker network isolation
#   ✓ Simplifies local debugging and development workflow
#   ✗ NEVER use this in production or exposed environments
#
# PRODUCTION (exposed environment):
#   VALKEY_PASSWORD=your-strong-password-here  # REQUIRED for security
#   ✓ Minimum 16 characters (alphanumeric + special chars)
#   ✓ Change default passwords immediately
#   ✓ Never expose port 6379 outside Docker network without auth
#
# Set according to your environment:
VALKEY_PASSWORD=  # Empty for dev, strong password for production

# Valkey Connection URL
# Automatically uses VALKEY_PASSWORD if set, empty password if not
# Default behavior:
#   Development: redis://:@archon-valkey:6379/0 (no password, VALKEY_PASSWORD empty)
#   Production: redis://:${VALKEY_PASSWORD}@archon-valkey:6379/0 (with password)
#
VALKEY_URL=redis://:${VALKEY_PASSWORD:-}@archon-valkey:6379/0
#
# Alternative configurations:
#   For localhost development (outside Docker):
#     VALKEY_URL=redis://:${VALKEY_PASSWORD:-}@localhost:6379/0
#   For different database number:
#     VALKEY_URL=redis://:${VALKEY_PASSWORD:-}@archon-valkey:6379/1

# Cache TTLs (seconds) - Per-operation-type configuration:
CACHE_TTL_PATTERNS=300        # 5 minutes - pattern discovery results
CACHE_TTL_INFRASTRUCTURE=3600 # 1 hour - infrastructure topology
CACHE_TTL_SCHEMAS=1800        # 30 minutes - database schemas

# Performance Targets:
#   - Query time reduction: 7,500ms → 1,500ms (with 60% cache hit rate)
#   - Cache hit rate: >60%
#   - Cache lookup time: <10ms p95

# Troubleshooting:
#   docker ps | grep archon-valkey
#   redis-cli -h localhost -p 6379 ping
#   redis-cli -h localhost -p 6379 keys "intelligence:*"

# ==============================================================================
# FEATURE FLAGS & OPTIMIZATION
# ==============================================================================
#
# ------------------------------------------------------------------------------
# Manifest Cache Configuration
# ------------------------------------------------------------------------------
# Used by: ManifestInjector (agents/lib/manifest_injector.py)
# Purpose: Configurable TTL for pattern/infrastructure/model caching
# Default: 300 seconds (5 minutes)
#
# Per-query-type TTLs (relative to default):
#   - patterns: 3x default (15 minutes) - patterns are relatively static
#   - infrastructure: 2x default (10 minutes) - changes infrequently
#   - models: 3x default (15 minutes) - relatively static
#   - database_schemas: 1x default (5 minutes)
#   - debug_intelligence: 0.5x default (2.5 minutes) - changes frequently
MANIFEST_CACHE_TTL_SECONDS=300

# ------------------------------------------------------------------------------
# Pattern Quality Filtering (Phase 2 Optimization)
# ------------------------------------------------------------------------------
# Used by: ManifestInjector (agents/lib/manifest_injector.py)
# Purpose: Filter low-quality patterns from manifest injection
# Integration: PatternQualityScorer evaluates patterns across 5 dimensions
#
# Scoring Dimensions:
# 1. Code Completeness (0-1.0): Has meaningful code vs stubs
# 2. Documentation Quality (0-1.0): Docstrings, comments, type hints
# 3. ONEX Compliance (0-1.0): Follows ONEX architecture patterns
# 4. Metadata Richness (0-1.0): Use cases, examples, node types
# 5. Complexity Appropriateness (0-1.0): Complexity matches use case
#
ENABLE_PATTERN_QUALITY_FILTER=false  # Set to 'true' to enable quality filtering
MIN_PATTERN_QUALITY=0.5              # 0.0-1.0 threshold (0.5 = fair quality minimum)

# Quality Thresholds:
#   0.9+ = Excellent (production-ready, well-documented)
#   0.7-0.9 = Good (functional, adequately documented)
#   0.5-0.7 = Fair (usable but needs improvement)
#   <0.5 = Poor (filtered out by default)

# ==============================================================================
# CODE GENERATION CONFIGURATION
# ==============================================================================
#
# ------------------------------------------------------------------------------
# Code Generation Service
# ------------------------------------------------------------------------------
# Used by: CodeGenerator (agents/parallel_execution/agent_code_generator.py)
# Purpose: Configure automatic code generation behavior and quality gates
#
# Kafka consumer group for code generation event processing
CODEGEN_CONSUMER_GROUP=omniclaude-codegen

# ------------------------------------------------------------------------------
# Generation Control Flags
# ------------------------------------------------------------------------------
# Enable/disable generation of specific code artifacts
# All flags default to true except CODEGEN_GENERATE_BUSINESS_LOGIC
#
CODEGEN_GENERATE_CONTRACTS=true        # Generate Pydantic contract models
CODEGEN_GENERATE_MODELS=true           # Generate data models
CODEGEN_GENERATE_ENUMS=true            # Generate enum types
CODEGEN_GENERATE_BUSINESS_LOGIC=false  # Generate business logic (starts with stubs)
CODEGEN_GENERATE_TESTS=true            # Generate unit tests

# ------------------------------------------------------------------------------
# Quality Gates
# ------------------------------------------------------------------------------
# Quality thresholds for generated code validation
# Values: 0.0-1.0 (0.0 = no requirements, 1.0 = perfect)
#
CODEGEN_QUALITY_THRESHOLD=0.8              # Minimum code quality threshold
CODEGEN_ONEX_COMPLIANCE_THRESHOLD=0.7      # Minimum ONEX compliance threshold
CODEGEN_REQUIRE_HUMAN_REVIEW=true          # Require human review before deployment

# ------------------------------------------------------------------------------
# Intelligence Timeouts
# ------------------------------------------------------------------------------
# Timeouts for AI-powered analysis and validation (in seconds)
#
CODEGEN_ANALYSIS_TIMEOUT_SECONDS=30        # Code analysis timeout
CODEGEN_VALIDATION_TIMEOUT_SECONDS=20      # Validation timeout

# Action logging timeout (for Kafka event publishing via subprocess)
# Default: 10 seconds (increased from 2s to handle slow Kafka connections)
ACTION_LOG_TIMEOUT_SECONDS=10              # Timeout for action logging subprocess calls

# ------------------------------------------------------------------------------
# Mixin Configuration
# ------------------------------------------------------------------------------
# Control automatic selection of code mixins/patterns
#
CODEGEN_AUTO_SELECT_MIXINS=true              # Auto-select appropriate mixins
CODEGEN_MIXIN_CONFIDENCE_THRESHOLD=0.7       # Minimum mixin confidence (0.0-1.0)

# ==============================================================================
# AGENT ROUTING CONFIGURATION
# ==============================================================================
#
# ------------------------------------------------------------------------------
# Kafka Consumer Configuration
# ------------------------------------------------------------------------------
# Consumer group ID for agent router event consumer
KAFKA_GROUP_ID=omniclaude-agent-router-consumer-group

# ------------------------------------------------------------------------------
# Agent Registry Configuration
# ------------------------------------------------------------------------------
# Path to agent registry file
# Default: ~/.claude/agent-definitions/agent-registry.yaml
# Note: Docker containers use REGISTRY_PATH=/agent-definitions/agent-registry.yaml
# For local development, you can override with:
# AGENT_REGISTRY_PATH=~/.claude/agent-definitions/agent-registry.yaml

# Path to OmniClaude agent framework directory
# Default: <project_root>/agents/parallel_execution
# Used by: agent_invoker.py, workflow_executor.py
# OMNICLAUDE_AGENTS_PATH=/path/to/agents/parallel_execution

# ------------------------------------------------------------------------------
# Feature Flags
# ------------------------------------------------------------------------------
# Enable event-based routing via Kafka (vs HTTP)
# Default: true
USE_EVENT_ROUTING=true

# Enable debug logging for agent actions, tool calls, errors, and successes
# When enabled, all agent actions are logged to:
#   - Kafka topic: agent-actions
#   - PostgreSQL table: agent_actions (via Kafka consumer)
# Default: true (enables comprehensive observability)
# Set to false in production for reduced overhead
DEBUG=true

# ==============================================================================
# OPTIONAL CONFIGURATION
# ==============================================================================
#
# ------------------------------------------------------------------------------
# Development Repository Paths
# ------------------------------------------------------------------------------
# Used by: Code refiner (agents/lib/code_refiner.py)
# Purpose: Production pattern discovery from sibling repositories
# Default: Automatically resolves to ../omniarchon, ../omninode_bridge
#
# Only set these if repositories are not in standard sibling location:
# OMNIARCHON_PATH=/custom/path/to/omniarchon
# OMNINODE_BRIDGE_PATH=/custom/path/to/omninode_bridge

# ------------------------------------------------------------------------------
# Security Configuration
# ------------------------------------------------------------------------------
# Secret key for application security (JWT, session encryption, etc.)
# Generate with: openssl rand -base64 32
SECRET_KEY=  # REQUIRED: Set a strong secret key (minimum 32 characters)

# Grafana admin password for monitoring dashboard
# Default username: admin
# SECURITY: Change this immediately in production!
GRAFANA_ADMIN_PASSWORD=  # REQUIRED: Set a strong password

# ------------------------------------------------------------------------------
# Git Hooks Configuration
# ------------------------------------------------------------------------------
# Used by: Git hooks for documentation validation
# Purpose: Enable/disable documentation validation before push
GIT_HOOK_VALIDATE_DOCS=false

# ==============================================================================
# SECURITY BEST PRACTICES
# ==============================================================================
# 1. Never commit .env files to version control
# 2. Rotate keys every 30-90 days (see SECURITY_KEY_ROTATION.md)
# 3. Use separate keys for development and production
# 4. Enable IP restrictions in provider dashboards
# 5. Set usage quotas to limit damage from leaks
# 6. Monitor API usage regularly
# 7. Change ALL default passwords immediately in production
# 8. Use environment variables for all sensitive configuration
#
# See SECURITY_KEY_ROTATION.md for:
# - How to obtain API keys from provider dashboards
# - Step-by-step rotation procedures
# - Testing new keys
# - Troubleshooting common issues
# ==============================================================================

# ==============================================================================
# TROUBLESHOOTING
# ==============================================================================
# Health Check:
#   ./scripts/health_check.sh
#
# Verify Services:
#   curl http://192.168.86.101:8053/health  # Archon Intelligence
#   curl http://localhost:6333/collections  # Qdrant
#
# Database Connection (ALWAYS source .env first):
#   source .env
#   psql -h ${POSTGRES_HOST} -p ${POSTGRES_PORT} -U ${POSTGRES_USER} -d ${POSTGRES_DATABASE}
#
# Kafka Topics:
#   docker exec -it omninode-bridge-redpanda rpk topic list
#
# Service Logs:
#   docker logs -f archon-intelligence
#   docker logs -f omninode-bridge-redpanda
#
# See CLAUDE.md for complete troubleshooting guide
# ==============================================================================
