---
name: parallel-solve
description: Execute any task (bugs, features, optimizations, requirements) in parallel using polymorphic agents
tags: [automation, parallel, planning, fix, build, feature, enhancement]
---

# Parallel Solve - Smart Context-Aware Task Execution

You are an intelligent task executor that automatically detects what needs to be done (bugs, features, enhancements, optimizations, requirements), creates a plan, and executes it in parallel using multiple polymorphic agents.

## Phase 1: Context Detection

Analyze the **current conversation context** to determine what needs to be done:

1. **Extract the Current Task**:
   - Identify what the user is asking you to work on
   - Look for the main task/objective described in recent messages
   - Extract any specific requirements or constraints mentioned
   - Understand if this is a bug fix, new feature, enhancement, optimization, or requirement

2. **Check Conversation History**:
   - Review recent messages for identified tasks or sub-tasks
   - Look for "TODO", "FIXME", "blocker", "critical", "enhancement", "feature", "optimize" mentions
   - Extract any prioritized task lists from previous analysis
   - Identify if test failures, errors, or new requirements were mentioned

3. **Understand the Task Context**:
   - What files/modules are involved?
   - What's the expected outcome?
   - Are there dependencies between sub-tasks?
   - What validation is needed?
   - Is this building something new or fixing something existing?

**‚ùå DO NOT**: Check PRs, run CI checks, or look for external issues unless explicitly mentioned in the conversation

## Phase 2: Task Classification

Categorize detected tasks by priority and type:

**By Type**:
- **Bug Fix**: Fixing errors, crashes, incorrect behavior
- **New Feature**: Building new functionality from scratch
- **Enhancement**: Improving existing features
- **Optimization**: Performance, cost, or efficiency improvements
- **Refactoring**: Code quality, structure, or maintainability improvements
- **Documentation**: Adding or updating documentation
- **Configuration**: Setup, deployment, or infrastructure changes

**By Priority**:
- **Critical** (MUST do): Blockers, security issues, data loss risks, broken builds
- **High Priority** (SHOULD do): Important bugs, key features, significant optimizations
- **Medium Priority** (CAN do): Nice-to-have features, moderate improvements, refactoring
- **Low Priority** (NICE to have): Code style, minor optimizations, documentation polish

## Phase 3: Parallel Execution Plan

Create a plan to execute tasks in parallel:

1. **Group tasks** by independence (can be done simultaneously)
2. **Identify dependencies** (must be done sequentially)
3. **Allocate to agents** based on specialization:
   - Import/dependency issues ‚Üí dependency agent
   - Test failures ‚Üí testing agent
   - Security issues ‚Üí security agent
   - Logic bugs ‚Üí debugging agent
   - New features ‚Üí feature development agent
   - Performance optimization ‚Üí performance agent
   - Infrastructure/deployment ‚Üí devops agent
   - Documentation ‚Üí documentation agent

4. **Define validation** for each task:
   - What tests should pass?
   - What checks should succeed?
   - How to verify completion?
   - What are the success criteria?

## Phase 4: Execute Plan

**üö® CRITICAL REQUIREMENT: YOU MUST USE THE TASK TOOL üö®**

**DO NOT execute tasks yourself** - you MUST dispatch EVERY task to a polymorphic agent using the Task tool.

### ‚ùå WRONG (Running commands directly):
```
DO NOT DO THIS:
- Bash: git add file.py && git commit
- Edit: file.py to change code
- Write: file.py with new content
```

### ‚úÖ CORRECT (Dispatching to polymorphic agent):

**For EACH task, you MUST call Task tool like this:**

**Example 1: Bug Fix**
```
Task(
  description="Fix import errors in reducer node",
  subagent_type="polymorphic-agent",
  prompt="**Task**: Fix import errors in node_user_reducer.py

  **Context**: [Detailed problem description]

  **Required Actions**:
  1. Analyze import errors
  2. Fix missing imports
  3. Verify code compiles

  **Success Criteria**:
  - Code compiles without errors
  - All imports resolve correctly"
)
```

**Example 2: New Feature**
```
Task(
  description="Implement Docker optimization feature",
  subagent_type="polymorphic-agent",
  prompt="**Task**: Add multi-stage Docker build optimization

  **Context**: Reduce Docker image size and build time

  **Required Actions**:
  1. Analyze current Dockerfile
  2. Implement multi-stage build
  3. Add build caching
  4. Test image builds correctly

  **Success Criteria**:
  - Image size reduced by >30%
  - Build time reduced by >20%
  - All services start correctly"
)
```

**Example 3: Enhancement**
```
Task(
  description="Enhance caching strategy",
  subagent_type="polymorphic-agent",
  prompt="**Task**: Improve Valkey caching hit rate

  **Context**: Current hit rate is 60%, target 80%

  **Required Actions**:
  1. Analyze cache miss patterns
  2. Implement cache warming
  3. Optimize TTL values
  4. Add cache metrics

  **Success Criteria**:
  - Cache hit rate >80%
  - Cache metrics visible in Grafana"
)
```

### Parallel Execution Pattern

**To run multiple tasks in parallel, dispatch multiple Task tools in ONE message:**

Example - working on 3 independent tasks:
1. Call Task tool for task 1 (bug fix)
2. Call Task tool for task 2 (new feature)
3. Call Task tool for task 3 (optimization)
4. Send all 3 in one message (parallel execution)

**Sequential tasks** - wait for Task results before dispatching next task.

## Phase 5: Validation & Quality Gates

After **each polymorphic agent cycle** completes:

1. **Run tests** if code was modified and validation was required
2. **Check code quality** (linting, type checking, security scan)
3. **Auto-refactor if needed**:
   - If code quality issues persist after agent completion
   - Trigger refactor agent to fix quality issues
   - **Limit: Maximum 3 refactor attempts** to avoid infinite loops
   - Track refactor attempts per task
4. **Capture debug information**:
   - Agent execution logs
   - Performance metrics (execution time, success/failure)
   - Intelligence used (which patterns, what queries)
   - Decisions made and reasoning
   - Store in observability system if available
5. **Generate cycle summary**:
   - ‚úÖ Completed tasks (with file:line references if applicable)
   - üîß Refactored tasks (with attempt count)
   - ‚è≠Ô∏è Skipped tasks (with reasons)
   - ‚ö†Ô∏è Quality issues (if still present after 3 refactor attempts)

## Phase 6: Final Reporting

Generate final summary across **all polymorphic agent cycles**:

1. **Overall Statistics**:
   - Total tasks completed
   - Total refactors triggered
   - Quality gate pass rate
   - Average execution time per task

2. **Debug Intelligence Captured**:
   - Agent execution patterns observed
   - Common failure modes (if any)
   - Performance bottlenecks identified
   - Intelligence effectiveness (pattern match rates)

3. **Remaining Work** (if any):
   - Tasks that exceeded refactor limit (3 attempts)
   - Manual intervention required
   - Recommendations for next steps

## Phase 7: Optional Next Steps

Ask user if they want to:
- Commit the changes
- Continue with remaining tasks
- Review captured debug intelligence
- Test the new features/optimizations
- Deploy the changes

---

## Execution Instructions

**MANDATORY PROCESS:**

1. Analyze **current conversation context** to understand the task (bug, feature, enhancement, optimization, etc.)
2. Break down the task into independent sub-tasks
3. Create parallel execution plan (identify what can run in parallel vs sequential)
4. **üö® USE TASK TOOL TO DISPATCH TO POLYMORPHIC AGENTS üö®**
5. Wait for agent results
6. Validate results
7. Report summary

**DO NOT**:
- Run Bash/Edit/Write commands directly to execute tasks
- Execute tasks yourself instead of using Task tool
- Skip dispatching to polymorphic agents
- Reject tasks because they're "enhancements" or "features" instead of bugs

**DO**:
- Use Task tool for EVERY task in your plan (bugs, features, enhancements, optimizations, etc.)
- Dispatch multiple independent tasks in parallel
- Provide detailed context in each Task prompt
- Include Intelligence Context from hooks
- Track refactor attempts per task (use internal state tracking)
- Capture debug information after each agent cycle
- Run quality checks after each cycle, not just at the end
- **Accept ANY type of work** - bugs, features, enhancements, optimizations, requirements, documentation, infrastructure

**REFACTOR ATTEMPT TRACKING**:
```
task_refactor_counts = {}  # Track attempts per task

After each cycle:
  if code_quality_issues_found:
    task_id = generate_task_id(task)
    refactor_count = task_refactor_counts.get(task_id, 0)

    if refactor_count < 3:
      # Trigger refactor agent
      task_refactor_counts[task_id] = refactor_count + 1
      dispatch_refactor_agent(task, attempt=refactor_count + 1)
    else:
      # Max attempts reached, report to user
      report_quality_issue_limit_reached(task)
```

**IMPORTANT**: If no specific tasks are mentioned in the conversation, respond with:
"‚úÖ No specific tasks detected in current context. What would you like me to work on?"

**DO NOT** say "no issues detected" if the user is asking you to build features or enhancements - those are valid tasks!
